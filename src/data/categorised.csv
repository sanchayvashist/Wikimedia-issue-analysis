Summary,Detection,Conclusion,Actionable,Time,cause_of_incident,severity_level,major_impact,
"During maintenance on our CDN edge cache layer in eqiad, all caching servers were accidentally depooled due to insufficient safeguards in tooling (and errors in following the maintenance procedure). Users whose traffic was routed to eqiad were unable to access wikis or any Wikimedia site for about 15 minutes.",Both humans and monitoring detected the issue. Humans were slightly faster. Icinga paged and alerted several SREs.,This outage could have been easily prevented if our infrastructure was more cautious about what it allowed.,"Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.* Create an automated alert for 'too many nodes depooled from a service' phab:T245058
* The depool & confctl commands should print warnings (or error out entirely, unless you override with e.g. a --force flag) if too many hosts are depooled from the same service. phab:T245059
* Investigate why Pybal didn't reject a configuration with only one server pooled, and send traffic to some of the depooled servers anyway. phab:T245060
* There should be an easy script for SREs and other technical contributors to override where their own traffic is routed for debugging tools (grafana/logstash/etc).  phab:T244761",2020-02-11,Configuration and Deployment Errors,high,Service Downtime,
"Shortly after the June 2021 switch over, the backend appservers were unable to serve requests to tr.wikivoyage.org for about 8 minutes from 14:22 UTC to 14:30 UTC. No other wikis were affected.Impact: For 8 minutes, registered users on the Turkish Wikivoyage were consistently unable to load any pages or perform any actions. The general public may have noticed it to a lesser extent due to caching at our CDN layer, although any pages absent from the CDN cache would have also been been temporarily unavailable.",,,T260297: db-eqiad and db-codfw sectionsByLoad can get out of sync,2021-06-29,Configuration and Deployment Errors,mid,Service Downtime,
"While performing a live switchover test in advance of the 2023 WMF datacenter switchover, an existing logical bug on the switchover test script accidentally set the secondary datacenter in read-only mode. While this didn't disrupt most users, mobile editing for people geolocated to codfw app servers (mostly, people in the Americas, and part of Asia and Oceania) had the editing interface disabled (while desktop users were redirected to edit through eqiad). While trying to fix this issue, an tooling interface issue caused all datacenters to be set in read-only mode, disabling editing for all users. This was quickly reverted for both datacenters and editing was restored.","Editing issue from mobile + codfw:
* No alerting went off because of this
* Reports from #wikimedia-tech surfaced ongoing issues when editing from the mobile interface (read only disabled the edit button, while on desktop edits were sent to codfw)Full read only mode issue:
* [12:20:17] <jinxer-wm>	 (MediaWikiHighErrorRate) firing: (4) Elevated rate of MediaWiki errors - appserver - https://wikitech.wikimedia.org/wiki/Application_servers/Runbook  - https://alerts.wikimedia.org/?q=alertname%3DMediaWikiHighErrorRateAlthough by this time the issue had been already corrected.Specifically, failing to set codfw as read-write wasn't detected as failing until some time passed and reports confirm the issue persisted.",,"sre.switchdc.mediawiki.07-set-readwrite doesn't reset both datacenter to rw 
* Stricter conftool data type validation?
* Uniformize mobile and desktop behaviour when in read only?
* : Globalize mwconfig ReadOnly (would avoid unpredictable behaviour when one DC is RO and not the other)==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	no 	
People                                        	Were fewer than five people paged?                                                                                	yes	No paging happened
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	No one was paged
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	https://docs.google.com/document/d/1SwXRLONP4fG6YKfCg5B26IozpQ6Hst424_ihOH0anEA/edit
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	yes	https://www.wikimediastatus.net/incidents/yhshxyn9pw22
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	The task that caused the issue was the one created to prevent the issue (circular dependency)
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	no 	Reverting the change caused confusion
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	10",2023-02-22,Configuration and Deployment Errors,high,Service Downtime,
"A power cable issue in asw2-d3-eqiad during a scheduled PDU upgrade, caused hosts in racks D1, 3, 4 to be unavailable (13:34 UTC). It rebooted into a different firmware image than the rest of the switches and had to be upgraded; once this happened, all hosts were again available (15:02 UTC).User impact: Wikifeeds was not working properly; Horizon unavailability meant that creation and updating of WMCS images was broken; dns1002 was flapping, causing sporadic dns lookup failures for eqiad hosts; logins failed to stat1005, stat1006.Other impact: kafka-jumbo1006 being unavailable meant that some data was lost for webrequest and EventLogging topics; some bacula full backups had errors and failed to run and the director had to be restarted; ferm was broken on about 30 hosts that were running puppet when DNS connectivity was lost/resolution failed and had to be manually restarted",,,"Create icinga hostgroup per rack row / Can we make row/rack visible in icinga tactical overview of hosts down somehow? (Akosiaris)
* Investigate why asw-d3-eqiad rebooted into a different boot partition with different JunOS version, causing it to be “inactive” in the VCF - and prevent this from happening again - https://phabricator.wikimedia.org/T262290
** Run “show system snapshot media internal ” and “request system snapshot slice alternate” on all EX switches
* Wikifeeds (and as a consequence, its endpoint for restbase) basically went down until the network issues in eqiad were resolved. Figure out what the hidden dependency is there. (Akosiaris)
* Investigate why routing around a missing switch member D3 did not work, hosts in D1, D2, D4 (at least) were also impacted - https://phabricator.wikimedia.org/T256112
* Check Anycast configuration as it seemed the flapping connections caused a few issues with dns1002 - https://phabricator.wikimedia.org/T262372
* Rsyslog deliveries to cengit trallog must fail independently - https://phabricator.wikimedia.org/T226703",2020-09-08,Server and Infrastructure Failures,high,Service Downtime,
"For about 30 minutes, Logstash was not getting any messages from the MediaWiki servers.",Icinga alerts.,"Logstash consumer failed, which does not recover by itself. Details at https://phabricator.wikimedia.org/T230847#5427615",None. See https://phabricator.wikimedia.org/T230847Category:Incident documentation,2019-08-20,API and Integration Failures,mid,Degraded System Performance,
"A Puppet patch (change 817307) was merged which would accidentally install confd on a significant number of production hosts. The Puppet provisioning for these confd installations failed half-way due to having no corresponding Icinga checks defined. This in turn fired an Icinga alert: 10:44 <icinga-wm> PROBLEM - Widespread puppet agent failures on alert1001 is CRITICAL: 0.1057 ge 0.01 Engineers started work on reversing it, by cleaning up the inadvertent installs of confd via Cumin. Security posture was not compromised and there was no external user impact.Documentation:* puppet failures - https://logstash.wikimedia.org/goto/a5b60af08e257d90a469a78d12056ec2*Confd* Cumin",,,"Git defaults to shows the author's date, not the commit date. Consider adding the following aliases to your git config as fix:
** lola = log --graph --pretty=format:\""%C(auto)%h%d%Creset %C(cyan)(%cr)%Creset %C(green)%cn <%ce>%Creset %s\"" --all
** grephist = log --graph --pretty=format:\""%C(auto)%h%d%Creset %C(cyan)(%cr)%Creset %C(green)%cn <%ce>%Creset %s\"" --all -S==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	Overlap of 3
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	n/a
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	n/a
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	no 	
Process                                       	Are the documented action items assigned?                                                                         	yes	action items limited to improved local bash aliases
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	12",2022-08-10,Configuration and Deployment Errors,mid,Degraded System Performance,
Hotlinking of an image on Commons caused link saturation in the eqsin datacentre.,"This incident was detected via paging for port utilisation in eqsin: 
<+jinxer-wm> FIRING: Primary outbound port utilisation over 80%  #page: Alert for device asw1-eqsin.mgmt.eqsin.wmnet - Primary outbound port utilisation over 80%  #page
Additionally FastNetMon detected what it perceived as a DDoS. This was more or less correct as the behaviours witnessed are similar to a simple DDoS attack.","This was a somewhat familiar pattern, as we have seen similar issues in the past on a larger scale.","Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	no 	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	9",2024-05-31,Performance and Load Issues,mid,Degraded System Performance,
"On Sunday 5th at 11:22UTC, the primary hard drive of cr3-eqsin (one of the two Singapore POP routers) crashed.This caused the router to reboot into its second disk, containing only a factory default configuration. Everything failed over cleanly to the redundant router.Impact: We lost at max ~15000 requests/s in a 7min window (see screenshot, and graph).","Was automated monitoring first to detect it? Yes
* Did the appropriate alert(s) fire? Yes
* PROBLEM - Host cr3-eqsin is DOWN: PING CRITICAL - Packet loss = 100% (paging alert)
* Was the alert volume manageable? Yes, only relevant alerts fired
* Did they point to the problem with as much accuracy as possible? Yes, the router went down, and only the router down paging alert triggered","This outage showed that our hardware redundancy and failover are solid
* Juniper recently introduced a new feature: vmhost snapshot that would have prevented the lack of redundancy (but not the crash itself)","cr3-eqsin disk 1 failure - https://phabricator.wikimedia.org/T257154
* Investigate Junos vmhost snapshot - https://phabricator.wikimedia.org/T257153",2020-07-05,Server and Infrastructure Failures,mid,Degraded System Performance,
"Slow database queries resulted in php-fpm worker exhaustion.Impact: For about 10 minutes, backends were slow or unavailable for all wikis. This affected logged-in users, most bots/API queries, and some page views from unregistered users (pages that were recently edited or otherwise expired from the CDN cache). Documentation:* Public incident task: T291311 * Similar to Incident documentation/2021-09-04 appserver latency and Incident documentation/2021-04-15 appserver latency.",,,T284419 (restricted),2021-09-18,Performance and Load Issues,high,Service Downtime,
"GitLab was switched to the other data center as a planned maintenance in T329931. During the switchover some configuration had to be changed depending on the instance state (production or replica). The daily restore job was not disabled on the new production host in codfw, resulting in a backup being restored on the production host. So all actions between the backup (Feb 28th 00:04:00) and the restore (Feb 28th 02:00:00) are lost.","The issue was detected by automatic monitoring and task creation (probeDown)  in https://phabricator.wikimedia.org/T330717    alertname: ProbeDown
    instance: gitlab2002:443
    job: probes/custom
    prometheus: ops
    severity: task
    site: codfw
    source: prometheus
    team: serviceops-collab",,"Enable and disable restore automatically depending on instance status (https://gerrit.wikimedia.org/r/c/operations/puppet/+/892892) - done
*Automate failover/switchover using a cookbook - T330771
*Add check of timers after a failover/switchover (manual or automated)?
*Add safeguard to restore script for production/active host - T331295==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	Yes in general, no for GitLab specifically, but that's to be expected in this case.
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	No Google doc, the response was tracked in a task only.
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	The monitoring was not meant for this specific failure, but worked nonetheless
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	10",2023-02-28,Configuration and Deployment Errors,mid,Data Loss or Corruption,
Brief outbound bandwidth spike for upload in Eqsin dc. Recovered by itself.* Restricted details.* Grafana,,,"T310997 Improve webrequest log
* https://gerrit.wikimedia.org/r/c/operations/puppet/+/768723",2022-07-13,Performance and Load Issues,low,Degraded System Performance,
"Unexpected loss of internal connectivity to codfw hosts and services for 5 minutes, creating user-visible failed queries for users whose traffic hits our eqsin and ulsfo edges, when they were using services that are active/active (Swift, Maps, Restbase API, ...)Cause was maintenance that required a linecard restart on cr1-codfw, which exposed a flaw in codfw's network design.Loss of some external connectivity to codfw was expected, and the site was CDN-depooled before the maintenance began.  However, what was unanticipated was that cr1-codfw would hold VRRP mastership for the duration of the linecard restart, so it tried to act as the default gateway for all hosts in the cluster, while effectively being a black hole for routing anywhere outside the cluster (as the linecard being rebooted has both all the cross-cluster links and the router-to-router interconnect).There was a second OSPF flap/convergence event around 12:22, however it doesn't seem to have been impactful.","Automated: Icinga pages for service IPs in codfw, in addition to alerts for socket timeouts against many hosts (especially appservers).Since all codfw appservers could not be reached, there *would* have been lots of alert spam in #wikimedia-operations (one per appserver) -- except that icinga-wm got Excess Flooded off of IRC.",,"Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Add linecard diversity to the router-to-router interconnect in codfw.  phab:T248506
* Consider plumbing a backup router cross-connect via a new VLAN on the access switches.",2020-03-25,Server and Infrastructure Failures,high,Service Downtime,
"Kubernetes API server process failed to start, causing the collapse of many Kubernetes managed services and disabling launching or managing of containers for tools while the server was down.Toolforge Kubernetes API services were down for several hours due to puppet changes that caused a CA certificate mismatch with the etcd servers.  Since it was precipitated by a restart of the service, the root cause took significant time to find and correct.  While it was down, Toolforge webservices using Kubernetes would fail to launch or restart and likely many services failed while the API server was not responding.",The first notice was icinga alerting on the check against http://checker.tools.wmflabs.org/k8s/nodes/ready was our first notice.,"Typically badly-behaved things"" like etcd confused the solutions here. Puppet certs are dangerous. Very old Kubernetes versions have terrible log errors.","NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Document etcd cluster processes 
* Upgrade Toolforge Kubernetes (already underway) to improve etcd and Kubernetes error reporting -- and also not require restarting the API server to add a tool 
* Attempt to make kube2proxy more resilient to API server failures 
* Audit puppet CA certs.  There really shouldn't be more than one within the tools project. 
* Make Kubernetes control plane HA (already in PoC in toolsbeta, so the task is closed and waiting on the full upgrade deployment)",2019-09-10,Server and Infrastructure Failures,high,Service Downtime,
"The service got overloaded and started to block client traffic, including Pybal which ultimately triggered the page.  Icinga sent some non-paging alerts at 13:50 but paging alerts didn't get sent until 16:52.",,,"To make that service stable is to re-architect and replace Blazegraph.  The Search team will discuss this and arrange follow up actions
* In the meantime, https://phabricator.wikimedia.org/T293862 might help to improve the reliability of Blazegraph.
* Investigate if earlier alerts should page https://phabricator.wikimedia.org/T303134==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	1    	
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	as weekend
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	N/A  	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	0    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	?    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	
Total score	Total score                                                                                                                                              	6",2022-03-06,Performance and Load Issues,mid,Degraded System Performance,
"For ~30min (from 18:25 until 19:06) average HTTP GET latency for mediawiki backends was higher than usual.* For ~12 hours, database replicas of many wikis were stale for Wikimedia Cloud Services such as Toolforge.	* For ~30min (from 18:25 until 19:06) average HTTP GET latency for mediawiki backends was higher than usual.* For ~12 hours, database replicas of many wikis were stale for Wikimedia Cloud Services such as Toolforge.	* For ~30min (from 18:25 until 19:06) average HTTP GET latency for mediawiki backends was higher than usual.* For ~12 hours, database replicas of many wikis were stale for Wikimedia Cloud Services such as Toolforge.The s3 replica (db1112.eqiad.wmnet) that handles recentchanges/watchlist/contributions queries went down, triggering an icinga alert for the host being down, and a few minutes later an alert for increased appserver latency on GET requests. Confusion over the role of db1112, as it's also the s3 sanitarium master, didn't appropriately recognize the severity. Only while investigating the latency alerts was it realized that the database server was down, leading it to be depooled and restarted via mgmt. Once the host came back, a page was sent out. The incident was resolved by pooling a different s3 replica in its place.s3 replication to WMCS wikireplicas was broken until it was restarted at 2021-10-26 09:15. s3 is the default database section for smaller wikis, which currently accounts for 92% of wikis (905/981 wikis). Impact:* For ~30min (from 18:25 until 19:06) average HTTP GET latency for mediawiki backends was higher than usual.* For ~12 hours, database replicas of many wikis were stale for Wikimedia Cloud Services such as Toolforge.",,,"T294490: db1112 being down did not trigger any alert that paged until the host was brought back up (we get paged for replication lag but not for host down, Marostegui said for DB hosts we should start paging on HOST down which we normally don't do. This would require a puppet change.)== Scorecard ==           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	Info not logged, scoring 0
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	No page
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	No page
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	No page
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	1    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	Monitoring did detect, although without paging severity (note: severity for related alert was increased after this incident)
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	
Total score	Total score                                                                                                                                              	6",2021-10-25,Schema and Database Issues,high,Degraded System Performance,
"At 02:05 UTC, the parse2007 server in Codfw started to spontaneously respond with fatal error, possibly due to a php-opcache corruption. At 11:20 UTC, the server was restarted per the (now common) procedure in response to such corruptions after which the errors immediately stopped. Other parse servers and MW servers were not affected.  Impact: For 9 hours, 10% of submissions to Parsoid to parse or save wiki pages were failing on all wikis.",Human reporting an error.,,"Let deployers ssh to parse hosts. T290144
* TODO: Re-evaluate alerting strategy around ""mediawiki-exceptions"". We have a breakdown by cluster (web,api,job,parsoid). Do we need a breakdown by HTTP verb? (E.g.  ""read"" GET/HEAD/OPTIONS vs write ""POST/DELETE"" or some such).
* TODO: I was unable to find stats on error rates of api.php requests in Grafana. HTTP-level app server stats are insufficient since api errors are HTTP 200. The Graphite metrics for API req breakdown don't measure errors currently. The Logstash data for api-appservers errors is also insufficient since properly handled errors wouldn't be exceptions and wouldn't be logged there as such (e.g. when action=visualeditoredit finds Restbase/Parsoid respond with http 500, it responds to the client with an error. Where do we measure this?)TODO: Add the #Sustainability (Incident Followup) Phabricator tag to these tasks.",2021-09-01,Code and Application Bugs,high,Degraded System Performance,
A change was deployed to puppet which inadvertently deleted the private repo from all puppet backend servers and puppet standalone servers.  A few standalone servers in the Cloud environment maintain secrets by applying local commits to the labs/private repo.  This event caused all secrets to be deleted required manual restorationImpact: Any cloud environments which had added private secrets would have reverted to using the dummy secrets in the labs/private repo,The issue was noticed by a member of the Cloud Services team.,,https://phabricator.wikimedia.org/T254491,2020-06-04,Configuration and Deployment Errors,mid,Data Loss or Corruption,
"The mw-page-content-change-enrich (flink) app is failing to start in eqiad (passive DC because of a network timeout on a dependent service (thanos-swift) https://logstash.wikimedia.org/goto/ce1765e186329ed74f179d375f8df182.The app needs swift for HA. The connection failure caused the k8s operator to fail startup. The incident was caused by incorrect egress rules. Since the app is DC agnostic, egress rules must be set for both DCs ip ranges for all deployments.This issue was fixed by applying the proper egress rules, a re-deploying the application. The active DC deployment was not affected.",Alerts where fired based on prometheus metrics. The issue was escalated to the DRI.,"there was no user visible outage.
* only passive DC was affected.
* mw-page-content-change-enrich egress rules to thanos were misconfigured.
* we never encountered this issue before, because swift-thanos was pooled in both eqiad and codfw.
* during DC switchover swift-thanos was depooled from eqiad.
* the network route issue manifested.","Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-09-20,Configuration and Deployment Errors,mid,API or Integration Failures,
"Log aggregation was failing on recently re-imaged Hadoop worker nodes. This was due to having the wrong value for the compression type to be used by the log aggregator service. This was spotted when some specific Airflow tasks were failing randomly when Airflow tried to fetch the aggregated logs. These were non-idempotent Airflow tasks (eg rm a tmp dir, or mv an previously created archive on HDFS). The Yarn applications were in a SUCCESS state, but the Airflow process failed to retrieve the logs and marked some of the task attempts as failed. This was fixed by correcting the compression type to gz from the erroneously submitted value of gzip.","Automated email alerts from AirflowLog aggregation alerts similar to:
[Data-engineering-alerts] Airflow alert: <TaskInstance: projectview_hourly.move_data_to_archive scheduled__2023-08-29T13:00:00+00:00 [failed]>Try 1 out of 1Exception:No logs found. Log aggregation may have not completed, or it may not be enabled.Log: LinkHost: an-launcher1002.eqiad.wmnetMark success: Link
Remove temp directory Alert
[Data-engineering-alerts] Airflow alert: <TaskInstance: druid_load_webrequest_sampled_128_daily.remove_temporary_directory scheduled__2023-08-29T00:00:00+00:00 [failed]>Try 6 out of 6
Exception:
SkeinHook druid_load_webrequest_sampled_128_daily__remove_temporary_directory__20230829 application_1692895131960_27376
Log: Link
Host: an-launcher1002.eqiad.wmnet
Mark success: LinkWrite how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?Copy the relevant alerts that fired in this section.Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?TODO: If human only, an actionable should probably be to ""add alerting"".","OPTIONAL: General conclusions (bullet points or narrative)* We need to test config changes on the Hadoop test cluster first before merging.
* We need to remember to restart the services after a verified config change commit","Define an alert if the yarn config file modification time is newer than the service start time by more than 24 hours
** Apply this new alert pattern to other (HDFS, Namenode etc)Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-08-30,Configuration and Deployment Errors,mid,Degraded System Performance,
"A large influx in requests led to excessive thread pool usage from codfw blazegraph backends, with concomitant increases in CPU load and throttling filter state size. This triggered a known bug in Blazegraph where its improper thread management leads to deadlock.The system self healed, perhaps due to application-level throttling state throttling the offender.Most or all CODFW-routed wdqs requests during the incident window failed.","The issue was rapidly detected by pybal monitoring of WDQS. A page was emitted by the monitoring system fairly quickly.2022-12-12 20:13:18 <+jinxer-wm> (ProbeDown) firing: Service wdqs-ssl:443 has failed probes (http_wdqs-ssl_ip4) #page - https://wikitech.wikimedia.org/wiki/Runbook#wdqs-ssl:443 - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown2022-12-12 20:13:18 <+jinxer-wm> (ProbeDown) firing: Service wdqs-ssl:443 has failed probes (http_wdqs-ssl_ip4) - https://wikitech.wikimedia.org/wiki/Runbook#wdqs-ssl:443 - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown2022-12-12 20:14:17 <+icinga-wm> PROBLEM - PyBal backends health check on lvs2009 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs2004.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs-ssl_443: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs2004.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs_80: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs202022-12-12 20:14:17 <+icinga-wm> .wmnet, wdqs2002.codfw.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal",OPTIONAL: General conclusions (bullet points or narrative),"Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the incident status section actively updated during the incident?                                             		
Process                                       	Was the public status page updated?                                                                               		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-12-12,Code and Application Bugs,high,Service Downtime,
https://phabricator.wikimedia.org/T257062 Lilypond seemingly bypassing security restrictions.,,,"Audit all mismatched/unrecognised wmf-config settings. – https://phabricator.wikimedia.org/T257079
* Lint against unrecognised wmf-config settings in CI. – https://phabricator.wikimedia.org/T248866",2020-07-03,Security and Access Issues,mid,Security Vulnerabilities or Breaches,
"On March 14 2021 the MediaWiki API were overloaded and ran out of php-fpm processes. This caused an API outage on all API servers from 17:00 to 17:26 UTC. The root cause of the outage were queries against commons that caused database s4 on server db1144 to be overloaded. Db1144 also serves queries to contributions, recentchanges, watchlist and other MediaWiki features. Task: T277417https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?viewPanel=46&orgId=1&from=1615734448378&to=1615746774986&var-datasource=eqiad%20prometheus%2Fops&var-cluster=api_appserver&var-method=GET&var-code=200https://grafana.wikimedia.org/d/000000273/mysql?viewPanel=37&orgId=1&var-server=db1144&var-port=13314&from=1615738747603&to=1615745074190The queries against commons were analyzed for inefficiencies but seem to be well written and optimized SQL. See task: T277416",,,Improve traceability of commons queries: T193050 (filed in 2018),2021-03-14,Performance and Load Issues,high,API or Integration Failures,
"While troubleshooting database issue on labtestwikitech, I (Andrew) dumped some internal data structures to stdout while debugging in PHP via print statements. These data structures contained database credentials. It took a bit for me to remember that because of how PHP works, print statements (also) write the response to web clients.The standard practice for ad-hoc debugging is wfDebugLog(), see also Debugging in production.As soon as Sam Reed noticed the leakage there was a quick response and the password was rotated. Due to incomplete automation, rotating the password took quite some time (maybe 60-90 minutes with several SREs participating).Because of firewalls and host-selective database grants, the leaked passwords are only useful from production hosts (10.64.0.0 and 10.192.0.0) so data integrity was not compromised.",,,"document the repool script for dbctl in wikitech
*reconsider labtestwikitech. Decommission, or standardize to some degree so it isn't managed as an afterthought
**https://phabricator.wikimedia.org/T310795
*Improve automation:
**upgrade & document password-rotation script
**productionize repool script
*Consider improved pw redaction:
**The data shown by var_dump(), print_r() etc. has theoretically been configurable via the __debugInfo() magic method since PHP 5.4 (RFC). T277618 proposed using this mechanism (originally to reduce the size of the output, rather than to redact sensitive information), but found PHP bug 80894, which is only fixed in PHP 7.4 or later. Once WMF is on PHP 7.4 (T271736), we should consider using __debugInfo() to remove the password from the debug output of database objects. (And $wgDBpassword from globals / config?)
**In PHP 8.2, the \SensitiveParameter attribute can be used to redact function parameters from stack traces (RFC), though that’s less relevant for us since (I think?) we never show stack traces with values anyways (only value types).==Scorecard==Incident Engagement ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	No 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	Yes	
People                                        	Were fewer than five people paged?                                                                                	   	No pages were sent
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	   	No pages were sent
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	   	No pages were sent
Process                                       	Was the incident status section actively updated during the incident?                                             	No 	No need
Process                                       	Was the public status page updated?                                                                               	No 	No need
Process                                       	Is there a phabricator task for the incident?                                                                     	Yes	
Process                                       	Are the documented action items assigned?                                                                         	No 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	Yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	   	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	Yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	No 	We have no monitoring for this sort of issue
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	Yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	No 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-06-16,Security and Access Issues,high,Security Vulnerabilities or Breaches,
"A particular banner was deployed via CentralNotice that was both enabled for all users and with 100% sampling rate for its event instrumentation.This caused instabilities at the outer traffic layer. The large amount of incoming traffic for event beacons, each of which had to be handed off to a backend service (eventgate-analytics-external), resulted in connections piling up and Varnish was unable to handle it and other traffic as a result, thus causing wikis to be unreachable in the affected regions. Initially Esams datacenter clients (mostly Europe, Africa and Middle East), with some temporary issues on other datacenters (Eqiad) as well when we initially attempted to reroute traffic to there.Documentation:*Varnish traffic 08:00-12:00 (Grafana)*Frontend traffic 2xx responses (Grafana)*navtiming pageview sampling (Grafana)*https://www.wikimediastatus.net/incidents/rhn1l6k33ynz*Restricted documented",,,"Avoid flood of CN banner analytics
*  Set a maximum for configurable sample rate of CentralNotice events that use EventGate==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	1    	
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	1    	
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	1    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	1    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	1    	
Total score	Total score                                                                                                                                              	11",2022-03-04,Performance and Load Issues,high,Service Downtime,
"On 2022-05-14 at 8:18 UTC there was a 3 minute impact on uncached traffic (high latency, unavailability) related to application server worker thread exhaustion caused by slow database response. There was no clear root cause at the time. The incident occurred again on the same database host on 2022-05-20 at 09:35 UTC, this time lasting for 5 minutes. After further investigation the likely root cause is a MariaDB 10.6 performance regression under load, further researched in https://phabricator.wikimedia.org/T311106.Documentation:*MediaWiki Exceptions",,,"Investigate mariadb 10.6 performance regression during spikes/high load==Scorecard==Incident Engagement™  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	yes	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	11",2022-05-20,Performance and Load Issues,mid,Degraded System Performance,
"s4 primary database master (db1138) had a hardware memory issue, mysqld process crashed and came back as read-only for 8 minutes.Impact: commonswiki didn't accept writes for 8 minutes. Reads remained unaffected",,"This was a hard to avoid crash - hardware crash on a memory DIMM.Masters start as read-only by default (to avoid letting more writes go through after a crash, until we are fully sure data and host are ok and still able to take the master role).We did see traces of issues on the idrac's error log, if we could alert on those, maybe we could have performed a master failover before this host crashes. If this crash happens on a slave, the impact wouldn't have been as big, as slaves are read-only by default and MW would have depooled the host automatically.","DONE] Documentation on how to proceed if a master pages for read-only = ON: https://phabricator.wikimedia.org/T253832
* [DONE] Failover db1138 to its candidate master (scheduled for Friday 29th at 05:00 AM UTC): https://phabricator.wikimedia.org/T253808
* [DONE] Replace failed DIMM on that host: https://phabricator.wikimedia.org/T253808
* Alert on ECC warnings in SEL https://phabricator.wikimedia.org/T253810
* Create a script to move replicas between hosts when the master isn't available https://phabricator.wikimedia.org/T196366",2020-05-28,Server and Infrastructure Failures,mid,Service Downtime,
"In-progress Cite extension code unleashed a flood of ""PHP notice"" logspam.  These were all non-fatal errors, but at approximately 3,000 per hour put a burden on human log-watchers.  Hotfixes helped reducing the logspam, before the actual issue was found, fixed, and backported.",noticed an increase in errors while monitoring logspam-watch during train deployment.,"What weaknesses did we learn about and how can we address them?The issue was a combination of several factors we never saw in any test or dev environment:
* Working with the Message class can cause it to request its own Parser, which is done with PHP's built-in clone command.
* This only happens if MessageCache hasn't been used yet in the requet.  Hence the majority of the failures came from the API, as this is much more likely there.
* Cloning an object in PHP will ""magically"" clone all of its properties, no matter if declared or dynamically created.
* The Cite extension uses a dynamic property $parser->extCite to store its instance, holding all state.
* A hook takes care that the Cite object is cloned as well when the Parser is cloned.
* Since , the state was stored two levels deep, but cloning only performed a shallow copy.
* Cloning now resulted in two different Parsers with two different Cite instances that share the same state array.
* The fresh Message parser needs to clean its state, which cleans the state in all Cite instances.  Hence all previously seen <ref> got lost.
* The loop currently rendering a non-empty <references /> section starts to access keys in an empty array.
* This only happens when there is a reason to render a Message in the middle of this loop, e.g. because of a bad <ref>, or a template that renders an error.","NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
*  – Original error report about the ""undefined index"" log spam. Resolved.
*  – User reporting incomplete rendering of references. Resolved.
*  and  – Two very different integration tests are now able to prevent this from happening again.
*  – Extra time was spent on reworking the Cite codebase to not rely on cloning any more.
*  – The QWERTY team will hold a retrospective meeting.",2019-12-10,Code and Application Bugs,low,Degraded System Performance,
"Multiple of our redundant network providers for the San Francisco datacenter simultaneously experienced connectivity loss. After 20 minutes, clients were rerouted to other datacenters.Documentation:*https://gerrit.wikimedia.org/r/c/operations/dns/+/767250/*https://www.wikimediastatus.net/incidents/2rp6n2cpym3m*https://phabricator.wikimedia.org/P21629#102776",,,"T303219 Integrate DNS depools with Etcd and automate/remove the need for writing a Git commit.* Can we increase fiber redundancy?==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	Info not logged
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	1    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	1    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	0    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	0    	one appears to be an open question
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	1    	
Total score	Total score                                                                                                                                              	7",2022-03-01,Server and Infrastructure Failures,high,Service Downtime,
"The metadata is aimed at helping provide a quick snapshot of context around what happened during the incident.Incident ID   	2021-12-03 mx                                                                                                                                                                                                  	UTC Start Timestamp:                                                                                                                                                                                           	YYYY-MM-DD hh:mm:ssIncident Task 	T297127                                                                                                                                                                                                        	UTC End Timestamp                                                                                                                                                                                              	YYYY-MM-DD hh:mm:ssPeople Paged  	<amount of people>                                                                                                                                                                                             	Responder Count                                                                                                                                                                                                	<amount of people>Coordinator(s)	Names - Emails                                                                                                                                                                                                 	Relevant Metrics / SLO(s) affected                                                                                                                                                                             	Relevant metrics% error budgetImpact:       	For about 24 hours, a portion of outgoing email from wikimedia.org was delayed in delivery. This affected staff Gmail, and Znuny/OTRS/Phabricator notifications. No mail was lost, it was eventually delivered.	For about 24 hours, a portion of outgoing email from wikimedia.org was delayed in delivery. This affected staff Gmail, and Znuny/OTRS/Phabricator notifications. No mail was lost, it was eventually delivered.	For about 24 hours, a portion of outgoing email from wikimedia.org was delayed in delivery. This affected staff Gmail, and Znuny/OTRS/Phabricator notifications. No mail was lost, it was eventually delivered.On November 24th the Linux kernel on one of our mail servers was upgraded. Then on November 30th changes were made to prioritization of mail servers used for wiki mail which sent more traffic than before to this server. Due to a kernel bug in the iptables conntrack module packets from our mail server towards Google mail servers started to be dropped. Outgoing mail became stuck in the outgoing queue. After a while the number of mails in the queue started to trigger monitoring alerts ,  and internal users started to report to ITS about issues with mail.SRE started to investigate and identified first the timeouts as a cause and then after some debugging that the cause for that was the firewall suddenly dropping packets. A manual fix was applied to add extra firewall rules to allow these dropped packets. (ip6tables -I INPUT -s 2620:0:861:102:10:64:16:8 -j ACCEPT)After this the outgoing mail stuck in the queue started to be sent out but it took some time to catch up. At this point it was still unclear what the further root cause of the firewall change was but mail was being sent out again normally. SRE informed ITS about the ongoing process.After further debugging, finding other bug reports and going through server admin logs it became clear that the change correlated nicely to the latest reboot which was done for a kernel upgrade.Finding this and after the queue was fully processed SRE proceeded to deactivate the affected mail server and rebooted it in order to downgrade it to the previous kernel version.While doing this another unrelated issue appeared, the ganeti VM that is the mail server did not come back from the reboot.It failed to get an IP on its network interface and was online but could not be reached via SSH and had no networking at all. After a little while SRE identified this as a known issue with device renumbering on ganeti VMs and was able to get the server back up by editing network configuration manually via root console followed by another reboot.At this point the server was now back up with the previous kernel version (5.10.0-8) and tests were made to see if the issue was gone. It was confirmed mail could be sent again with that kernel!Some changes that had been reverted in Gerrit were re-reverted and a decision was made to keep the mailserver deactivated over the weekend.Incident was closed, of course minus the follow-ups.","It was detected by Icinga at: ""(2021-12-02 10:24:02 - Icinga notices the mail queue size grew over the alerting threshold of 2000 (is: 4013) and notifies IRC about it).""This could be seen on IRC and on the Icinga web UI but did not send direct SMS or emails to specific people.A follow-up will be to change this and make it page SRE (phab:T297144).In parallel it was noticed by wikimedia.org staff who reported to ITS who created phab:T297017.Icinga - Alert Notifications for mx2001Icinga - exim queue size check on mx2001","Monitoring exists but notifications need to be worked on.
* More specifically test sending out mail after reboots/upgrades on mail servers.","phab:T297128 - Bringing mx2001 back into service
*phab:T297144 - large MX queues should page",2021-12-03,Configuration and Deployment Errors,mid,Degraded System Performance,
A change in the MediaWiki REST API caused requests for old revisions to serve the current revision. It was noticed because visual diffs were all indicating no changes happened.,The issue was manually detected by a human.,,"Automated testing to verify behavior of requesting HTML of old revisions from the REST API (https://gerrit.wikimedia.org/r/866621, https://gerrit.wikimedia.org/r/866622)
** Should examine why/how Parsoid's ""old"" api-testing pathways regressed, which would/should have covered this code.  Are there other tests from the old api-testing suite which have similarly vanished?
*** The tests for retrieving revision HTML by revision ID existed and was covering this path - but the revision ID it was using for the test was the current revision. We never had a test specifically for an old revision. This has now been added. daniel (talk) 16:54, 15 December 2022 (UTC)
* There are alternate 'easier' ways to do rollbacks now.  Update documentation.
* Is the 22-minute long sync-wikiversions expected? No.
** The kubernetes multi-version image build took 10 minutes.  The reason is a bad behavior in the incremental image build process when wikiversions rolls back.  https://phabricator.wikimedia.org/T325576
* Documentation to make it clearer that rollback was the correct solution to this problem.
* Something around testing forward- and backward-compatibility of Parsoid/RESTBase version downgrades
* Something around testing forward-compatibility of ParserCache changes
** The documentation doesn't just need to exist, it needs to be discoverable in the the right spot. Probably in a place that developers would touch when trying to implement backwards compatibility.
* Is there some way to enhance the visibility of breakages in the ""visual diff"" feature, to make it more likely regressions in the feature will be caught during group0 or group1 rollout?
** It would be nice to have Selenium tests for this feature. Selenium tests are slow, brittle, and tricky to set up locally. It would help if we could streamline this.
* Reduce the gap between the UBN being filed and the relevant team springing into action
** Platform Engineering currently doesn't have a notification mechanism for UBNs. A bot posting to Slack would be helpful.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	batphone because after business hours
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	all SRE were paged, Content Transform Team was not paged
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	This wiki page was created before the incident was resolved, but Kunal the IC didn't have access needed to create a incident status google doc.
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	T324801
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	no 	See above regarding usage of Signal and #page in #mediawiki_security
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	Issue was manually detected by a human
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	phabricator, klaxon, scap sync-wikiversions worked
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	Heterogeneous_deployment/Train_deploys#Rollback
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2022-12-09,Code and Application Bugs,mid,API or Integration Failures,
"During a routine maintenance consisting of upgrading HAProxy on cache hosts, all of the backends (ATS) in the text cache cluster in esams and eqiad were accidentally depooled due to a mismatch on the maintenance run between depooling the hosts individually and pooling back the cdn. This caused both cached and uncached traffic requests for wikis and other ATS-backed services to fail and return errors to clients, mostly in parts of Europe, Africa and Asia. Approximately 17 million HTTP requests (according to varnish) / 5 million user requests (according to NEL estimation) errored out in total. Editing rate was reduced to less than half. Upload cluster, clients geolocated to drmrs, codfw, ulsfo or eqsin, and GET requests cached in memory were not affected.","Automated alerts / pages fired (FrontendUnavailable)* FrontendUnavailable cache_text ()
* FrontendUnavailable (varnish-text)
* [5x] ProbeDown (probes/service eqiad)",,"Update tunnelencabulator, some SREs had trouble accessing graphs during the outage
** https://github.com/cdanis/tunnelencabulator/pull/6
* T330272 Provide a cookbook to perform HAProxy upgrades on CDN nodes
* T330405 Improve FrontendUnavailable alerts with more information/context of what's failing==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	yes	
Process                                       	Is there a phabricator task for the incident?                                                                     	no 	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	13",2023-02-22,Configuration and Deployment Errors,high,Service Downtime,
Roll out of wmf.8 to group1 broke the world.,Initial indicators of the issue were picked up in logstash and via logspam-watch on mwlog1001.  A large number of Icinga alerts followed.It seems likely that the primary issue was obscured during the initial deploy by a focus on Parsoid errors.,,,2019-12-04,Configuration and Deployment Errors,high,Service Downtime,
During scap deploys one of the LVS servers in eqiad was taken down which resulted in multiple servers being depooled making eqiad unable so serve traffic.,The issue was detected immediately by SRE doing maintenance work. Multiple alerts as well as pages immediately followed.,,"https://phabricator.wikimedia.org/T334703==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	yes	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	Same issue as Incidents/2023-04-17 eqiad/LVS
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	10",2023-04-17,Server and Infrastructure Failures,high,Service Downtime,
"A template changes in itwiki triggered translusion updates to many pages. Changeprop (with retries) issued thousands of requests to the API cluster to reparse the transcluding pages, including page summaries, which are done by Mobileapps.","The consequences of the issues, that is not having enough PHP-FPM workers available was detected in a timely manner from icinga multiple times* 14:36: PROBLEM - Not enough idle PHP-FPM workers for Mediawiki api_appserver at eqiad #page
* 15:33: PROBLEM - Some MediaWiki servers are running out of idle PHP-FPM workers in api_appserver at eqiad
* 19:09 Pages again: PROBLEM - Not enough idle PHP-FPM workers for Mediawiki api_appserver at eqiad #page on alert1001 is CRITICAL: 0.2933 lt 0.3 Unfortunately it took a considerable amount of time to pin down the root cause.",,"Mobileapps is often throttled in codfw T305482
*Limit changeprop transclusion concurrency. https://gerrit.wikimedia.org/r/c/operations/deployment-charts/+/774462==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	1    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	N/A  	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	0    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	0    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	
Total score	Total score                                                                                                                                              	5",2022-03-27,Performance and Load Issues,mid,Degraded System Performance,
"The main page of the French Wikipedia was loading much slower than usual for some time on October 30th 2020. The page did load but took several seconds and up to 20 seconds to finish.The incident lasted roughly 7 hours and the group of affected users were all logged in users using the main page of French Wikipedia during that time. Logged out users / general readers were served cached results and did not notice the incident. We can roughly estimate the number of affected users to about ~ 625, based on about 2500 unique editors per day.The ones who did notice started reporting on the French Wikipedia Bistro page and a ticket on phabricator was created.The FeaturedFeeds extension adds Atom feed <link> tags to the head of main pages, but internally it generates the entire feed, just to add the tags. Normally it reads from the cache so this wasn't noticed until the feed became too big to fit in memcached (ITEM_TOO_BIG error) and was being generated/parsed on every main page load, including action=history or action=edit.The issue was resolved after the wmgFeaturedFeedsOverrides limit used by the FeaturedFeeds extensionwas reduced from 60 items per feed to the default of 10 items per feed that is used by other wikis.This did not change rendering of the main page; the only lasting effect will be that users of fr.planet.wikimedia.org, the RSS feed aggregator for Wikimedia-related feeds, will see only 10 instead of 60 items from the fr.wikipedia featured feed.The feed config and special case for frwiki had been added as part of  back in 2017.An additional mini report can be found in T266865#6592372.",,,"reduce number of feed items from 60 to default (done)
* : FeaturedFeeds should not load all feed content just to output the feed URLs on the main page (done)",2020-10-30,Code and Application Bugs,mid,Degraded System Performance,
"An upgrade of Gerrit from 3.4.8 to 3.5.4 was scheduled on November 17th at 9:00 UTC. After the upgrade, the root partition filled up causing Gerrit to no longer be able to write to its indexes. The first symptoms were inability to write a comment or cast a vote. The service got stopped entirely to relocate Gerrit data to a dedicated partition. The upgraded Gerrit was back at 11:45 UTC.","The first report was at 9:16 by Valentin. He was getting no content beside header/footer in https://gerrit.wikimedia.org/r/dashboard/self . Same for https://gerrit.wikimedia.org/r/q/status:open+-is:wip . It was unclear whether it was related and it got dismissed based on browser caches being out of sync.Timo reports at 10:36 that no write action works. It is when the root partition had filled.Icinga alerts did not trigger since Gerrit/Upgrade#Deploying asks to put both hosts in maintenance mode, hence none of the probes (such as the disk space check) emit any alarm.","When a full reindexing of changes, it would probably be better to conduct the upgrade very early in the UTC morning or over the week-end.
* The upgrade procedure requests to disable the Icinga monitoring at the host level. As a side effect it also disables the disk space checks.
* The H2 databases have probably been carried over since we originally started Gerrit. It is unclear whether they should be so large. Antoine is puzzled by gerrit show-caches output which reports gerrit_file_diff to have Space: 134.20m but the corresponding H2 database file gerrit_file_diff.h2.db file was 12 GBytes.
* Relocating the large files to a dedicated partition was the fix","Move Gerrit data out of root partition
*  Investigate why the H2 database files are so large
* Document the new partition layout? (partition to be removed after data get moved)
* Check with SRE collab about the partitioning scheme on gerrit1001 since the root partition is not LVM managed.
* 2022-12-16 wrote a blog post Phame > Doing the needful > Shrinking H2 database files==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	Yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	Yes	
People                                        	Were fewer than five people paged?                                                                                	No 	No people was paged at all.
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	No 	No people was paged at all/alerts were disabled.
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	No 	No people was paged at all.
Process                                       	Was the incident status section actively updated during the incident?                                             	No 	
Process                                       	Was the public status page updated?                                                                               	No 	Internal people was kept up to date prudently on both Wikitech-l, Slack and IRC
Process                                       	Is there a phabricator task for the incident?                                                                     	Yes	
Process                                       	Are the documented action items assigned?                                                                         	No 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	Yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	Yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	Yes	Over IRC
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	No 	Monitoring was disabled
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	Yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	No 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2022-11-17,Server and Infrastructure Failures,high,Service Downtime,
"eqsin is connected to the core DCs via two transport links, one of them has been suffering a long fiber cut (see T322529) the other one went down due to a planned maintenance from the transport provider. For ~11min (+ the time user's DNS resolvers pick up eqsin depool, long tail up to 30min) users normally redirected to eqsin (mostly in the APAC region) were only able to read Wikipedia pages already cached in eqsin.",,OPTIONAL: General conclusions (bullet points or narrative),"Create a backup GRE tunnel - https://phabricator.wikimedia.org/T327265
* Ensure the long down transport link comes back up properly once fixed - https://phabricator.wikimedia.org/T322529
* Automatically parse maintenance notifications and alert on conflicting maintenance - https://phabricator.wikimedia.org/T230835Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard)  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	no, 2 of the 3 pages escalated to batphone
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	same as above
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	yes	https://www.wikimediastatus.net/incidents/h3kkhqf88msr
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	T328354
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	There was no open task but one could be open as soon as we receive the maintenance email from the provider.
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	11",2023-01-10,Server and Infrastructure Failures,mid,Degraded System Performance,
,Automated monitoring detected the alert but a human noticed the outage and triaged it with the alert.,"To prevent similar incidents from happening in the future, the team reviewed their upgrade and alerting procedures to ensure that all necessary checks and tests are performed before updates are applied in production.","ThanosCompactHalted error on overlapping blocks. Find and nuke the non-aligned blocks. T335406
* Ensure that the replica label is set for all Prometheus hosts. Make puppet fail when replica=unset. T335406
* Alert when no data is received from Prometheus in a certain amount of time. T336448
* Update the migration procedure on Wikitech. T309979==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-05-05,,,,
"Impact: For about 20 minutes, almost all content page parses were broken. They received, along with any other user interaction with the wb_items_per_site table in some way, a database error message instead of page content. This affected many (but not all or even most) page views for logged-in users, and many (possibly most, but not all) edits made to wikis. For 19 hours, all users looking at pages rendered since the incident start received some incorrect metadata on pageviews (infobox content, cross-language links, wikdata item relation). Also, Wikidata's checks for duplicate site links failed, leading to many hundreds of duplicate items being created.Cause: Wikidata's wb_items_per_site secondary table, used for the rendering of every page connected to Wikidata, was dropped by a mis-configured weekly cron script which executed the update.php code path, which itself had been misconfigured for eight years to drop this table. This immediately led to a DBQueryError on content page loads (reads). The table was re-created as blank, at which point pages began to paint again (though wrongly).","First report to #wikimedia-operations at 23:03:57 UTC <NotASpy> just got an error - [Xou1IQpAIDEAAC74T5wAAAOQ] 2020-04-06 23:03:09: Fatal exception of type ""Wikimedia\Rdbms\DBQueryError"" (log)
* icinga-wm reported problems a minute later at 23:05:33 UTC <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops* There was not an alert that indicated this was happening for most parser cache renders, or that there was a high rate (>700/second) of Mediawiki serving 50X errors to the cache servers .  There should have been automated detection of such a condition, including paging SRE -- if this had happened at off-hours, it could have gone unnoticed by engineers for far too long.",,"Remove ALTER and DROP permissions from the wikiadmin user (which is used by automated crons).  Create a separate user for humans performing schema changes. https://phabricator.wikimedia.org/T249683
** Dumps processes probably shouldn't run with a SQL user that can alter any data, let alone drop tables.
** Potentially, permissions should be much more fine-grained:  separate users for maintenance scripts/crons; for testing ro queries; one for dumps generation; core vs extensions maintenance, etc.
** Maybe DROP TABLE should not granted to anyone except DBAs?
** Maybe ""Database::dropTable"" should check for production environment (similar to update.php) and fail if something like that happens.
* sql.php must re-implement update.php's not-in-production checks if we're going to restore it.
* Having both mwscript sql.php and sql (aka mwscript mysql.php) which run very different code paths is confusing. Fix this.
* Change Wikidata cronjob to use mwscript mysql.php instead of mwscript sql.php: gerrit:587218
* sql.php must not run LoadExtensionSchemaUpdates hooks: phab:T157651
* Pursue restructuring Wikibase repository's SQL directory: phab:T205094
* Consider adding paging based on ats-be-perceived 50X error rate, which considering recent history looks to be a good signal. TODO make a task
* Wikibase schema updaters must not modify database directly phab:T249598
TODO: Add the #Wikimedia-Incident Phabricator tag to these tasks and move them to the ""Follow-up"" column.",2020-04-07,Schema and Database Issues,high,Data Loss or Corruption,
"On Friday September 13, map servers were saturating CPU due to some badly formed requests that were not validated properly by the service. This led to partial unavailability of maps from ~4:30 UTC to ~14:30 UTC. Situation was resolved by validating traffic at the caching layer.","HTTP availability for Varnish was flapping starting 4:26 UTC, getting worse by 6:49 UTC
* No page was sent, no direct alert pointing to maps / kartotherian explicitly","A bug was introduced when fixing linting issues to introduce the CI into the CI pipeline, this created a failure in the HTTP error handler making Kartotherian unable to validate request parameters that leads to high CPU cost and timeout. This needs to be addressed in Kartotherian itself ().The deploy of the code containing the bug occurred September 12 at 21:09 UTC.The amount of support we have on maps does not match the exposure of the service. While the few people working on maps are dedicated to their work and doing their best, we have too many (bad) surprises. The technical stack has many known and unknown issues and our knowledge of that stack is insufficient.The majority of maps traffic comes from other websites or apps reusing our tiles. This is allowed (at least to some extend) by Maps Terms of Use and was the original intent of the project. Given the amount of support we have at the moment, this might need to be revisited.","Fix HTTP error handler in kartotherian -  (code merged, but needs to be tested and deployed)
*Improve testing tin kartotherian endpoints
* Review the amount of support Maps has in regard of its visibility and use casesCategory:Maps outages, 2019",2019-09-13,Performance and Load Issues,mid,Service Downtime,
"Due to a uptream bug introduced in a firejail update, Thumbor constantly restarting itself. This lead to increased error rates, and also increased delays from the HAProxy in front of it.","Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?Copy the relevant alerts that fired in this section.Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?TODO: If human only, an actionable should probably be to ""add alerting"".",,"Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard)  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the incident status section actively updated during the incident?                                             		
Process                                       	Was the public status page updated?                                                                               		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-07-10,External Dependencies,mid,Degraded System Performance,
"On May 5th, 2020, all Wikidata Query Service instances were down for about 5 minutes due to a failed deployment. The Failed deployment was a result of two factors - an incorrectly executed deployment procedure and a bug in the code. Deployment that day was itself a result of a failed deployment the day before (May 4th), which in turn was caused by double jars present in the deploy repo.Bug in the code that ultimately brought down the service (Blazegraph) was the removal of serialVersionUID field from WKTSerializer class. It turned out to be needed by Blazegraph, but that that need wasn't caught by standard integration/unit tests. Effect on the instances updated with faulty code was that Blazegraph (WDQS's backend) couldn't start.Deployment was stopped after some number of instances started to fail and the previous version was rolled back. Because of how WDQS metrics are presented (all of the errors are rolled into one metric, even ""standard"" ones like user throttling), we don't know the exact number of queries affected, but metrics do not show significant impact. In any case, visibility here is problematic and needs to be addressed).","The issue was detected by icinga and the alert was posted on #wikimedia-operations. 
 PROBLEM - Check systemd state on wdqs1003 is CRITICAL: CRITICAL - degraded: The system is operational but one or more units failed. https://wikitech.wikimedia.org/wiki/Monitoring/check_systemd_state
09:43 PROBLEM - Query Service HTTP Port on wdqs2003 is CRITICAL: HTTP CRITICAL: HTTP/1.1 500 Server Error - 9597 bytes in 0.006 second response time https://wikitech.wikimedia.org/wiki/Wikidata_query_service
09:43 PROBLEM - Query Service HTTP Port on wdqs2001 is CRITICAL: HTTP CRITICAL: HTTP/1.1 500 Server Error - 9597 bytes in 0.008 second response time https://wikitech.wikimedia.org/wiki/Wikidata_query_service
09:44 PROBLEM - Check systemd state on wdqs2003 is CRITICAL: CRITICAL - degraded: The system is operational but one or more units failed. https://wikitech.wikimedia.org/wiki/Monitoring/check_systemd_state
09:45 PROBLEM - PyBal backends health check on lvs2010 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2002.codfw.wmnet are marked down but pooled: wdqs-ssl_443: Servers wdqs2002.codfw.wmnet are marked down but pooled: wdqs-internal_80: Servers wdqs2004.codfw.wmnet are marked down but pooled: wdqs_80: Servers wdqs2002.codfw.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal
09:45 PROBLEM - LVS HTTP codfw IPv4 #page on wdqs.svc.codfw.wmnet is CRITICAL: HTTP CRITICAL: HTTP/1.1 500 Server Error - 9597 bytes in 0.078 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
09:45 PROBLEM - Check systemd state on wdqs2001 is CRITICAL: CRITICAL - degraded: The system is operational but one or more units failed. https://wikitech.wikimedia.org/wiki/Monitoring/check_systemd_stateFortunately, preexisting warnings were enough to stop deployment and rollback servers already updated with the faulty version.As for the faulty version itself, since manual tests weren't run after the canary (what should've happen), we didn't actually know that service was faulty. There are no automatic tests during deployment that showed us that that service didn't work properly.","Two main things that happened during the deployment:
* the issue with the faulty version wasn't noticed before the deployment
The problem here is that the version of WDQS was faulty from the start - it wouldn't run properly after updating on any server. Unfortunately, we do not have CI/CD test servers that would have allowed us to spot the issue sooner. It seems that having automated deployments of each master version would be beneficial - we could get information if something fails much faster, e.g. by having a set of smoke tests executed there (we already have such a suite). It is worth it to point out that we already have a server (wdqs1009) that is being updated automatically, but only with the deploy repo version and we tend to deploy immediately after updating the version on that one. Version update with the master version would be more beneficial in scenarios like this one.
* tests were not run after canary deployment
During the deployment there is a step when the process halts - after deploying a canary (single server). The deployer should at this point run the manual tests to verify correctness - that didn't happen. That was caused by human error, but there is no reason why tests - which are normally executed by a person - couldn't be executed as a part of the deployment process.After the outage, in turn, it was hard to precisely assess the impact. While graphs (dashboard) show error rates, they also count in errors that normally happen (like rejections because of throttling).","Update documentation to describe manual testing procedure after canary deployment DONE
* Create automatically updated CI test environment (phab:T252503)
* Include smoke tests as a part of deployment procedure (phab:T252504)
* Improve outage visibility to see actual impact of an outage (phab:T252508)",2020-05-05,Configuration and Deployment Errors,mid,Service Downtime,
"An increase in load on a database server resulted in many queries being much slower to respond. This in turn meant backend traffic occupies appserver php-fpm workers for much longer, and a proportion of those requests will fail entirely due to unavailable workers. The failed requests got an error page with the message ""upstream connect error or disconnect/reset before headers. reset reason: overflow"".Impact: For 37 minutes, backends were slow (taking several seconds to respond) and 2% of requests failed entirely. This affected logged-in users, most bots/API queries, and some page views from unregistered users for pages that were recently edited or otherwise expired from the CDN cache.Documentation:* Public task about incident. T290373*Grafana: Application RED dashboardFile:4-Sep-2021-http-status.png|HTTP error rates.File:4-Sep-2021-latency.png|Latency buckets.File:4-Sep-2021-latency-quantile.png|Latency quantile estimates.",,,T277416 (restricted),2021-09-04,Performance and Load Issues,mid,Degraded System Performance,
"On 14 April, a refactor of mediawiki-BagOStuff was deployed which introduced a bug that caused revision text blobs to no longer be cached in Memcached. Over a 7-day period, this gradually increased connection and query load on ExternalStore DB servers. On 22 April, the last revision-text blobs expired from Memcached, but we remained just under the DB load limits. Around 13:00 UTC on 29 April, an unrelated increase in backend traffic that we are normally able to handle just fine, resulted in further unthrottled demand on ExternalStore DB. We were now over the load limits, and thus until the incident resolution around 18:00 UTC, there were numerous one-minute spikes where 1-10% of appserver requests responded with HTTP 5xx errors. Impact: During a period of five hours (13:00-18:00 UTC) there were 18 distinct minutes during which the error rate was severely elevated above the normal <0.1%, with various minutes having had 1-10% of backend requests responded to with errors.","Grafana: MySQL load (queries, connections)
* Grafana: WANObjectCache stats (cache misses)
* Grafana: Application Servers (traffic, latencies, error rate)
* T281480: SqlBlobStore no longer caching blobs, DBConnectionError Too many connections",,"secondary metric -database qps- monitoring improvement)
* Revert https://gerrit.wikimedia.org/r/c/operations/puppet/%2B/683682/ (not the cause)",2021-04-29,Code and Application Bugs,high,Degraded System Performance,
"Applying a change to our cirrus elasticsearch systemd unit triggered puppet to automatically restart the associated elasticsearch services. Sufficient number of hosts (6) were having puppet-agent ran simultaneously to drop the cluster into yellow and then red status; the large bandwidth/resources required to reschedule and assign shards when the respective elasticsearch services came back up on each host likely contributed to general cluster instability leading to a cascading failure scenario.Elasticsearch itself did its job in coming back and immediately recovering shards, but given the volume of hosts restarted, it took about 40 minutes for the impact to users to become largely invisible, and 2 hours and 5 minutes for a full recovery of the backend (i.e. green cluster status).* 16:18 Puppet change touching cirrus elasticsearch systemd units merged: https://gerrit.wikimedia.org/r/c/operations/puppet/+/720667* 16:23 operator runs puppet manually on elastic2052, confirmed elasticsearch service came back up properly* 16:25 operator runs puppet on rest of fleet, 6 hosts at a time: sudo cumin -b 6 'P{elastic*}' 'sudo run-puppet-agent'* 16:29 PROBLEM - PyBal backends health check on lvs1015 is CRITICAL: PYBAL CRITICAL - CRITICAL - search-psi-https_9643: Servers elastic1059.eqiad.wmnet, elastic1049.eqiad.wmnet, elastic1044.eqiad.wmnet, elastic1048.eqiad.wmnet, elastic1052.eqiad.wmnet, elastic1047.eqiad.wmnet, elastic1067.eqiad.wmnet, elastic1035.eqiad.wmnet, elastic1045.eqiad.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal* 16:31 ElasticSearch health check for shards on 9243 on search.svc.codfw.wmnet is CRITICAL. At the same time, production-search-codfw has dropped into red cluster status* 17:09 enwiki searches begin to work again (presumably due to recovery of the associated enwiki shards). User-visible impact decreases.* 18:xx codfw enters green cluster status (full recovery)* 18:xx eqiad enters green cluster status (full recovery)* 18:30 MediaWiki api_appserver latency returns to normal levels* 18:34 PoolCounter stops rejecting requests, user-visible impact ends.Impact: For about 2 hours (from 16:29 until 18:34) search requests on en.wikipedia.org (and likely other wikis) failed with ""An error has occurred while searching: Search is currently too busy. Please try again later."". Search suggestions (from API opensearch) were absent or delayed. During the incident, the api_appserver cluster saw higher average latency overall due to the proportion of search queries.Documentation:* Impact to shards during incident: https://grafana.wikimedia.org/d/000000455/elasticsearch-percentiles?viewPanel=64&orgId=1&from=1631549942000&to=1631558700000&var-cirrus_group=codfw&var-cluster=elasticsearch&var-exported_cluster=production-search&var-smoothing=1*Impact on API opensearch queries: https://grafana.wikimedia.org/d/000000559/api-requests-breakdown?orgId=1&var-metric=&var-module=opensearch&from=1631521577658&to=1631588860150* Impact on per-host CPU load averages during incident: https://grafana.wikimedia.org/d/000000455/elasticsearch-percentiles?viewPanel=63&orgId=1&from=1631549942000&to=1631558700000&var-cirrus_group=codfw&var-cluster=elasticsearch&var-exported_cluster=production-search&var-smoothing=1* Impact to PoolCounter rejections (proxy for user impact): https://grafana.wikimedia.org/d/qrOStmdGk/elasticsearch-pool-counters?orgId=1&from=1631549942000&to=1631558700000* #wikimedia-operations log",,,Iron out procedure to roll out cirrus/elasticsearch changes involving [implicitly via puppet or explicitly] service restarts,2021-09-13,Configuration and Deployment Errors,high,Service Downtime,
A sudden spike in requests to shellbox-syntaxhighlight overloaded the service leading to slow response times and increased failures.The majority of requests where originating from jobrunners and api_appservers.,"Initially detected by a human spotting alerts on IRC, closely followed by a page.",,"Figure out what caused the burst (there's a suggestion a template was changed leading to a lot of pages needing re-rendering at once)
* Document how to scale up shellbox runners? (or link to it from Shellbox)==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	Yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	Yes	
People                                        	Were fewer than five people paged?                                                                                	Yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	Yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	Yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	No 	no google doc was created, only 3 responders
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	No 	
Process                                       	Is there a phabricator task for the incident?                                                                     	Yes	
Process                                       	Are the documented action items assigned?                                                                         	No 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	No 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	No 	While they might not have fully prevented this incident, there are (long) outstanding performance issues with syntaxhighlight: ,
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	Yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	Yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	Yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	No 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	9",2022-12-21,Performance and Load Issues,mid,Degraded System Performance,
"The sessionstore service suffered an outage that lead to the inability of logged-in users to submit edits. The root cause of the outage was insufficient capacity to respond to a sudden increase of requests reaching mediawiki.TODO: These are raw numbers, to be used as input to a calculation of actual user impact that hasn't been performed yet. More to come here.Sessionstore impact: About 32 million requests were lost between 18:36 and 19:20. Based on the pre-incident steady state of about 85% 404s from sessionstore, about 4.8 million requests were lost that would have returned with 2xx status. (source). It's important to point out that 404s from sessionstore are from readers and hence are totally expected. TODO: Clarify why the error rate is normally so high, and whether it's the total or 2xx requests that reflect actual impact here.MediaWiki-reported save failures: A total of 8,953 during the same period (all edit.failures.session_loss), which is unrealistically low, given that we believe all logged-in edits failed during the outage. (source)Deficit in MediaWiki-reported edits: The trough in successful edits between 18:36 and 19:00 can be seen in MediaWiki-reported stats, as well as an increase from 19:00 to about 20:30, as users—presumably both humans and bots—retried their edits that failed during the outage. During the outage, the deficit is about 24,000 edits; for the total window from 18:36 to 20:30, the deficit is about 18,000 edits (that is, about 6k edits were effectively delayed rather than dropped). (source)Editors were affected on all wikis in all geographic regions by being unable to edit, login or logout of the sites. Readers were completely unaffected.","Detection was automated, with the first IRC alert about five minutes after the kask pods initially crashed, and the first (and only) page about 30 seconds later. Icinga's full transcript from #wikimedia-operations is below, comprising 39 total alerts.Note that the ""MediaWiki exceptions and fatals per minute"" alerts persisted, spuriously, for some hours after the underlying problem was solved. This was due to Logstash's delay in processing MediaWiki log entries: the alert reacts to the rate of log entry ingestion, not log entry production, so when Logstash is behind, the alert is behind too.18:41:46 <icinga-wm> PROBLEM - Prometheus jobs reduced availability on icinga1001 is CRITICAL: job={swagger_check_eventgate_analytics_cluster_eqiad,swagger_check_eventgate_analytics_external_cluster_eqiad,swagger_check_eventgate_main_cluster_eqiad,swagger_check_mathoid_cluster_eqiad,swagger_check_sessionstore_eqiad} site=eqiad https://wikitech.wikimedia.org/wiki/Prometheus%23Prometheus_job_unavailable https://grafana.wikimedia.org/d/NEJu05xZz/prometh
18:42:08 <icinga-wm> PROBLEM - PyBal backends health check on lvs1015 is CRITICAL: PYBAL CRITICAL - CRITICAL - sessionstore_8081: Servers kubernetes1001.eqiad.wmnet, kubernetes1003.eqiad.wmnet, kubernetes1005.eqiad.wmnet, kubernetes1006.eqiad.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal
18:42:19 <icinga-wm> PROBLEM - LVS sessionstore eqiad port 8081/tcp - Session store- sessionstore.svc.eqiad.wmnet IPv4 #page on sessionstore.svc.eqiad.wmnet is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:42:20 <icinga-wm> PROBLEM - Cxserver LVS eqiad on cxserver.svc.eqiad.wmnet is CRITICAL: /v1/mt/{from}/{to}{/provider} (Machine translate an HTML fragment using TestClient.) timed out before a response was received: / (root with wrong query param) timed out before a response was received: /v1/dictionary/{word}/{from}/{to}{/provider} (Fetch dictionary meaning without specifying a provider) timed out before a response was received: /v2/suggest/source
18:42:20 <icinga-wm> ggest a source title to use for translation) timed out before a response was received: /v1/list/pair/{from}/{to} (Get the tools between two language pairs) timed out before a response was received: /_info/name (retrieve service name) timed out before a response was received: /v1/page/{language}/{title}{/revision} (Fetch enwiki protected page) timed out before a response was received https://wikitech.wikimedia.org/wiki/CX
18:42:22 <icinga-wm> PROBLEM - restbase endpoints health on restbase1021 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:42:26 <icinga-wm> PROBLEM - restbase endpoints health on restbase1027 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:42:26 <icinga-wm> PROBLEM - restbase endpoints health on restbase-dev1005 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:42:28 <icinga-wm> PROBLEM - PyBal backends health check on lvs1016 is CRITICAL: PYBAL CRITICAL - CRITICAL - sessionstore_8081: Servers kubernetes1003.eqiad.wmnet, kubernetes1004.eqiad.wmnet, kubernetes1005.eqiad.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal
18:42:28 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
18:42:38 <icinga-wm> PROBLEM - LVS wikifeeds eqiad port 8889/tcp - A node webservice supporting featured wiki content feeds. termbox.svc.eqiad.wmnet IPv4 on wikifeeds.svc.eqiad.wmnet is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:42:38 <icinga-wm> PROBLEM - Citoid LVS eqiad on citoid.svc.eqiad.wmnet is CRITICAL: /api (bad URL) timed out before a response was received: /api (Zotero and citoid alive) timed out before a response was received https://wikitech.wikimedia.org/wiki/Citoid
18:42:42 <icinga-wm> PROBLEM - LVS echostore eqiad port 8082/tcp - Echo store- echostore.svc.eqiad.wmnet IPv4 on echostore.svc.eqiad.wmnet is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:42:44 <icinga-wm> PROBLEM - restbase endpoints health on restbase1018 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:42:54 <icinga-wm> PROBLEM - eventgate-main LVS eqiad on eventgate-main.svc.eqiad.wmnet is CRITICAL: / (root with no query params) timed out before a response was received: / (root with wrong query param) timed out before a response was received https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:42:56 <icinga-wm> PROBLEM - eventgate-logging-external LVS eqiad on eventgate-logging-external.svc.eqiad.wmnet is CRITICAL: / (root with no query params) timed out before a response was received: / (root with wrong query param) timed out before a response was received: /robots.txt (robots.txt check) timed out before a response was received https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:43:02 <icinga-wm> PROBLEM - restbase endpoints health on restbase-dev1004 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:43:14 <icinga-wm> PROBLEM - Host kubernetes1003 is DOWN: PING CRITICAL - Packet loss = 100%
18:43:18 <icinga-wm> PROBLEM - eventgate-analytics-external LVS eqiad on eventgate-analytics-external.svc.eqiad.wmnet is CRITICAL: WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None)) after connection broken by ConnectTimeoutError(urllib3.connection.VerifiedHTTPSConnection object at 0x7efce6308518, Connection to eventgate-analytics-external.svc.eqiad.wmnet timed out. (connect timeout=15)): /?spec https://wi
18:43:18 <icinga-wm> org/wiki/Event_Platform/EventGate
18:43:48 <icinga-wm> PROBLEM - Host kubernetes1005 is DOWN: PING CRITICAL - Packet loss = 100%
18:43:56 <icinga-wm> PROBLEM - Host kubernetes1001 is DOWN: PING CRITICAL - Packet loss = 100%
18:44:08 <icinga-wm> RECOVERY - restbase endpoints health on restbase1021 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:44:12 <icinga-wm> RECOVERY - restbase endpoints health on restbase1027 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:44:12 <icinga-wm> RECOVERY - restbase endpoints health on restbase-dev1005 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:44:14 <icinga-wm> RECOVERY - Host kubernetes1003 is UP: PING WARNING - Packet loss = 33%, RTA = 47.03 ms
18:44:18 <icinga-wm> RECOVERY - LVS wikifeeds eqiad port 8889/tcp - A node webservice supporting featured wiki content feeds. termbox.svc.eqiad.wmnet IPv4 on wikifeeds.svc.eqiad.wmnet is OK: HTTP OK: HTTP/1.1 200 OK - 945 bytes in 0.003 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:44:20 <icinga-wm> RECOVERY - Host kubernetes1005 is UP: PING OK - Packet loss = 0%, RTA = 0.19 ms
18:44:20 <icinga-wm> RECOVERY - LVS echostore eqiad port 8082/tcp - Echo store- echostore.svc.eqiad.wmnet IPv4 on echostore.svc.eqiad.wmnet is OK: HTTP OK: Status line output matched 200 - 258 bytes in 0.016 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:44:22 <icinga-wm> RECOVERY - Citoid LVS eqiad on citoid.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Citoid
18:44:28 <icinga-wm> RECOVERY - restbase endpoints health on restbase1018 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:44:28 <icinga-wm> RECOVERY - eventgate-analytics-external LVS eqiad on eventgate-analytics-external.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:44:34 <icinga-wm> RECOVERY - Host kubernetes1001 is UP: PING OK - Packet loss = 0%, RTA = 0.18 ms
18:44:34 <icinga-wm> RECOVERY - eventgate-main LVS eqiad on eventgate-main.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:44:38 <icinga-wm> RECOVERY - eventgate-logging-external LVS eqiad on eventgate-logging-external.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:44:44 <icinga-wm> RECOVERY - restbase endpoints health on restbase-dev1004 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:45:04 <icinga-wm> PROBLEM - Host kubernetes1003 is DOWN: PING CRITICAL - Packet loss = 100%
18:46:10 <icinga-wm> RECOVERY - Host kubernetes1003 is UP: PING OK - Packet loss = 0%, RTA = 54.98 ms
18:47:04 <icinga-wm> PROBLEM - MediaWiki edit session loss on graphite1004 is CRITICAL: CRITICAL: 60.00% of data above the critical threshold [50.0] https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/dashboard/db/edit-count?panelId=13&fullscreen&orgId=1
18:47:28 <icinga-wm> PROBLEM - k8s API server requests latencies on argon is CRITICAL: instance=10.64.32.133:6443 verb=LIST https://wikitech.wikimedia.org/wiki/Kubernetes https://grafana.wikimedia.org/dashboard/db/kubernetes-api
18:49:18 <icinga-wm> RECOVERY - k8s API server requests latencies on argon is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Kubernetes https://grafana.wikimedia.org/dashboard/db/kubernetes-api
18:52:10 <icinga-wm> PROBLEM - Host kubernetes1003 is DOWN: PING CRITICAL - Packet loss = 100%
18:54:16 <icinga-wm> PROBLEM - Host kubernetes1001 is DOWN: PING CRITICAL - Packet loss = 100%
18:54:46 <icinga-wm> PROBLEM - Host kubernetes1005 is DOWN: PING CRITICAL - Packet loss = 100%
18:55:06 <icinga-wm> RECOVERY - Host kubernetes1003 is UP: PING OK - Packet loss = 0%, RTA = 0.25 ms
18:55:06 <icinga-wm> RECOVERY - Host kubernetes1001 is UP: PING OK - Packet loss = 0%, RTA = 0.23 ms
18:55:14 <icinga-wm> PROBLEM - Cxserver LVS eqiad on cxserver.svc.eqiad.wmnet is CRITICAL: /v2/suggest/source/{title}/{to} (Suggest a source title to use for translation) timed out before a response was received https://wikitech.wikimedia.org/wiki/CX
18:55:16 <icinga-wm> PROBLEM - Too many messages in kafka logging-eqiad on icinga1001 is CRITICAL: cluster=misc exported_cluster=logging-eqiad group={logstash,logstash-codfw,logstash7-codfw,logstash7-eqiad} instance=kafkamon1001:9501 job=burrow partition={0,1,2,3,4,5} site=eqiad topic=udp_localhost-err https://wikitech.wikimedia.org/wiki/Logstash%23Kafka_consumer_lag https://grafana.wikimedia.org/d/000000484/kafka-consumer-lag?from=now-3h&to=now&orgI
18:55:16 <icinga-wm> e=eqiad+prometheus/ops&var-cluster=logging-eqiad&var-topic=All&var-consumer_group=All
18:55:18 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
18:55:20 <icinga-wm> RECOVERY - Host kubernetes1005 is UP: PING OK - Packet loss = 0%, RTA = 0.18 ms
18:58:34 <icinga-wm> PROBLEM - Host kubernetes1001 is DOWN: PING CRITICAL - Packet loss = 100%
18:58:38 <icinga-wm> PROBLEM - Host kubernetes1006 is DOWN: PING CRITICAL - Packet loss = 100%
18:58:48 <icinga-wm> RECOVERY - Cxserver LVS eqiad on cxserver.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/CX
18:59:02 <icinga-wm> RECOVERY - Host kubernetes1006 is UP: PING OK - Packet loss = 0%, RTA = 0.23 ms
18:59:04 <icinga-wm> RECOVERY - Host kubernetes1001 is UP: PING OK - Packet loss = 0%, RTA = 0.21 ms
19:00:48 <icinga-wm> RECOVERY - MediaWiki exceptions and fatals per minute on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:02:18 <icinga-wm> RECOVERY - PyBal backends health check on lvs1015 is OK: PYBAL OK - All pools are healthy https://wikitech.wikimedia.org/wiki/PyBal
19:02:21 <icinga-wm> RECOVERY - LVS sessionstore eqiad port 8081/tcp - Session store- sessionstore.svc.eqiad.wmnet IPv4 #page on sessionstore.svc.eqiad.wmnet is OK: HTTP OK: Status line output matched 200 - 258 bytes in 0.012 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
19:02:36 <icinga-wm> RECOVERY - PyBal backends health check on lvs1016 is OK: PYBAL OK - All pools are healthy https://wikitech.wikimedia.org/wiki/PyBal
19:03:44 <icinga-wm> RECOVERY - Prometheus jobs reduced availability on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Prometheus%23Prometheus_job_unavailable https://grafana.wikimedia.org/d/NEJu05xZz/prometheus-targets
19:04:26 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:06:02 <icinga-wm> PROBLEM - High average GET latency for mw requests on api_appserver in codfw on icinga1001 is CRITICAL: cluster=api_appserver code=200 handler=proxy:unix:/run/php/fpm-www.sock https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=codfw+prometheus/ops&var-cluster=api_appserver&var-m
19:10:02 <icinga-wm> PROBLEM - High average GET latency for mw requests on appserver in codfw on icinga1001 is CRITICAL: cluster=appserver code=200 handler=proxy:unix:/run/php/fpm-www.sock https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=codfw+prometheus/ops&var-cluster=appserver&var-method=GET
19:11:28 <icinga-wm> RECOVERY - High average GET latency for mw requests on api_appserver in codfw on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=codfw+prometheus/ops&var-cluster=api_appserver&var-method=GET
19:11:50 <icinga-wm> RECOVERY - High average GET latency for mw requests on appserver in codfw on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=codfw+prometheus/ops&var-cluster=appserver&var-method=GET
19:18:54 <icinga-wm> RECOVERY - MediaWiki exceptions and fatals per minute on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:23:22 <icinga-wm> RECOVERY - MediaWiki edit session loss on graphite1004 is OK: OK: Less than 30.00% above the threshold [10.0] https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/dashboard/db/edit-count?panelId=13&fullscreen&orgId=1
19:24:18 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:42:26 <icinga-wm> RECOVERY - MediaWiki exceptions and fatals per minute on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:43:14 <icinga-wm> PROBLEM - Logstash rate of ingestion percent change compared to yesterday on icinga1001 is CRITICAL: 413.5 ge 210 https://phabricator.wikimedia.org/T202307 https://grafana.wikimedia.org/dashboard/db/logstash?orgId=1&panelId=2&fullscreen
19:46:02 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:55:07 <icinga-wm> PROBLEM - Citoid LVS eqiad on citoid.svc.eqiad.wmnet is CRITICAL: /api (bad URL) timed out before a response was received: /api (Zotero and citoid alive) timed out before a response was received https://wikitech.wikimedia.org/wiki/Citoid
19:56:11 <icinga-wm> RECOVERY - Citoid LVS eqiad on citoid.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Citoid
20:04:21 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/opsA lot of alerts that fired were unrelated to sessionstore. However the total volume (39) was manageable. The first alerts pinpointed the problematic service accurately.",,"Increase Logstash ingestion capacity / handle logspam situations better https://phabricator.wikimedia.org/T255243
* Increase capacity of the sessionstore dedicated kubernetes nodes https://phabricator.wikimedia.org/T256236 
* Increase kubernetes capacity overall https://phabricator.wikimedia.org/T252185 (codfw) and https://phabricator.wikimedia.org/T241850 (eqiad) 
* Investigate the iowait issues plaguing kubernetes nodes since 2020-05-29. https://phabricator.wikimedia.org/T255975 
* Investigate the apparent network connectivity loss for kubernetes100[1,3,5] during the incident
* Investigate adding resource utilization alerts to services hosted on kubernetes
* Adopt SLIs/SLOs for sessionstore https://phabricator.wikimedia.org/T256629",2020-06-11,Performance and Load Issues,high,Service Downtime,
"A stuck vrts aliases generating process on mx2001 resulted in rejects for dcw@wikimedia.org, a new VRTS queue.On 2022-02-02 an SRE with long-time knowledge about VRTS received an email to their individual work address from a known VRTS admin, stating that a newly created VRTS queue ""dcw@wikimedia.org"" returned errors to some users that tried to use it (but not always, e.g. manual testing worked fine). The errors were of type SMTP 550 Error and looked as follows: 208.80.153.45 does not like recipient. Remote host said: 550 Previous (cached) callout verification failureA few hours later (by 2022-02-22 13:29), an investigation independently verified that email would not always be reliably sent to this VRTS email queue and the issue was escalated to a couple of other knowledgeable SREs. Given the incoming path and the fact that the only failing email address was a relatively new one not yet in widespread use, the incident was implicitly triaged as low priority. By 14:35 UTC it was verified again, adding more data points and a first theory formulated that our Google work email system was at fault as emails from other MTAs were sent out successfully but sending from wikimedia.org domains failed. However, by 16:47 UTC, it became clear that the generate_otrs_aliases.service systemd timer job was stuck and was not updating VRTS mailing lists/queues on mx2001 while it was running fine on mx1001 (that discrepancy explains why it was sometimes reproducible). After restart of the systemd timer job, the issue was fixed and the fix communicated to the VRTS admin.",,,"Figure out why generate_otrs_aliases.service was stuck.
*Alert on a stuck generate_otrs_aliases.service.TODO: Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard) Phabricator tag to these tasks.==Scorecard==           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	1    	
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	N/A  	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	N/A  	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	N/A  	
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	N/A  	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	N/A  	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	0    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	0    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	0    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	0    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	1    	
Total score	Total score                                                                                                                                              	4",2022-02-22,Code and Application Bugs,low,Degraded System Performance,
"Increased mediawiki logging  led to eventgate-analytics congestion, starving the api_appservers of idle workers. No user-facing impact.","Issue detected through monitoring of api_appserver idle starvation. The alerts were accurate as to the symptom, but not the actual cause (eventgate-external pods getting CPU throttled).",,"CR Bumping the number of replicas https://gerrit.wikimedia.org/r/c/operations/deployment-charts/+/866612
* CR Reverting the logging increase https://gerrit.wikimedia.org/r/c/mediawiki/core/+/864722
* CR Reverting the replica increase https://gerrit.wikimedia.org/r/c/operations/deployment-charts/+/867597
* An investigation into uneven load-balancing may be warranted https://phabricator.wikimedia.org/T325068==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	no 	Flamegraph wasn't in the list of tools known by initial responder
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	non user-facing incident
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	I don't know (defaulting to yes if there are no known previous occurrences)
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	https://phabricator.wikimedia.org/T266216
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	9",2022-12-09,Performance and Load Issues,low,Degraded System Performance,
"Current status: resolved =incident coordinator: Ben Tullis	status last updated: n/aResponders: Luca Toscano ,Balthazar Rouberol, Steve Munene, Joseph Allemandou, David Causse (WDQS), Peter Fischer (WDQS)Provide some bullet points communicating:* End users of mediawiki projects are not currently affected** We are seeing mirror makers crashing and restarting, when mirroring topics from main-eqiad to kafka-jumbo.** This may have a knock-on effect on our data processing pipelines, but we are not aware of any data loss at the moment.** Wikidata Query Service updates were lagging during the outage, leading to self-throttling of bots and stale data being returned to users.* The mirror-maker processes start up successfully, but then crash at some time afterwards.* They are restarted by systemd, but continue to crash in this loop.* We have not seen any other mirror-maker processes fail","The issue was first detected by Steve Munene and other engineers checking the #wikimedia-analytics IRC channelWe saw lots of Icinga messages like this:PROBLEM - Kafka MirrorMaker main-eqiad_to_jumbo-eqiad@0 on kafka-jumbo1001 is CRITICAL: PROCS CRITICAL: 0 processes with command name java, regex args kafka.tools.MirrorMaker.+/etc/kafka/mirror/main-eqiad_to_jumbo-eqiad@0/producer\.properties https://wikitech.wikimedia.org/wiki/Kafka/Administration%23MirrorMakerPROBLEM - Kafka MirrorMaker main-eqiad_to_jumbo-eqiad average message consume rate in last 30m on alert1001 is CRITICAL: 0 le 0 https://wikitech.wikimedia.org/wiki/Kafka/Administration%23MirrorMaker https://grafana.wikimedia.org/d/000000521/kafka-mirrormaker?var-datasource=eqiad+prometheus/ops&var-lag_datasource=eqiad+prometheus/ops&var-mirror_name=main-eqiad_to_jumbo-eqiadPROBLEM - Kafka MirrorMaker main-eqiad_to_jumbo-eqiad average message produce rate in last 30m on alert1001 is CRITICAL: 0 le 0 https://wikitech.wikimedia.org/wiki/Kafka/Administration https://grafana.wikimedia.org/d/000000521/kafka-mirrormaker?var-datasource=eqiad+prometheus/ops&var-lag_datasource=eqiad+prometheus/ops&var-mirror_name=main-eqiad_to_jumbo-eqiadWe also saw AlertManager alerts like this:(SystemdUnitCrashLoop) firing: (15)  crashloop on kafka-jumbo1001:9100 - TODO - https://grafana.wikimedia.org/d/g-AaZRFWk/systemd-status - https://alerts.wikimedia.org/?q=alertname%3DSystemdUnitCrashLoop","We think that the root cause is a bug in our version of Kafka and/or Mirrormaker
* This triggers when we increase the number of mirrormaker instances pulling from kafka-main-eqiad to kafka-jumbo to exactly 15 (or probably more)
* There is a proposed workaround, but this requires changing the settings of kafka-main, which we did not want to do.","Upgrade Kafka to 2.x or 3.x
* We could consider whether we want to change the settings on kafka-main but for now we have just selected a smaller number of mirrormakers.
* phab:T347515: better sandboxing of the WDQS updater test instance==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-09-27,Code and Application Bugs,mid,API or Integration Failures,
Logstash indexes were temporarily stale with newer messages from Kafka not yet available. This was caused by TODO.Impact: Monitoring alerts that are based on Logstash missed potential errors. Developers were briefly unable to see the latest messages in Kibana. They may've also been reduced availability in querying older messages from Logstash.,"Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?Copy the relevant alerts that fired in this section.Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?TODO: If human only, an actionable should probably be to ""add alerting"".",What weaknesses did we learn about and how can we address them?,"Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.* To do #1 (TODO: Create task)
* Create spiceracks cookbooks for restarting Logstash's Elastic cluster (we have ones for Cirrus' Elastic cluster) – https://phabricator.wikimedia.org/T255864TODO: Add the #Wikimedia-Incident-Prevention Phabricator tag to these tasks.",2020-06-19,Data Management and Retention,mid,Degraded System Performance,
"When replication broke between local MainStash databases, a previously undiscovered bug in how MediaWiki handles failures of local MainStash replicas, produced user-noticeable exceptions (mainly during edit-related activities).Unlike thought at the time, write failures to the MainStash database were not fully prevented from being noticed by MediaWiki's shutdown handler. ChronologyProtector was (wrongly) enabled for the x2 database section, and so similar to core databases, MainStash required at least one replica to be up and not lagged in order to allow MainStash writes to succeed.Earlier in the day, cross-dc replication for x2 databases broke, this was caused by the application requiring STATEMENT-based replication while the databases were (wrongly) configured with the incompatible ROW-based replication. This caused a split brain state, where eqiad and codfw drifted apart in their respective datasets. This by itself had low to no impact as codfw traffic at the time was restricted to testwiki, test2wiki and mediawiki.org. Service itself would have not been impacted even if codfw received more significiant traffic as MainStash does not require or observe data consistency.In response to alerts about x2 replication breakage, operators tried to fix the then lower priority issue, by running SET GLOBAL sql_slave_skip_counter = X, for values of 1 and 2 a few times. This caused replication to break further, also affecting local replicas within Eqiad. Current understanding of MainStash service only requires the primary DB to be up and tolerates replica failure, so replication breaking or stopping within a datacenter shouldn't cause an outage, as the replicas are meant to be passive standby hosts. However, because ChronologyProtector was still enabled, this caused MainStash to observe the replication lag and thus prevent MainStash writes to the primary DB. These write failures in turn triggered a bug that allowed MainStash write failures to be insufficiently silenced and thus cause user-noticeable error messages in certain editing-related MediaWiki actions. Saving of edits was unaffected (as verified by edit rate metric), although error rates were comparable to edit rates in terms of volume: Explicit transaction still active; a caller might have failed to call endAtomic() or cancelAtomic().While a few other things were attempted, such as wiping out the ephemeral MainStash dataset and resetting replication, the way replication was restored was by:# Switching active write hosts to STATEMENT-based replication.# Disabling GTID (this fixed the cross-dc replication).# Skipping statements on the replicas with CHANGE MASTER until they started replicating STATEMENT binlog positions.Approximately 1 hour of MainStash data from x2 was lost and the servers ended up replicating but with different data each. Because both data and logs were purged at earlier states of debugging, later root cause findings were much harder to find.","alert pages were sent as soon as internal replication broke on the primary x2 servers. This alert was promptly attended.The user-facing outage started later, with more pages about the replicas. Although we're unsure if sufficiently different from the first 2 pages to indicate the severity. User reports reached SRE at this point too, but people were already working on a resolution by then.",,"Switch x2 to statement-based replication, and in general re-review db configuration (disable GTID, read-write, slave_exec_mode)
* Review runbooks/procedures for x2 (and parsercache), specifically regarding replication issues
** E.g. Sometimes doing nothing and having a split brain is better than trying to fix stuff manually (e.g. waiting & depooling a dc, and cloning afterwards)
* Restore replication on codfw replicas, repool them, remove downtimes/silences, recheck read-only mode 
* Validate MW’s concept of multi-master conflict resolution by performing simultaneous writes and simultaneous purges on both DCs
** As far as I was told, UPSERTs seemed to work but purges may need review (DELETE IGNORE?)
* Fix uncaught exception from LoadBalancer::approvePrimaryChanges() which caused total failure rather than graceful failure https://gerrit.wikimedia.org/r/c/mediawiki/core/+/823791
* Remove chronology protector checks so databases don't go into read only if local replicas crash or get replication broken: 
* Re-enable multi-DC mode on testwiki, test2wiki and mediawikiwiki https://gerrit.wikimedia.org/r/c/operations/puppet/+/824039
* Make attempts to mistakenly depool a primary db have a more helpful message: 
* See tasks  (initial replication breakage/multi-dc implications) and  (user visible outage, mw errors, within-dc replication issues) for discussion. See https://docs.google.com/document/d/1_nDpRvLEK9dGI2XVNeZVC1TcA89EYBCptiVCKItlWug for full logs and other data.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	overlap of 4
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	only 1 unassigned
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	11",2022-08-16,Schema and Database Issues,mid,Degraded System Performance,
"A Puppet change seem to have caused an apache restart https://gerrit.wikimedia.org/r/c/operations/puppet/+/798615/ that didn’t work as it made it listen 443 regardless whether mod_ssl is enabled** Alerts: connect to address 10.X.X.X and port 80: Connection refused | CRITICAL - degraded: The following units failed: apache2.service** Puppet was quickly disabled to prevent a site-wide outage (apache failing everywhere affected)** Initial triage was done via cumin: sudo cumin -m async 'mw1396*' 'sed -i"""" ""s/Listen 443//"" /etc/apache2/ports.conf '  'systemctl start apache2 '  as the patch revert didn’t work.** The final fix: https://gerrit.wikimedia.org/r/c/operations/puppet/+/798631/*","The first detection was by the engineer who merged the patch.
* Later IRC alerts and pager.
** <+jinxer-wm> (ProbeDown) firing: Service kibana7:443 has failed probes (http_kibana7_ip4) #page - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown <+icinga-wm> PROBLEM - Apache HTTP on parse2001 is CRITICAL: connect to address 10.192.0.182 and port 80: Connection refused https://wikitech.wikimedia.org/wiki/Application_servers  <+icinga-wm> PROBLEM - Apache HTTP on mw1320 is CRITICAL: connect to address 10.64.32.41 and port 80: Connection refused https://wikitech.wikimedia.org/wiki/Application_servers  <+icinga-wm> PROBLEM - Apache HTTP on mw1361 is CRITICAL: connect to address 10.64.48.203 and port 80: Connection refused https://wikitech.wikimedia.org/wiki/Application_servers","During the Incident Review ritual, it was pointed out that if we had a way to deploy those changes in a controlled environment (e.g. canary) we could have been saved from this one. It was also noted that PCC did not catch this one as puppet complied this fine, it's just the resulting Apache configuration was unconditionally also listening on port 443.","Scorecard==Incident Engagement™  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	No 	At least 4 out of 5 are usual responders (Marostegui, jbond, _joe_, jynus)
People                                        	Were the people who responded prepared enough to respond effectively                                              	Yes	
People                                        	Were fewer than five people paged?                                                                                	No 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	No 	N/A
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	Yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	Yes	
Process                                       	Was the public status page updated?                                                                               	No 	
Process                                       	Is there a phabricator task for the incident?                                                                     	No 	
Process                                       	Are the documented action items assigned?                                                                         	No 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	No 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	No 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	Yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	Yes	
Tooling                                       	Were all engineering tools required available and in service?                                                     	No 	At least Kibana and piwik/matomo where down
Tooling                                       	Was there a runbook for all known issues present?                                                                 	No 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	5",2022-05-24,Configuration and Deployment Errors,high,Service Downtime,
"Switch asw-b2-codfw failed; asw-b-codfw master failed over to b7 as designed. This, however, left all the systems in B2 offline. A volunteer noticed the alerts, and used Klaxon. We were unable to restore asw-b2-codfw to service, and depooled the swift and thanos frontends in B2. This left us operational, but at reduced redundancy. A further complication was that lvs2008 reaches all of row B via asw-b2-codfw, which meant that trying to change things in codfw on Monday was difficult; as a result of which mediawiki in codfw was depooled until asw-b2-codfw could be replaced.","Automated monitoring detected the outage, a human made the decision to Klaxon.<icinga-wm> PROBLEM - Host cp2031 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host ms-be2046 is DOWN: PING CRITICAL - Packet loss =
<icinga-wm> PROBLEM - Host elastic2041 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host kafka-logging2002 is DOWN: PING CRITICAL - Packet
	    loss = 100%
<icinga-wm> PROBLEM - Host mc2043 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host thanos-fe2002 is DOWN: PING CRITICAL - Packet loss
	    = 100%
<icinga-wm> PROBLEM - Host elastic2063 is DOWN: PING CRITICAL - Packet loss =
	    100%  [08:19]
<icinga-wm> PROBLEM - Host cp2032 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host elastic2064 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host elastic2057 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host lvs2008 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host elastic2077 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host elastic2078 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host mc2042 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host ms-fe2010 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host ms-be2041 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host ml-cache2002 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host elastic2042 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - BGP status on cr1-codfw is CRITICAL: BGP CRITICAL -
	    AS64600/IPv4: Connect - PyBal
	    https://wikitech.wikimedia.org/wiki/Network_monitoring%23BGP_status
<icinga-wm> PROBLEM - Router interfaces on cr1-codfw is CRITICAL: CRITICAL:
	    host 208.80.153.192, interfaces up: 127, down: 2, dormant: 0,
	    excluded: 0, unused: 0:
	    https://wikitech.wikimedia.org/wiki/Network_monitoring%23Router_interface_down
<icinga-wm> PROBLEM - Juniper virtual chassis ports on asw-b-codfw is
	    CRITICAL: CRIT: Down: 7 Unknown: 0
	    https://wikitech.wikimedia.org/wiki/Network_monitoring%23VCP_status
<icinga-wm> PROBLEM - BGP status on cr2-codfw is CRITICAL: BGP CRITICAL -
	    AS64600/IPv4: Connect - PyBal
	    https://wikitech.wikimedia.org/wiki/Network_monitoring%23BGP_status
<jinxer-wm> (virtual-chassis crash) firing: Alert for device
	    asw-b-codfw.mgmt.codfw.wmnet - virtual-chassis crash   -
	    https://alerts.wikimedia.org/?q=alertname%3Dvirtual-chassis+crashThe initial DOWN alerts came before alerting picked up the switch failure; in an ideal world we would have picked up the switch failure and not separately alerted about the dependent hosts.It's not clear if a switch failing should page, given we are able to continue without one.",,"Decide whether switch failure should page or not
* Cookbook for rack downtime - https://phabricator.wikimedia.org/T327300Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard)  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	   	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	   	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	   	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	   	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	   	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	   	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-01-14,Server and Infrastructure Failures,high,Service Downtime,
"Widespread service timeouts starting immediately after moving group2 wikis to 1.35.0-wmf.18. Reverting to 1.35.0-wmf.16 immediately brought most services back to life.A single root cause was responsible for two incidents, this one on Thursday the 6th and again on Friday the 7th. See Incident_documentation/20200207-wikidata for more relevant details on this incident.",of icinga alerts fired simultaneously.,"The root cause was a typo in a config setting for Wikibase.On Jan 16  this config change  was deployed. This would have caused all Wikibase client wikis to read items from the new wb terms store. It did not take effect because of a typo in the name elsewhere in the config files. This means that the default setting in the Wikibase extension was used, with all clients reading from the old store.On Jan 22 the default setting in the Wikibase extension was changed to have clients all read from the new store. This did not make it into a branch until wmf.18, because of All-Hands. It went live to groups 0 on Feb 4th, and group 1 on Feb 5th. This would have impacted Commons and Wikidata but they do very few Wikibase client reads compared to the bulk of the wikis.
The change went live to group 2 on Feb 6th, when we saw the outage.A similar but smaller scale incident occurred the next day when trying to fix that config variable typo.
Friday's incident report: Incident documentation/20200207-wikidata.The similarity of the two incidents (onset, type of impact, graphs , ) led us to believe that the root cause of Friday's incident is also the root cause of Thursday's incident. The root cause of Friday's incident was narrowed down to the specific config change because only that single change was deployed at the time of the incident on Friday, which is unlike Thursday when we were routinely deploying a larger batch of changes during the MediaWiki train.The Monday rollout of wmf.18 to group2 without incident confirms this understanding.","T244535 - wikifeeds: Fix the CPU limits so that it doesn't get starved
* When text-esams was down, Grafana was not available to European SREs. Workarounds below were mentioned:
** echo $(dig +short text-lb.eqsin.wikimedia.org) grafana.wikimedia.org | sudo tee -a /etc/hosts 
** ssh grafana1002.eqiad.wmnet -L3000:localhost:3000
* conversations about moving monitoring interfaces outside the normal traffic path (Herron). continue them and turn into a ticket
* T243009 - Make scap skip restarting php-fpm when using --force
* T217924 - Make canary wait time configurable
* T244544 - add a force-revert command to scap to shorten the time it takes to revert
* T244533 - Slow query hitting commonswiki
* T183999 - scap canary has a shifting baseline (scap, why did canaries not catch the bad deploy (tangentially related))
* Consider moving to more of a continuous deployment model with only groups of related changes being deployed together (User:20after4 is thinking about this)
* As an underling problem. Typos are really easy to happen on mediawiki-config.
**T183999 - Define variant Wikimedia production config in compiled, static files
**T220775 - Consider creating a puppet-compiler equivalent for mediawiki-config.git",2020-02-06,Code and Application Bugs,high,Service Downtime,
"An Nginx server restart (RC) triggered an etcdmirror outage that started affecting end-users during a subsequent MediaWiki deployment. The etcd outage led to php-fpm not being able to contact its configuration server and failing to restart for the deployment. The appservers got depooled because of the failure until pybal depool protection kicked in. When etcdmirror was restarted to resolve the restart issue, the configuration state with the depooled servers was synchronized, which triggered the depooling of 50% of codfw api-https, api_appserver, appserver, and parsoid servers.","Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?claime reports errors during scap sync-file, jayme picks up on conf2005/etcdmirror being in a CRITICAL stateCopy the relevant alerts that fired in this section.15:22:02   +icinga-wm | PROBLEM - etcdmirror-conftool-eqiad-wmnet service on conf2005 is CRITICAL: CRITICAL - Expecting active but unit etcdmirror-conftool-eqiad-wmnet is failedDid the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?Alert fired on IRC but no page went out.TODO: If human only, an actionable should probably be to ""add alerting"".",OPTIONAL: General conclusions (bullet points or narrative),"Page on etcdmirror alert
*Add etcdmirror connection retry on etcd-tls-proxy unavailability
*Update Etcd/Main cluster#Replication with safe restart conditions and information
*Add etcdmirror status check to scap
*Add failure rate triggered rollback to scapCreate a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard)  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	got lucky
People                                        	Were fewer than five people paged?                                                                                	no 	no pages -- but we wanted one
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	no pages
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	not warranted
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	irc alert only
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	etcdmirror documentation is spooky
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	6",2022-09-08,Server and Infrastructure Failures,high,Service Downtime,
An increase in POST requests to de.wikipedia.org caused an increase in load on one of the DB servers resulting in an increase in 503 responses and increased response time,"Error was detected by alert manager monitoring20:08 <+jinxer-wm> (ProbeDown) firing: (8) Service text-https:443 has failed probes (http_text-https_ip4) #page - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown -
https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown
20:08 <+jinxer-wm> (FrontendUnavailable) firing: HAProxy (cache_text) has reduced HTTP availability #page - TODO - https://grafana.wikimedia.org/d/000000479/frontend-traffic?viewPanel=13 - https://alerts.wikimedia.org/?q=alertname%3DFrontendUnavailable
20:09 <+jinxer-wm> (FrontendUnavailable) firing: varnish-text has reduced HTTP availability #page - https://wikitech.wikimedia.org/wiki/Varnish#Diagnosing_Varnish_alerts - https://grafana.wikimedia.org/d/000000479/frontend-traffic?viewPanel=3 - https://alerts.wikimedia.org/?q=alertname%3DFrontendUnavailable",Understanding of legitimate backed traffic would enable us to better sanitize bad traffic at the front end,"T309147  any POST that doesn't go to /w/*.php  or /wiki/.* should become a 301 to the same url
*T309186 Created sampled log of post data
*T310009 Make it easier to create a new requestctl object==Scorecard==Incident Engagement™  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	(created retrospectively)
Process                                       	Are the documented action items assigned?                                                                         	   	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	(similar to ""2022-05-20 Database slow / appserver"")
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were all engineering tools required available and in service?                                                     	yes	
Tooling                                       	Was there a runbook for all known issues present?                                                                 	no 	Setting to no as we need to update the DDoS playbook. We have also updated the question from now on to reflect that.
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2022-05-25,Performance and Load Issues,mid,Degraded System Performance,
"Today at 11:41 the icinga check for `ms-fe.svc.codfw.wmnet` timed out and thus paged (and recovered three minutes later):11:41 -icinga-wm:#wikimedia-operations- PROBLEM - LVS swift-https codfw port 443/tcp - Swift/Ceph media           storage IPv4 #page on ms-fe.svc.codfw.wmnet is CRITICAL: CRITICAL - Socket timeout after 10           seconds https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems...11:44 -icinga-wm:#wikimedia-operations- RECOVERY - LVS swift-https codfw port 443/tcp - Swift/Ceph media           storage IPv4 #page on ms-fe.svc.codfw.wmnet is OK: HTTP OK: HTTP/1.1 200 OK - 396 bytes in           0.140 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problemsUsers in codfw/ulsfo/eqsin have experienced ~15min of higher latency (possibly timeouts) for hit-local and miss requests (10-25% of the site's requests, depending on the site).Specifically hitting /monitoring/backend timed out, this in turn meant that some of the backend server(s) where the monitoring container lives were slow/unhealthy.Case in point ms-be2033.codfw.wmnet was reported as slow from /var/log/swift/server.log on e.g. ms-fe2006.codfw.wmnet:Feb  1 11:44:29 ms-fe2006 proxy-server: ERROR with Object server 10.192.16.15:6000/sdk1 re: Trying to GET /v1/AUTH_mw/monitoring/backend: Timeout (10.0s) (txn: txe96767fb630b4828af04a-006017e993) (client_ip: 208.80.154.88)The slowness was induced by an earlier swift rebalance () and the way we do rebalances at the moment means that such operations are generally noisy/impactful to the cluster (e.g. , ). Swift has been depooled internally from its discovery record (essentially anticipating ).",,,"Change /monitoring/backend to /monitoring/frontend (i.e. check the frontend itself) for icinga service check and pybal's proxyfetch 
* Consider depooling swift's discovery records during rebalances",2021-02-01,Performance and Load Issues,mid,Degraded System Performance,
db1127 was too busy to respond even to the simple statistics queries that the mysql prometheus exporter runs. The MariaDB query killer has a bug in 10.6.10 where queries were not properly killed. This was fixed in 10.6.12. db1127 was one of the remaining hosts to still be on 10.6.10.,"Automated alert via VictorOps:Critical: PHPFPMTooBusy api_appserver (php7.4-fpm.service eqiad)https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad%20prometheus/ops&var-cluster=api_appserver The MediaWiki cluster api_appserver in eqiad is experiencing saturation of php7.4-fpm.service workers 9.851% https://bit.ly/wmf-fpmsat Not enough idle php7.4-fpm.service workers for Mediawiki api_appserver at eqiad #page
Alerts Firing:
Labels:
 - alertname = PHPFPMTooBusy
 - cluster = api_appserver
 - prometheus = ops
 - service = php7.4-fpm.service
 - severity = page
 - site = eqiad
 - source = prometheus
 - team = sre
Annotations:
 - dashboard = https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad%20prometheus/ops&var-cluster=api_appserver
 - description = The MediaWiki cluster api_appserver in eqiad is experiencing saturation of php7.4-fpm.service workers 9.851%
 - runbook = https://bit.ly/wmf-fpmsat
 - summary = Not enough idle php7.4-fpm.service workers for Mediawiki api_appserver at eqiad #page
Source: https://prometheus-eqiad.wikimedia.org/ops/graph?g0.expr=sum+by%28cluster%2C+service%29+%28phpfpm_statustext_processes%7Bcluster%3D~%22%28api_appserver%7Cappserver%7Cparsoid%29%22%2Cstate%3D%22idle%22%7D%29+%2F+sum+by%28cluster%2C+service%29+%28phpfpm_statustext_processes%7Bcluster%3D~%22%28api_appserver%7Cappserver%7Cparsoid%29%22%7D%29+%3C%3D+0.3&g0.tab=1",,"Report this issue upstream (MDEV-30760) ==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	https://phabricator.wikimedia.org/T330422
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	12",2023-02-23,Code and Application Bugs,mid,Degraded System Performance,
"WDQS in Codfw entered a state of deadlock that persisted until service restarts were performed. Note that WDQS eqiad was depooled at the time of the incident, reducing total capacity.",,,"To make that service stable is to re-architect and replace Blazegraph.  The Search team will discuss this and arrange follow up actions
* In the meantime, https://phabricator.wikimedia.org/T293862 might help to improve the reliability of Blazegraph.
* As the service is fairly fragile, but recovers quickly after a restart, simple auto-remediation such as scheduled service restarts might be appropriate.
* Investigate if earlier alerts should page https://phabricator.wikimedia.org/T303134
* As discussed here, the command-line utility jstack can detect deadlocks, and is installed on all wdqs hosts. Perhaps we can use it to monitor for these deadlocks.
* Update https://wikitech.wikimedia.org/wiki/Wikidata_Query_Service/Runbook#Blazegraph_deadlock with the exact verbiage from the alerts and examples of what Grafana looks like during these outages.==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	1    	
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	as weekend
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	N/A  	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	0    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	?    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	
Total score	Total score                                                                                                                                              	6",2022-03-27,Code and Application Bugs,mid,Service Downtime,
"In an attempt to deploy a change to block excessive scraping of the search API from a particular User-Agent running on AWS hosts, instead, by accident, all enwiki search API traffic was blocked.",Detection was a human report in #wikimedia-operations.  No automated detection.(There is a larger architectural issue here about the 'proper' interface between Traffic and other services,,"There's two sections here: one for easy and obvious things, and another for larger ideas that require more discussion.* Update docs to mention the recently-added public_cloud_nets IP list recently made available.  
* Add some more documentation on how to write and run VTC tests.  Consider adding a test suite of common requests expected to return 200.=== Grander schemes ===
* A 'differ' tool that would re-play the last ~1h of HTTP requests (or a set of canned queries) into an 'old' and 'new' Varnish, and show differences between the routing/rejection decisions that each made, would have caught this mistake before it was deployed, and also probably would be generically useful.
* An external monitoring system, if explicitly configured to probe the srsearch API, would have caught this mistake quickly after it was deployed.
* It is likely desirable to add more structure to how traffic blocks are configured.  Currently the options are ""add one line in the private Puppet repo to block all traffic from a given IP range"", or ""write arbitrary VCL to do something else"".  We could make more options available by filling out a few lines of a data structure instead of writing code (and knowing where to place said code).
** As we scale the organization/our technical infrastructure/the number of services we run, this will probably prove necessary, along with other measures to make it more self-service: service owners shouldn't have to escalate to Traffic/SRE to implement blocks, nor should each of them have to implement their own blocking logic in every application.
** If we made the notion of a 'traffic blocking rule' into a first-class entity, we could also add instrumentation around them -- and know how many rps were being blocked by which blocking rule, etc.",2019-12-31,Configuration and Deployment Errors,high,API or Integration Failures,
"A firewall change was pushed to ulsfo routers, which caused ulsfo to lose connectivity to the other POPs and core sites for 3min.",,,,2022-02-01,Configuration and Deployment Errors,mid,Service Downtime,
For 49 minutes the Swift/mediawiki file backend returned errors (how many? Which percentage?) for both reads and new uploads.,"The issue was detected automatically and the engineers On Call received a page from Splunk on CallAlerts that fired during the incident:* Incident #3133
* Incident #3134
* Incident #3135
* Incident #3136
* Incident #3137The alerts that fired were useful for the engineers to solve the incident.",,"Investigate why the alerts scalated to batphone even when the engineers on call have already ACK'd the initial alert.* Add runbooks, documentation on how to troubleshoot this issues.== Scorecard ==Incident Engagement ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	no 	preparedness, we just discussed we don't understand what happened and that the documentation is a decade old
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	yes	Jaime was not one of the oncallers nor the IC, but he was the first to speak up with the suggestion of updating the status page, quite a long time into the outageChecking who can access file
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	https://phabricator.wikimedia.org/T322424
Process                                       	Are the documented action items assigned?                                                                         	no 	The incident is very recent
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	We don't know what's causing the issue so there was no way to have a task for it
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	no 	We didn't have any cummin cookbooks on how to restart the Swift service so the engineers had to figure out the right commands during the incident
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	6",2022-11-04,API and Integration Failures,high,Degraded System Performance,
"Increased db load for enwiki (s1) resulted in slower responses, which in turn resulted in overall php-fpm worker limits being reached and thus affecting requests for all wikis. For requests above the limit, the error was ""upstream connect error or disconnect/reset before headers. reset reason: overflow"".Impact: For about 15 minutes, backend appservers were slower or unable to respond for all wikis. This mainly affected logged-in users and most bot/API queries. Some page views from unregistered users were affected, for pages that were recently edited or otherwise expired from the CDN cache.  Documentation:* Public incident task: T291767* Similar to Incident documentation/2021-09-04 appserver latency and Incident documentation/2021-09-18 appserver latency.",,,"T291767 (restricted)
* T251885 (restricted)",2021-09-26,Performance and Load Issues,high,Degraded System Performance,
An issue with inconsistent state of deployed code during a backport deployment caused MediaWiki to crash for all logged-in page views.The root cause of this issue was MediaWiki re-reading and applying changes to extension.json before a php-fpm restart would have picked up changes to the PHP code.,"Humans and automated alerts detected the issue quickly, and the alert volume was manageable.",,"TODO: can canary checks detect this issue?==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	responders were deployers, not SREs
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	pages were only sent out right before mitigating action
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	The page was not routable to sub-teams because it originated in the frontend
(Basically the sub-team was everyone)
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	no incident doc was created because it was not necessary to create one
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	responders did not have access or training on wikimediastatus.net usage
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	created by the community
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	Serve production traffic via Kubernetes: https://phabricator.wikimedia.org/T290536
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	Backport_windows/Deployers#Reverting, although frequent deployers usually are familiar with the process and don't use the documentation
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	13",2023-01-17,Configuration and Deployment Errors,high,Service Downtime,
"During a planned GitLab maintenance to switch the main GitLab host from codfw to eqiad Phabricator users noticed that they were not able to load tasks, instead seeing an """"Unhandled Exception (""RuntimeException"")"" message. The following errors were observed in the logs:Invalid argument supplied for foreach()` `called at [<wmf-ext-misc>/src/customfields/GitLabPatchesCustomField.php:113]fatal: unable to access 'https://gitlab.wikimedia.org/toolforge-repos/toolpilot.git/': The requested URL returned error: 502Based on the above findings a decision was made to abandon the planned maintenance and take GitLab out of maintenance mode. This resulted in Phabricator tasks loading as expected, other than a short caching period for already open tasks.The most likely cause of the incident was a Phabricator widget deployed in T324149","The issue was brought to our attention by users in the #wikimedia-operations IRC channel. There were no automated alerts as the Phabricator service was up and pages were loading, but displaying an error instead of expected contents",,"T333347 will be used to troublehoot the widget behavior.
* Maybe create blackbox checks for certain tasks? (like https://phabricator.wikimedia.org/T1)==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	No pages
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	No pages
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	No pages
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	No Google doc was necessary
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	T333347
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	T333347
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	8",2023-05-02,API and Integration Failures,mid,Degraded System Performance,
"An upload of 65 video 4k files via the server-side upload process caused high CPU/socket timeout errors on jobrunners (all jobrunner hosts are also videoscalers).  This caused an increase in job backlog and unavailability on several mw-related servers (job queue runners, etc.).  It seems that a combination of the files being 4k (and thus requiring many different downscales), long (averaging an hour in length), combined with the fact that the videos were uploads from a local server (mwmaint) with a fast connection to the rest of our infrastructure resulted in too much load being placed on the jobqueue infrastructure.Halting the uploads and temporarily splitting the jobqueue into videoscalers and other jobrunners allowed the infrastructure to catch up.",,,"Document that users should use --sleep to pause between files when running importImages.php (done)
*  Rate limit the process to upload large files 
*  Add rate limiting to the jobqueue videoscalers 
*  Add alerting for Memcached timeout errors 
*  Update Runboook wikis for the application and LVS servers 
* Have some dedicated jobrunners that aren't active videoscalers",2021-03-30,Performance and Load Issues,mid,Degraded System Performance,
There was an increase in requests to the API cluster that resulted in reduction of availabe PHP workers and a database host (db1132) running out of available connections. This database host runs MariaDB 10.6 which is known to be sensitive to high load. This resulted in an increase in latency and errors returned to clients. The spike auto-recovered and it's not clear what the exact root cause was. Documentation:*Appservers RED dashboard,,,"https://phabricator.wikimedia.org/T311106 - investigate mariadb 10.6 performance regression during spikes/high loads.==Scorecard==Incident Engagement ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	no 	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2022-07-11,Performance and Load Issues,mid,Degraded System Performance,
"A patch was merged and deployed to all hosts containing a syntax error on the /etc/sudoers file. This meant sudo did not work for the period of time indicated below, affecting mostly nagios execution (alerting) and creating root mail spam. As a consequence, also mail delivery got overloaded/delayed.",,,"Add `validate_cmd` to sudoers file
** https://gerrit.wikimedia.org/r/667119
** https://gerrit.wikimedia.org/r/667120 
* Understand safe batch numbers for fleetwide puppet runs
* Prevent automatic email to get marked as spam by GMail
* Add exim queue metrics to grafana https://phabricator.wikimedia.org/T275867
* Investigate why the exim queue has many frozen messages for wiki at wikimedia.org and otrs at ticket.wikimedia.org",2021-02-26,Configuration and Deployment Errors,mid,Degraded System Performance,
"Various Wikidata Query Service instances were down (failing readiness probe). Nodes that were down had errors in their blazegraph logs, and the problem would not spontaneously resolve without a restart of blazegraph.Impact: There was a period of a few minutes where there was a full outage; all WDQS queries failed to complete during this window. There was a more extended period of degraded service where a subset of queries would fail depending on which instance received the request.","Issue was first detected when a critical alert was issued and plumbed through to IRC:PROBLEM - PyBal backends health check on lvs2009 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2003.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs-ssl_443: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs_80: Servers wdqs2003.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooledInitial critical alerts that fired:PyBal backends health check on lvs2009PyBal IPVS diff check on lvs2010PyBal backends health check on lvs2010Alert volume was manageable and fairly clearly indicated the problem (too many nodes were down in order to maintain full availability). Alert could perhaps explicitly show the connection between ""nodes are marked down but not pooled"" and ""the aforementioned is occurring because too many nodes are down"".","It's known that WDQS presents many unique scaling challenges, and that exposing a public endpoint comes with risks.We have throttling in place. This state is maintained within each server rather than being shared; as a result, blazegraph restarts will clear any throttling behavior automatically put in place.We don't think implementing shared state is a good idea because it won't fully solve the problem and makes the system more difficult to reason about (due to persisting state across restarts).Ultimately this issue is more related to expectations of service availability / performance: while there are technical measures we can put in place to minimize ""blast radius"", there will always be potential for user behavior to topple the system, thus communication should center around the level of availability of this service. As such one of our action items is to define clear SLOs and an error budget for Wikidata Query Service. The amount of effort required to support the required level of availability needs to be assessed.","T258739 wdqs admins should have access to nginx logs, jstack on wdqs machines
* T258754 Define SLOs, error budget for WDQSNote: We discussed submitting an upstream bug report for T242453, but relevant context here is that Blazegraph was acquired by Amazon. As a result, while our e-mails are still responded to, we don't expect there to be engineering resources available to help troubleshoot a concurrency/deadlock issue, as Blazegraph is no longer under active development. As such we have not submitted an upstream bug report because we don't expect the time investment of a proper bug report to be fruitful.",2020-07-23,Server and Infrastructure Failures,mid,Service Downtime,
"Since the upgrade of MediaWiki appservers to Debian Buster early in 2021, large file uploads (anecdotally anything over 300MB) have been failing because of timeouts when uploading the file to Swift cross-datacenter (all uploads are sent to both primary datacenters). The cause was determined to be the libcurl upgrade enabling HTTP/2 by default, which is generally slower at these kinds of transfers than HTTP/1.1 or HTTP/1.0 is (see this Cloudflare blog post for a brief explainer). Forcing internal requests from MediaWiki PHP to Swift to use HTTP/1 immediately resolved the issue. We have since also disabled HTTP/2 more generally in the internal Nginx proxy we use for TLS termination to Swift. The nginx puppet code we used here was originally written for public traffic TLS termination (this now uses ATS), which explains why it had HTTP/2 enabled.","The issue was first detected as a train blocker because the timeouts were causing database query errors on the MediaWiki side. Further manual investigation discovered the timeouts in MediaWiki/Swift communication.In this specific case, alerting would not have helped given that the issue was noticed almost immediately, there was just no one actively looking into it for a significant amount of time.For the general case, having metrics about throughput of uploads to Swift for large files and alerting (non-paging) based on that average might be useful (see actionables).",,"The biggest issue identified here is needing a maintainer/owner for backend file upload/management related components.* There is a general need for better metrics collection and logging of the individual actions in SwiftFileBackend.* T283045: LocalFile::lock() rearchitecting - don't hold a database transaction open for the duration of the upload
* T295008: MediaWiki uploads files to Swift in eqiad and codfw in serial, not parallel
*T295482: Track throughput of large file uploads to Swift
* Log headers in MultiHttpClient
=Scorecard =
Read more about Incident Scorecard.           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	This is an unusual incident involving many people over 9 months
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	0    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	No page
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	Task was updated, but incident docs were created after resolving the issue
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	Issue was eventually given high priority, but wasn't considered UBN. troubleshooting status was logged in task, and incident status was updated after resolution
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	Yes
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	0    	Some, some are not
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	No tooling issues were reported, although communication was slow due to unclear ownership
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	0    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	Yes
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	
Total score	Total score                                                                                                                                              	3",2021-11-04,Configuration and Deployment Errors,mid,Degraded System Performance,
"The reason appears to be background parsing associated with VisualEditor. The MWExtensionDialog as used in Score has the default 0.25s debounce preview, meaning we're shelling out to Lilypond through Shellbox every quarter-second while the user is typing -- regardless of whether an existing shellout is in flight. That's reasonable for lots of parsing applications that take much less time than that, but for something as heavy as these score parses, we should extend that interval, which would have the effect of cutting down on the request rate to shellbox.",ProbeDown) firing: Service shellbox:4008 has failed probes (http_shellbox_ip4) #page - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All,,"Reduce Lilypond shellouts from VisualEditor https://phabricator.wikimedia.org/T312319
* Update MobileApp to set a proper User-Agent https://phabricator.wikimedia.org/T314663
* Add X-IP header to proxied traffic==Scorecard==Incident Engagement ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	8",2022-07-11,Code and Application Bugs,mid,Degraded System Performance,
,"Replication lag was detected first by Icinga alerts. Phabricator task was created and triaged.
2023-05-25 05:11:16 <icinga-wm> PROBLEM - MariaDB Replica SQL: s1 on db1154 is CRITICAL: CRITICAL slave_sql_state Slave_SQL_Running: No, Errno: 1032, Errmsg: Could not execute Delete_rows_v1 event on table enwiki.user_properties: Cant find record in user_properties, Error_code: 1032: handler error HA_ERR_KEY_NOT_FOUND: the events master log db1196-bin.001099, end_log_pos 654625806 https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_replica
2023-05-25 05:11:44 <icinga-wm> PROBLEM - MariaDB Replica SQL: s2 on db1155 is CRITICAL: CRITICAL slave_sql_state Slave_SQL_Running: No, Errno: 1032, Errmsg: Could not execute Delete_rows_v1 event on table plwiki.user_properties: Cant find record in user_properties, Error_code: 1032: handler error HA_ERR_KEY_NOT_FOUND: the events master log db1156-bin.003729, end_log_pos 633898246 https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_replicaBroken replication on sanitarium host:
Broken replication on sanitarium host
PROBLEM - MariaDB Replica SQL: s5 on db1154 is CRITICAL: CRITICAL slave_sql_state Slave_SQL_Running: No, Errno: 1032, Errmsg: Could not execute Delete_rows_v1 event on table dewiki.flaggedpage_pending: Cant find record in flaggedpage_pending, Error_code: 1032: handler error HA_ERR_KEY_NOT_FOUND: the events master log db1161-bin.001646, end_log_pos 385492288MariaDB crashing when inserting the missing row example log:","mariadb version 10.4.29 maybe a replication bug - hard/impossible to reproduce
* wikireplicas is quite critical for the community although it's not considered a ""production"" service","Add more documentation of wikireplicas setup
* Alerting for wikireplicas lag
* Discuss SLO for wikireplicas and level of ""production"", per Leon Ziemba
** ""I wanted to ask something I've genuinely been curious about for years -- since the wiki replicas are relied upon so heavily by the editing communities (and to some degree, readers), should we as an org treat their health with more scrutiny? This of course is insignificant compared to the production replicas going down, but nonetheless the effects were surely felt all across the movement (editathons don't have live tracking, stewards can't query for global contribs, important bots stop working, etc.). I.e. I wonder if there's any appetite to file an incident report, especially if we feel there are lessons to be learned to prevent similar future outages? I noticed other comparatively low-impact incidents have been documented, such as PAWS outages."" ==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	At least not for database problems
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	Only for the database related parts
People                                        	Were fewer than five people paged?                                                                                	yes	no page
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	no page
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	no page
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	no incident document
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	T337446
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	no 	Documentation lacking. Index rebuild script was bit rotten.
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-05-28,,,,
An expensive query sent over and over again by an external user(s) caused errors and timeouts for users accessing the WDQS service from our CODFW datacenter. A requestctl rule was put in place to mitigate the issue.,"Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an errorDetected by monitoringCopy the relevant alerts that fired in this section.See above for example.Did the appropriate alert(s) fire? YesWas the alert volume manageable? YesDid they point to the problem with as much accuracy as possible?No. We saw increased 5xx errors and lots of time spent in old garbage collection, but it's still difficult to troubleshoot this type of abuse.",,"Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-05-23,Performance and Load Issues,mid,Degraded System Performance,
"For ~40 minutes (from 20:06 to 20:48 UTC), clients users who geographically reach us via Eqiad had trouble connecting and received timeout errors. Based on traffic dashboards, we lost about 7K requests/second. Recovery of the incident included temporarily depooling Eqiad to route clients to a different datacenter, which increases latencies. After 10 minutes, Eqiad was ready and repooled as well.	For ~40 minutes (from 20:06 to 20:48 UTC), clients users who geographically reach us via Eqiad had trouble connecting and received timeout errors. Based on traffic dashboards, we lost about 7K requests/second. Recovery of the incident included temporarily depooling Eqiad to route clients to a different datacenter, which increases latencies. After 10 minutes, Eqiad was ready and repooled as well.	For ~40 minutes (from 20:06 to 20:48 UTC), clients users who geographically reach us via Eqiad had trouble connecting and received timeout errors. Based on traffic dashboards, we lost about 7K requests/second. Recovery of the incident included temporarily depooling Eqiad to route clients to a different datacenter, which increases latencies. After 10 minutes, Eqiad was ready and repooled as well.Users had trouble connecting to the Eqiad datacenter (wikis and CDN, as well as developer services like Phabricator and Gerrit) because of a return path issue after datacenter maintenance (see T293726#7454820 for an explanation of how the maintenance went wrong). Because of other networking maintenance taking place at the same time it took a while to diagnose the underlying issue and gain confidence that we had fixed the correct issue. In addition, the alert only went out over IRC, not the VictorOps pager because about half of our outbound paths to the Internet were broken due to the outage, which affected the ability for the alert to reach our external alert vendor. Impact: For ~40 minutes (from 20:06 to 20:48 UTC), clients users who geographically reach us via Eqiad had trouble connecting and received timeout errors. Based on traffic dashboards, we lost about 7K requests/second. Recovery of the incident included temporarily depooling Eqiad to route clients to a different datacenter, which increases latencies. After 10 minutes, Eqiad was ready and repooled as well.",,,"T293726: Investigate & document why adding a new patch cable to the Equinix IX ports caused return path issues
* T294166: NEL alert included #page in IRC but did not page through VictorOps (and then it did page through VO, at 21:07)
** This was because our outbound path to any internet destinations best reached via the Equinix IX were broken from (half of?) eqiad for the duration of the incident.
**Icinga also missed reporting when ""BGP status on cr2-eqiad"" went down, only reporting its recovery, our tentative theory is that it was also affected by the networking issues.
**Working around this is difficult but we should think about what to do.
* Remind people that using Klaxon when they're relatively confident something is wrong is preferred to waiting for automated monitoring to page.
** It's possible that Klaxon would also not work for the same users -- Klaxon is hosted in eqiad on alert1001.wikimedia.org, with the codfw installation being a passive backup host.
**It would have required someone on IRC to relay the issue through Klaxon, which had happened, just lack of actually using the tool.
*Have an incident replay, looking at both the technical aspects as well as procedural.
*T295672: Use next-hop-self for iBGP sessions",2021-10-22,Server and Infrastructure Failures,high,Service Downtime,
"For about 3 minutes (from 7:32 to 7:35 UTC), eventgate-main was unavailable. This resulted in 25,000 unrecoverable MediaWiki backend errors due to inability to queue new jobs. About 1,000 user-facing web requests and API requests failed with an HTTP 500 error. Event intake processing rate measured by eventgate briefly dropped from ~3000/second to 0/second during the outage.	For about 3 minutes (from 7:32 to 7:35 UTC), eventgate-main was unavailable. This resulted in 25,000 unrecoverable MediaWiki backend errors due to inability to queue new jobs. About 1,000 user-facing web requests and API requests failed with an HTTP 500 error. Event intake processing rate measured by eventgate briefly dropped from ~3000/second to 0/second during the outage.	For about 3 minutes (from 7:32 to 7:35 UTC), eventgate-main was unavailable. This resulted in 25,000 unrecoverable MediaWiki backend errors due to inability to queue new jobs. About 1,000 user-facing web requests and API requests failed with an HTTP 500 error. Event intake processing rate measured by eventgate briefly dropped from ~3000/second to 0/second during the outage.During the helm3 migration of eqiad Kubernetes cluster the service eventgate-main experience an outage. The service was not available between 7:32 and 7:35 UTC. For the helm3 migration the service had to be removed and re-deployed to the cluster. Most Kubernetes services were explicitly pooled in codfw-only during the re-deployments. eventgate-main was also falsely assumed to be served by Codfw but was still pooled in Eqiad. So during the time of removing and re-creating the pods, no traffic could be served for this service.The commands used to migrate and re-deploy codfw (see T251305#7492328) were adapted and re-used for eqiad (see T251305#7526591). Due to a small difference in what Kubernetes services are pooled as active-active and what are active-passive, eventgate-main was missing in the depooling command (as is it not pooled in codfw currently).Impact: For about 3 minutes (from 7:32 to 7:35 UTC), eventgate-main was unavailable. This resulted in 25,000 unrecoverable MediaWiki backend errors due to inability to queue new jobs. About 1,000 user-facing web requests and API requests failed with an HTTP 500 error. Event intake processing rate measured by eventgate briefly dropped from ~3000/second to 0/second during the outage.Documentation:*Grafana Dashboard MediaWiki Exceptions*Grafana eventgate statistics*Grafana Varnish http errorsFile:2021-11-25-mediawiki-exceptions.pngFile:2021-11-25-eventgate-statistics.pngFile:2021-11-25-varnish-http500.png=Scorecard=           	Question                                                                                                                                                 	Score	NotesPeople     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	1    	People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	1    	People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	N/A  	People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	N/A  	Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	N/A  	Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	N/A  	Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	0    	Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	N/A  	Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	N/A  	Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	Total score	Total score                                                                                                                                              	5",,,"automate maintenance and proper de-depooling of Kubernetes services using a cookbook T277677 and T260663
*reduce snowflake services which need special treatment and make most/all of them active-active (for example T288685)
*optional: create a lvs/pybal/k8s service dashboard to see which service is pooled in which DC (will create a task)
*T296699: Pool eventgate-main in both datacenters (active/active)",2021-11-25,Configuration and Deployment Errors,mid,Service Downtime,
"Starting on Sat 4 May 2022, incoming emails from Google Mail servers began being rejected with the log message ""503 BDAT command used when CHUNKING not advertised"". These errors were not noticed by us until five days later on Thu 9 May 2022. After some investigation, it was determined that disabling chunking support in Exim would mitigate the errors. During the time span of the incident about 14,000 emails were rejected with an SMTP 503 error code, the senders are naturally notified by their email provider about undelivered mail.","The issue was first detected by users sending emails, https://phabricator.wikimedia.org/T307873. Though, the messages were rejected by Exim with a 503 error code. We do graph the number of bounced messages, but our alerting did not pick up these bounces, https://grafana.wikimedia.org/d/000000451/mail?orgId=1&from=1651536000000&to=1652227199000",Email monitoring of bounced messages does not account for all bounces.,"Improve monitoring, https://phabricator.wikimedia.org/T309237==Scorecard==Incident Engagement™  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were all engineering tools required available and in service?                                                     	yes	
Tooling                                       	Was there a runbook for all known issues present?                                                                 	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	10",2022-05-09,Code and Application Bugs,mid,Degraded System Performance,
"Maps servers fully saturated on CPU, resulting in an increase in user-experienced latency and request error rate.  In order to shed load and restore service for users of Wikimedia projects, traffic from non-Wikimedia sites was blocked (and as of 2020-03-04, is still blocked).The proximate cause is not fully known, but a large part of it is likely fallout of a Mediawiki API outage, as Maps servers need to fetch data from the Mediawiki API for some kinds of requests, and there have been previous Mediawiki incidents where similar Maps impact was seen.The deeper causes involve a long-running lack of staffing on the software and its infrastructure, one of the manifestations of which is a lack of familiarity with the software and the infrastructure by the SRE team and most others involved in day-to-day operations.","Automated detection via basic LVS/PyBal alerts.However, note that users were already experiencing elevated latency and errors well before the extant alerts fired.",What weaknesses did we learn about and how can we address them?The following sub-sections should have a couple brief bullet points each.,"Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Rebalance Maps traffic between eqiad and codfw: eqiad is often much hotter than codfw, as it sees all the European load.  (TODO: Create task)
** Making more use of codfw (and ulsfo) in general is desired by the Traffic team; can potentially use Maps as a proving ground for a new geographical DNS mapping.
* Make a policy decision about whether or not to continue disallowing external sites to embed our maps.  If we decide to do so:
** Probably, begin disallowing cache hits as well as cache misses; it's confusing.
** Announce this publicly; remove Wikimedia from OSM's list of map tile providers; etc.
* Perform some capacity planning for Maps. phab:T228497
** This was an actionable in a few previous Maps outages, but probably didn't happen?
*** Incident documentation/20190308-maps
*** Incident documentation/20190715-maps
*** Incident documentation/20190913-maps
* Investigate why Maps overloads when appservers have high latency (or are returning errors)
** One of the Kartotherian log messages that was prominent in the incident was Bad geojson - unknown type object which seems to correlate with appserver trouble.
* Attempt to get some more staffing behind Maps?Category:Maps outages, 2020",2020-02-04,Performance and Load Issues,high,Degraded System Performance,
"Some mailing lists have disabled archiving, with the expectation that once current subscribers get a copy of the message then the mailing list server will no longer have a copy of the message. However, it saves any attachment permanently, crucially, this includes the HTML part of any multi-part message. These attachments are publicly accessible via a URL: https://lists.wikimedia.org/pipermail/wikidata-bugs/attachments/20200901/7b1e75bd/attachment.htm. It would require brute-forcing a 8 character hex string per day to figure out the URL. This functionality exists for the purpose of supporting digests, as the digests are sent as plain text and attachments are only referred to by URL. The URL is public because Mailman2 doesn't have any real web authentication support. It has been publicly filed in the Mailman2 bug tracker since 2006.Old messages have been kept since the entire lifetime of our Mailman2 installation. Audits of the available Apache HTTPD access logs (30 days) found no unauthorized access.A systemd timer was put in place to delete these archived attachments after 31 days. Some lists also disabled digests to prevent these attachment files from being saved in the first place. This issue was fully resolved with the migration to Mailman3, which does not suffer from this flaw.Impact: Any person who sent an email to a list that was not supposed to be archived could have had their message contents publicly accessible and leaked. It is impossible to say whether someone ever did brute-force these URLs given how long this vulnerability was present and how short our access logs are kept for.",Amir discovered this issue while going through archives on lists1001 while preparing for the Mailman3 migration. It's unclear how we could have noticed this automatically or through monitoring.,,"Set up a timer to purge old attachments
*  Verify Mailman3 does not have this issue
*  Ensure digest message contents are excluded from Mailman2 and Mailman3 backups
*  Migrate all lists with archiving disabled over to Mailman3",2021-04-03,Security and Access Issues,high,Security Vulnerabilities or Breaches,
"For approximately 12 minutes some internal services (e.g. Bacula and Etherpad) were not available or operated at reduced capacity. This was caused by a faulty memory stick leading to a reboot of db1128, which was at the time the primary host of the m1 database section.Documentation:*Full list of potentially affected services",,,"Failover m1 primary db from db1128 to db1164
*db1128 faulty memory==Scorecard==Incident Engagement™  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were all engineering tools required available and in service?                                                     	yes	
Tooling                                       	Was there a runbook for all known issues present?                                                                 	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2022-05-26,Server and Infrastructure Failures,mid,Service Downtime,
A bug introduced to the MediaWiki codebase caused an increase in connections to Confd hosts from systems responsible for Dumps which in turn lead to a high volume of log events and ultimately a filled up filesystem.,The last symptom of his issue was detected by an Icinga alert: conf1008 icinga alert: <icinga-wm> PROBLEM - Disk space on conf1008 is CRITICAL: DISK CRITICAL - free space: / 2744 MB (3% inode=98%): /tmp 2744 MB (3% inode=98%): /var/tmp 2744 MB (3% inode=98%): https://wikitech.wikimedia.org/wiki/Monitoring/Disk_space,,"conf* hosts ran out of disk space due to log spam; 
* Monitor high load on etcd/conf* hosts to prevent incidents of software requiring config reload too often; ==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	Yes	although some had responded to previous incidents as well
People                                        	Were the people who responded prepared enough to respond effectively                                              	Yes	
People                                        	Were fewer than five people paged?                                                                                	Yes	No page
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	No 	No page
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	No 	No page
Process                                       	Was the incident status section actively updated during the incident?                                             	No 	IC came in late
Process                                       	Was the public status page updated?                                                                               	No 	
Process                                       	Is there a phabricator task for the incident?                                                                     	Yes	
Process                                       	Are the documented action items assigned?                                                                         	Yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	No 	From the memory of review ritual participants we had that exact same issue before
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	Yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	Yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	Yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	Yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	No 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	9",2022-11-03,Code and Application Bugs,mid,Degraded System Performance,
"Users of Jio ISP (India, AS 55836) unable to reach Wikimedia sites.Caused by a misconfiguration in Jio's own RPKI records, which can only be fixed on their end.  Their network engineers are aware and working on the issue.As temporary workaround, Wikimedia changed its network advertisements to accept invalid RPKI to let Jio costumers reach our sites again.",,,"https://phabricator.wikimedia.org/T260449
* https://phabricator.wikimedia.org/T260452",2020-08-14,External Dependencies,high,Service Downtime,
Connecting a new server to our eqsin top of rack switches triggered a Juniper bug  which caused one of its processes to be killed and interrupting traffic transiting through the switch. This event caused also a Virtual-Chassis master switchover extending the outage. The process got automatically re-started and the situation stabilized by itself in about 5min.,"Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?Copy the relevant alerts that fired in this section.Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?TODO: If human only, an actionable should probably be to ""add alerting"".",OPTIONAL: General conclusions (bullet points or narrative),"Upgrade POPs asw to Junos 21 - https://phabricator.wikimedia.org/T316532
* We're phasing out virtual chassis in the new POP network designs (cf. drmrs). Even though such bugs might always be a possibility, the new design is more resilient (each switch is independent)==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	no 	
People                                        	Were fewer than five people paged?                                                                                	no 	no pages
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	no pages
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	no pages
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	no 	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	auto-resolved
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-11-15,Server and Infrastructure Failures,mid,Service Downtime,
"During a package cleanup,  was merged to remove some packages. This caused to bullseye VMs in the cloud realm to remove isc-dhcp-client, and once the ip leases for these started to expire the VMs started losing network connectivity.This eventually included the proxies CloudVPS uses to serve external traffic, making any hosted project lose that traffic too.This also took down the metricsinfra VMs that are in charge of the monitoring and alerting for CloudVPS hosted projects, so there were no alerts from it.From there recovery included having to roll-reboot all the toolforge VMs that depend on the nfs servers as the nfs service itself got affected and clients got stuck (common procedure, but slow).","The issue was first detected by users, and it was not until the first admin started their work day that they noticed something was wrong.The only page was received way after, once the recovery had started.Note that the outage took one of the monitoring and alerting systems down, though we would not have been paged by any alert there (https://phabricator.wikimedia.org/T323510).",,"investigate why we did not get any pages, and fix/add them
*  - add meta-monitoring for metricsinfra
*  - create a cookbook to run commands through virsh console
*  - improve current nfs setup so it does not require to reboot + run puppet to bring online (as it might take 30 min for puppet to run unattended)==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                                                                                                                                       	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?                                                                                                                           	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                                                                                                                                           	yes	
People                                        	Were fewer than five people paged?                                                                                                                                                                                             	yes	Alerting was broken.
People                                        	Were pages routed to the correct sub-team(s)?                                                                                                                                                                                  	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.                                                                                                             	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                                                                                                                                     	no 	None created
Process                                       	Was a public wikimediastatus.net entry created?                                                                                                                                                                                	no 	
Process                                       	Is there a phabricator task for the incident?                                                                                                                                                                                  	yes	
Process                                       	Are the documented action items assigned?                                                                                                                                                                                      	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?                                                                                                                            	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?                                                                                                                      	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                                                                                                                                         	no 	will pursue meta-monitoring
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                                                                                                                                 	no 	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                                                                                                                                	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                                                                                                                                 	9",2023-09-29,Configuration and Deployment Errors,high,Service Downtime,
Toolforge Redis refused new connections because there were too many active connections. This happened intermittently for 1 hour until the service was restarted manually.,Toolschecker detected a problem with Redis and Icinga fired an alert. This was routed to Alertmanager and to VictorOps. VictorOps sent a page to the on-call engineer (User:FNegri).,,"phab:T363709==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                                                                                                                                       	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?                                                                                                                           	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                                                                                                                                           	yes	
People                                        	Were fewer than five people paged?                                                                                                                                                                                             	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                                                                                                                                  	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.                                                                                                             	no 	during a weekend
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                                                                                                                                     	no 	
Process                                       	Was a public wikimediastatus.net entry created?                                                                                                                                                                                	no 	WMCS incident
Process                                       	Is there a phabricator task for the incident?                                                                                                                                                                                  	yes	
Process                                       	Are the documented action items assigned?                                                                                                                                                                                      	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?                                                                                                                            	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?                                                                                                                      	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                                                                                                                                         	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                                                                                                                                 	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                                                                                                                                	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                                                                                                                                 	10",2024-04-28,Performance and Load Issues,mid,Degraded System Performance,
A schema change (phab:T307501) made mariadb's optimizer change its query plan and made a very frequent query to globalblocks table on centralauth database (s7) to take 5 seconds instead of less than a second.,"Alerts (IRC and pages)
[05:39:18]  <+jinxer-wm> (ProbeDown) firing: (10) Service appservers-https:443 has failed probes (http_appservers-https_ip4) - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown
[05:39:18]  <+jinxer-wm> (ProbeDown) firing: (20) Service appservers-https:443 has failed probes (http_appservers-https_ip4) #page - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown
[05:40:04]  <+icinga-wm> PROBLEM - High average POST latency for mw requests on appserver in eqiad on alert1001 is CRITICAL: cluster=appserver code=200 handler=proxy:unix:/run/php/fpm-www.sock https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad+prometheus/ops&var-cluster=appserver&var-method=POST
[05:40:10]  <+icinga-wm> PROBLEM - Not enough idle PHP-FPM workers for Mediawiki api_appserver at eqiad #page on alert1001 is CRITICAL: 0.07182 lt 0.3 https://bit .ly/wmf-fpmsat https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad+prometheus/ops&var-cluster=api_appserver
[05:40:14]  <+icinga-wm> PROBLEM - High average GET latency for mw requests on api_appserver in eqiad on alert1001 is CRITICAL: cluster=api_appserver code=200 handler=proxy:unix:/run/php/fpm-www.sock https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad+prometheus/ops&var-cluster=api_appserver&var-method=GET
* User reported it on IRC",,"Investigate the mariadb optimizer behaviour for this specific table: https://phabricator.wikimedia.org/T307501==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	1    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	1    	There is no way to prevent this
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	We don't really need it
Total score	Total score                                                                                                                                              	8",2022-05-05,Schema and Database Issues,mid,Degraded System Performance,
"A faulty optic on one of the fiber links between WMCS racks caused packet loss between nodes in the Cloud VPS Ceph storage cluster. This made writes any writes stall since Ceph could not confirm those writes had been committed on all nodes they were supposed to on. Cloud VPS VMs cannot handle their storage hanging like this and stalled too, which made any services hosted on Cloud VPS inaccessible for the duration of the incident.","Automated alerting noticed the issue - the first alert was a warning that pointed towards a Ceph issue of some sort at 14:58:09, and a page was sent out from a toolschecker NFS alert about four minutes later (15:01:50). The first human report arrived on IRC several minutes later:
15:08:35 <Lucas_WMDE> it looks like there might be connection issues at the moment? I can’t connect to tools via HTTPS nor SSHThe initial alerts located the issue well, although they were followed by a high volume of generic ""VPS instance"" down alerts (on IRC and via email).TODO: did metricsinfra or its meta check send any pages? if not, why?",,"T367199 Replace the faulty optic
* Figure out if we should alert for interface errors like this one
*  phab:T367336 Add both sides of the links to discard/error graphs for router connectivity to the ceph dashboardsCreate a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2024-06-11,Server and Infrastructure Failures,high,Service Downtime,
"At Tue May 31 17:09:10 UTC 2022 analytics-alerts@wikimedia.org received an email alert: ""At least one Hadoop HDFS NameNode is active is CRITICAL""* Otto, Btullis, Joal, and Mforns jumped in hangout to troubleshoot.* /var/lib/hadoop/journal on all 5 journalnodes was full* Otto and Btullis stopped namenodes and journalnodes* Btullis increases /var/lib/hadoop/journal on journalnodes from 10 GB to 30 GB* Btullis starts journalnodes, then master namenode.* Otto forces HDFS to stay in safe mode.* Wait for master namenode to apply edits from journalnodes.* Start standby namenode* Wait for standby namenode to apply edits* Otto allows HDFS to leave safe mode.Documentation:Cause analysis:On Sunday evening May 22, /srv filled up on an-master1002. an-master1002 takes daily fs image snapshots, and saves them in /srv/backup/hadoop/namenode, keeping the last 20.  Over time, as the number of HDFS blocks has increased, so has the size of these backup images.  We received an alert email for a failure of the hadoop-namenode-backup-fetchimage that takes these backups with the subject ""an-master1002/Check unit status of hadoop-namenode-backup-fetchimage is CRITICAL"".24 hours later, this backup job succeeded, even if no new image backup was taken, and we got a RECOVERY status email for this job.  Otto was on ops week, and only working half days this week.  Otto most likely saw the RECOVERY email and ignored the alert.On Tuesday May 31, /var/lib/hadoop/journal on all journalnodes completely filled, and NameNodes crashed as they were not able to get ACKs from the journalnodes that their writes had been saved.We believe that after /srv/backup/hadoop/namenode filled up on May 22, the standby NameNode was no longer able to save its image to /srv/hadoop/name/current.  Because no new image was saved, the hadoop-namenode-backup-fetchimage did not detect that a new image was present, it did not try to take a new backup.  The hadoop-namenode-backup-prune kept purning backup files older than 20 days, freeing up space on the /srv partition.  However, because the standby NameNode was not able to save its FS images snapshots, JournalNodes were not able to clear up historical edits files, which caused them to fill up their journal partitions.After the NameNodes were recovered and out of safe mode, writes could proceed.  All ingestion is handled either via Kafka or periodic jobs, and these can resume from where they left off.  No lasting impact.",,,"The following ticket contains all actionable items. https://phabricator.wikimedia.org/T309649These are:* Make old journalnode edits files are cleaned properly now that namenodes are back online and saving fs image snapshots.
* Reduce profile::hadoop::backup::namenode::fsimage_retention_days, 20 is too many
* Create an alert for the freshness of the standby namenode's FSImage dump in /srv/hadoop/name/current
* Make sure journalnodes alert sooner about disk journalnode partition
* Check that bacula backups of fs image snapshots are available and usable
* Check that the alerting for disk space is correct on an-master hosts - since we seem not to have been alerted to /srv/ becoming full on an-master1002All have now been completed.==Scorecard==Incident Engagement™  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	Yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	Yes	
People                                        	Were fewer than five people paged?                                                                                	Yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	Yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	Yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	Yes	
Process                                       	Was the public status page updated?                                                                               	n/a	This is not a public-facing service, but we notified users of HDFS via email, Slack and IRC.
Process                                       	Is there a phabricator task for the incident?                                                                     	Yes	
Process                                       	Are the documented action items assigned?                                                                         	Yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	Yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	Yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	Yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	No 	Monitoring has been updated to inform the Data Engineering team for all relevant services.
Tooling                                       	Were all engineering tools required available and in service?                                                     	Yes	
Tooling                                       	Was there a runbook for all known issues present?                                                                 	Yes	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	13",2022-05-31,Data Management and Retention,high,Service Downtime,
"For at least 15 minutes, users of the Wikidata Query Service either could not connect or received extremely slow responses.","The issue was detected by monitoring (pybal alerts)Example alert verbiage: PROBLEM - PyBal backends health check on lvs1020 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs1015.eqiad.wmnet, wdqs1012.eqiad.wmnet, wdqs1004.eqiad.wmnet, wdqs1014.eqiad.wmnet, wdqs1016.eqiad.wmnet, wdqs1007.eqiad.wmnet, The appropriate alerts fired, and contained enough actionable information for humans to quickly remediate the problem.","Search team has been aware of the brittle nature of WDQS for some time, and there are ongoing efforts to migrate off its current technology stack (specifically Blazegraph). We are also in the process of defining an SLO for WDQS.","Email to Wikidata Users list for awareness==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	10",2022-11-22,Performance and Load Issues,mid,Degraded System Performance,
eqiad's Logstash experienced message congestion that exhausted the latency budget. This incident consumed 160% of the quarterly budget for delayed messages.,"One IRC-only alert fired:
 [10:45:55] <jinxer-wm> (LogstashKafkaConsumerLag) firing: Too many messages in kafka logging - https://wikitech.wikimedia.org/wiki/Logstash#Kafka_consumer_lag - https://grafana.wikimedia.org/d/000000484/kafka-consumer-lag?var-cluster=logging-eqiad&var-datasource=eqiad%20prometheus/ops - https://alerts.wikimedia.org/?q=alertname%3DLogstashKafkaConsumerLag
It was Saturday, and there was no page, so nobody saw or responded to the alert. The problem wasn't noticed until the end of the SLO quarter, when we discovered in the normal reporting process that Logstash had missed its latency SLO in eqiad.",OPTIONAL: General conclusions (bullet points or narrative),"TODO: Decide whether to include an action item for a paging alert on Kafka lag/backlog
* ...Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard)  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes    	No one was paged or responded at the time of the incident. These answers are a best effort for that situation, based on the intent of the questions. (E.g., here, ""yes"" because the incident didn't contribute to burning anyone out.)
People                                        	Were the people who responded prepared enough to respond effectively                                              	no     	
People                                        	Were fewer than five people paged?                                                                                	yes    	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no     	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes    	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no     	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no     	But one wouldn't have been appropriate, even if we had responded -- Logstash is critical SRE-facing infrastructure but is not user-facing.
Process                                       	Is there a phabricator task for the incident?                                                                     	yes    	
Process                                       	Are the documented action items assigned?                                                                         	not yet	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes    	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes    	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes    	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no     	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	?      	Blank for now because I'm not sure. Logstash itself would have been a key debugging tool, and mid-incident it would have shown the culprit messages but at a long delay; between 10:54 and 12:55, we wouldn't have known that the flood was over. But maybe other tools would have had that information available.
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no     	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2023-02-11,Performance and Load Issues,mid,Degraded System Performance,
"As part of the work done in T334194 a reindex of the Elasticsearch indices for Wikibase enabled wikis (wikidata and commons) was scheduled. Reindexing is a routine task the search teams uses to enable new settings at the index level (generally to tune how languages are processed). For this particular task the reason was to optimize the number of analyzers created on these wikis by de-duplicating them (300+ languages). De-duplicating analyzers means that any code referring to a particular analyzer might now possibly reference one that was de-duplicated and thus non-existent. The Search Team analyzed such cases and found nothing problematic scanning the code-base. This was untrue, after the Wikidata reindex was done when the new index was automatically promoted to production the fulltext search queries started to fail. Reason is that the token_count_router query was still referencing the text_search analyzer directly which was now nonexistent because de-duplicated. The token_count_router is a feature that counts the number of token in a query to help not run costly phrase queries on queries that has too many tokens.Mitigations that were evaluated:* disabling the token_count_router could have fixed the immediate problem but could have put the whole cluster under the risk of being overloaded by such pathological queries.* reverting the initial feature was not possible since it requires a full re-index of the wiki (long procedure, 10+hours)* adding the text_search analyzer manually on the wikidata and commons indices could have fixed the issue but required closing the index which is a heavy maintenance task (search traffic switching).* fix the token_count_router to not reference the text_search analyzer directly, one-liner. This approach was preferred.",The problem was detected by users and then raised on IRC via the #mediawiki_security channel.,The reindex maintenance procedure can promote a broken index to production causing immediate failures on the affected wikis. It could possibly try to generate a couple representative queries and make sure that they run before doing such promotion?,"phab:T339939 Add alerting on the number of CirrusSearch failures (rejected), the problem was detected by users
* phab:T339938 Consider running some test queries before switch index aliases$
* phab:T339935 Consider testing a few wikibase queries from the integration tests Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	?  	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	SRE was explicitly skipped in favor of reaching out directly to Search Team
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	It was a Sunday
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	We used T339810
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	Since SRE got skipped
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	T339810
Process                                       	Are the documented action items assigned?                                                                         	   	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	   	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	IRC + Google Meet
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	   	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-06-18,Configuration and Deployment Errors,high,Service Downtime,
"With the introduction of a number of changes to make HTTPS redirects unconditionalhttps://phabricator.wikimedia.org/T256095, the the MediaWiki API virtual hosts returned a HTTP 302 redirect to the HTTPS edge (e.g. external) address for every request that doesn't include the X-Forwarded-Proto: httpshttps://en.wikipedia.org/wiki/X-Forwarded-For header. For instance a request to http://api-ro.discovery.wmnet  would return a 302 to  Location: https://www.wikidata.org/w/index.phpThis made some Termbox and Wikifeeds requests run into timeouts, because traffic towards MediaWiki via our edge (egress) is not allowed from the Kubernetes Clusters.Impact: With a total of ~15 hours the outage was very long. During this period around 12% of requests to Termbox to failed with HTTP 500 (~186.000 requests). For Wikifeeds, only a specific endpoint was affected but for that more or less every request took longer than 30s and should be considered a failure. That's a total of around 1.250.000 requests lost.Since we have 2 layers of caching in our edges, the actual user impact (e.g. the Wikipedia mobile app homepage not loading) was smaller.","High latency in restbase was recognized by an SRE but was not classified as a critical because the number of requests timing out was acceptable and no #pages had fired.Hours later another SRE reacted to the Icinga alerts and warnings, and verified that this is a user facing issue (Wikipedia mobile app main page not loading).","Even though there was a minor user impact, which was evident in our graphs, it went unnoticed for hours. If Wikifeeds and Termbox were more heavily used, it is possible we would have caught this earlier.",Monitoring/Alerting for Wikipedia mobile app errors,2020-07-14,Configuration and Deployment Errors,high,API or Integration Failures,
"A MediaWiki API outage happened two days earlier on 2021-09-04. It was correlated back then to a rise in HTTP 503s returned by the Wikifeeds service for the next 2 days. The issue was particularly subtle since only some requests ended up in HTTP 503, so the service health checks failed intermittently for a brief amount of time every now and then during the weekend, only noticed the next Monday. A rolling restart of the Wikifeeds Kubernetes pods reportedly restored the service to a healthy status. While the possibility that the Mediawiki API outage is related remains, it was discovered that end-user traffic to the Wikifeeds service more than quadrupled (4x) at peak, violating the traffic Wikifeeds SLO leaving the service in a degraded state and returning many 503 and 504s. The tls-proxy container was throttled at times during the peaks of the incident, causing the timeouts.  Impact: For 3 days, the Wikifeeds API failed about 1% of its requests (e.g. 5 of every 500 per second).","The detection of the issue happened two days after it started, thanks to a service-checker icinga alert:
17:32 +<icinga-wm> PROBLEM - wikifeeds codfw on wikifeeds.svc.codfw.wmnet is CRITICAL: /{domain}/v1/page/featured/{year}/{month}/{day} 
                   (retrieve title of the featured article for April 29, 2016) is CRITICAL: Test retrieve title of the featured article for 
                   April 29, 2016 returned the unexpected status 504 (expecting: 200): 
                   /{domain}/v1/media/image/featured/{year}/{month}/{day} (retrieve featured image data for April 29, 2016) is CRITICAL: 
                   Test retrieve featu
17:32 +<icinga-wm> e data for April 29, 2016 returned the unexpected status 504 (expecting: 200): 
                   /{domain}/v1/page/most-read/{year}/{month}/{day} (retrieve the most-read articles for January 1, 2016 (with 
                   aggregated=true)) is CRITICAL: Test retrieve the most-read articles for January 1, 2016 (with aggregated=true) returned 
                   the unexpected status 504 (expecting: 200) https://wikitech.wikimedia.org/wiki/Wikifeeds
This alert fired and self recovered every now and then during the weekend, where attention to IRC reported errors is lower, and it got noticed by one SRE by chance while looking at the #wikimedia-operations IRC channel. A lot of time was spent trying to figure out how the service worked, how to reproduce the problem and what could be the root cause of it.","The main pain point was surely to identify what systems are involved in handling a request for the Wikifeeds API, and how to reproduce one error case. The documentation on Wikitech is good but generic, and there were some important details that not all SREs involved had clear in mind (one above all, the fact that a Wikifeeds request involves Restbase, Wikifeeds on Kubernetes, and possibly again Restbase to fetch some auxiliary data). Due to the unlucky circumstances (weekend plus US holiday) there were fewer SREs available to work on the problem, and it was not clear if it was worth a page or not. This situation will probably get better when we'll introduce SLOs and more formal on-call process.","Add (more) documentation to the Wikifeeds Wikitech page to describe what is the relationship between Restbase, Wikifeeds and its tls-proxy container. T291912
*Increase the capacity for wikifeeds T291914
*Originally, it was deemed as a worthy actionable to find a way to reproduce the Envoy inconsistent state, or check if there is already literature/links/etc.. about it. However, since this was related to the greatly increased end-user traffic levels and since the incident review and investigation did not manage to reveal signs of envoy having an inconsistent state, this should not happen",2021-09-06,Performance and Load Issues,mid,API or Integration Failures,
"From 10:28 to 11:43, Wikimedia Commons was unavailable and/or slow to respond. Additionally, from 10:41 to 11:10, commons was set in unscheduled read only mode (unable to create new users, upload files or edit pages). The immediate cause was large amount of InnoDB contention for read and write queries on the primary s4 database (which serves commonswiki and testcommonswiki writes). After a restart and a forced kill, the database came back cleanly in read only mode. Global usage extension was temporarily disabled (several queries were observed from this extension blocked on the master) before the server was set back up in read write, ending the incident. GlobalUsage was signaled as the most probable cause, as it was sending large amount of long-running queries at the moment of the issue to the primary DB server. Because of the general InnoDB error obtained -contention on an Innodb index (on an engine where everything, including data, are indexes)- it is difficult to say if this was the primary cause, or another underlying cause caused them to block, leading the the outage. Other scaling problems are known to occur on commonswiki (e.g. large image table). Work has been done to send as much traffic as possible to the replicas.","received Alerts relating to Mysql replication on S4
 PROBLEM - MariaDB Replica IO: s4 #page on db1143 is CRITICAL: CRITICAL slave_io_state 
          Slave_IO_Running: No, Errno: 2013, Errmsg: error reconnecting to master repl@db1138.eqiad.wmnet:3306 - 
          retry-time: 60 maximum-retries: 86400 message: Lost connection to MySQL server at waiting for initial 
          communication packet, system error: 110 Connection timed out 
          https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_replica* 10:33: The master was quickly identified as the source of problemsOn the database error log, it was seen: Apr 27 10:29:59 db1138 mysqld[3848]: 2021-04-27 10:29:59 139919263487744 [Note] Threadpool has  
 been blocked for 30 seconds
 Apr 27 10:30:14 db1138 mysqld[3848]: InnoDB: Warning: a long semaphore wait:
 Apr 27 10:30:14 db1138 mysqld[3848]: --Thread 139901381777152 has waited at fut0fut.ic line 51  
 for 241.00 seconds the semaphore:
 Apr 27 10:30:14 db1138 mysqld[3848]: X-lock on RW-latch at 0x7f51d3e0c898 '&block->lock'
 Apr 27 10:30:14 db1138 mysqld[3848]: a writer (thread id 139927395628800) has reserved it in mo
 de  exclusive
 Apr 27 10:30:14 db1138 mysqld[3848]: number of readers 0, waiters flag 1, lock_word: 0
 Apr 27 10:30:14 db1138 mysqld[3848]: Last time read locked in file row0sel.cc line 3075
 Apr 27 10:30:14 db1138 mysqld[3848]: Last time write locked in file buf0buf.cc line 4279
 Apr 27 10:30:14 db1138 mysqld[3848]: Holder thread 0 file not yet reserved line 0Which ended up blocking any operation, even killing threads for a clean shutdown.",,"update icinga alert link
* Disable GlobalUsage does selects on the master database
* update documentation for Changeprop
* Re-enable globalusage
* Modify an existing tool that quickly shows the db replication status",2021-04-27,Schema and Database Issues,high,Service Downtime,
"The s6 master, db1131, went offline due to a bad DIMM. We rebooted it via ipmitool and restarted mariadb, then failed over to db1173 and depooled db1131.","We were paged once for Host db1131 #page is DOWN: PING CRITICAL - Packet loss = 100% about three minutes after user impact began.We also got paged for MariaDB Replica IO: s6 #page on db1173 is CRITICAL: CRITICAL slave_io_state Slave_IO_Running: No, Errno: 2003, Errmsg: error reconnecting to master repl@db1131.eqiad.wmnet:3306 - retry-time: 60 maximum-retries: 86400 message: Cant connect to MySQL server on db1131.eqiad.wmnet (110 Connection timed out) and identical errors for db1098, db1113, and db1168. Those alerts were redundant but the volume wasn't unmanageable.",,"Check and replace the suspected-failed DIMM.
*: Implement (or refactor) a script to move replicas when the master is not available==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	yes	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	DC Ops task isn't assigned yet, but is routed to #ops-eqiad per their process
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	9",2022-10-15,Server and Infrastructure Failures,mid,Service Downtime,
Documentation:*https://www.wikimediastatus.net/incidents/nb6v1zxp86ns,,,,2022-06-12,,,,
"A wave of spam email to an info@ address was routed from mail servers to the VRT machine (otrs1001).Many Perl processes were spawned which used up all the RAM of the virtual machine. oom-killer killed clamav-daemon.Without clamav mail delivery stopped.More mails started queing up on both otrs1001 and then the mail server mx1001.When the mail queue reached a critical threshold on mx1001, SRE got paged.Measures taken included increasing RAM available on the otrs1001 VM and deleting spam email.Eventually all mail was delivered, just with a delay.","On-call SRE got paged by Splunk-On-Call (VictorOps)incident name was: Critical: [FIRING:1] MXQueueHigh misc (node ops page prometheus sre), incident ID: VictorOps 3094",We should have more than a single VRTS server.Spam should not take down the VRTS machine.,"Increase RAM assigned to otrs1001 VM (done, increased from 4 to 8GB RAM)==Scorecard==Incident Engagement ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	VO attempted to page 6 people. 2 of them were on-call and were reached. 4 more seem to opt-in for 24/7 pages but did not respond. 3 other users did respond without being paged.
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	There was no expectation that would happen. It was during assigned on-call rotation.
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	To the best of our knowledge
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	unless you count the general exim->postfix switch which might come with rspamd
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-10-17,Performance and Load Issues,high,Service Downtime,
"On 2019-12-10, mainly between 22:26 and 22:39 UTC (but there were other smaller instances in the previous hour), Common wiki database replicas were lagging behind, causing slowdown, returning stale results, errors and, as a consequence of the original bug, category counts were updated incorrectly.","First notice was on IRC (see timeline).There is an alert for lag, but it only triggers after 300 seconds for 10 checks (while slowdown happen after lag is higher than 1 second, and hard down is after 6 seconds). Top production lag was around 100 seconds. [JCrespo thinks (but is unsure) that the reason for this discrepancy is because SRE concerns are only about broken mysql replication and excessive (e.g. 1 hours) lag, and no one attending application problems currently receives these alerts].",,"T240405 Immediate issue to avoid recurrence of the same problem.
* T221795 To fix category counts
* T108255 Enable strict mode to prevent lose sql mode for MySQL
* TODO: Change some dangerous updates of a single row to be LIMIT 1?",2019-12-10,Schema and Database Issues,mid,Degraded System Performance,
,"Humans noticed the problem immediately, as it was directly caused by operator error.No alerts had time to fire.",,"Update docs (done, see ""relevant documentation"" section above)
* All other action items listed in https://phabricator.wikimedia.org/T361288==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	N 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	Y 	
People                                        	Were fewer than five people paged?                                                                                	Y 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	NA	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	NA	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	NA	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	NA	
Process                                       	Is there a phabricator task for the incident?                                                                     	Y 	
Process                                       	Are the documented action items assigned?                                                                         	Y 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	Y 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	N 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	Y 	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	NA	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	Y 	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	N 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2024-03-27,,,,
Details restricted. See https://phabricator.wikimedia.org/T272215,,,https://phabricator.wikimedia.org/T272262 - service restart script doesn't detect failure when running with poolcounter.,2021-01-16,,,,
"Session storage is provided by an HTTP service (Kask) that uses Cassandra for persistence. As part of routine maintenance, one of the Cassandra hosts in eqiad (sessionstore1001) was rebooted.  While the host was down, connections were removed (de-pooled) by Kask, and requests rerouted to the remaining two, as expected.  However, once the host rejoined the cluster, clients that selected sessionstore1001 as coordinator encountered errors (an inability to achieve LOCAL_QUORUM consistency).This is likely (at least) similar to Incidents/2022-09-15 sessionstore quorum issues (if not in fact, the same issue).","Monitoring did not alert; A manual page was issued once TheresNoTime noticed an issue/Users started reporting issues:21:57:42 <TheresNoTime> Successful wiki edits has just started to drop, users reported repeated ""loss of session data"" persisting a refreshAs alerts did not fire, manual debugging was used to determine the issue at hand. It took little time to determine that SessionStorage was the issue.",,"Setup notifications for elevated 500 error rate (sessionstore) () 
* Notifications from service error logs(?) () 
* Determine root cause of unavailable errors (i.e. ""cannot achieve consistency level"") () 
* De-pool datacenter prior to hosts reboots (as an interim to properly fixing the connection pooling) ==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                                                                                                                                       	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?                                                                                                                           	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                                                                                                                                           	no 	Other people handled the issue.
People                                        	Were fewer than five people paged?                                                                                                                                                                                             	no 	Alerts did not fire
People                                        	Were pages routed to the correct sub-team(s)?                                                                                                                                                                                  	no 	Alerts did not fire
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.                                                                                                             	no 	Alerts did not fire
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                                                                                                                                     	no 	There was no Google doc
Process                                       	Was a public wikimediastatus.net entry created?                                                                                                                                                                                	yes	https://www.wikimediastatus.net/incidents/05cntb1k1myb
Process                                       	Is there a phabricator task for the incident?                                                                                                                                                                                  	yes	phab:T327815
Process                                       	Are the documented action items assigned?                                                                                                                                                                                      	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?                                                                                                                            	no 	Same failure mode but manifesting in a different way
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are open tasks that would prevent this incident or make mitigation easier if implemented.	no 	There weren't open tasks as such, the failure mode was known and considered fixed
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?                                                                                                                      	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                                                                                                                                         	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                                                                                                                                 	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                                                                                                                                	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                                                                                                                                 	5",2023-01-24,Server and Infrastructure Failures,mid,Degraded System Performance,
"ESAMS DC was unreachable by users. Globally, we experienced a request drop from ~150k req/s to ~91k req/s as outlined in the Grafana dashboard. Users that were trying to reach Amsterdam DC experienced network errors and delays.The incident doc still mentioned ""We’re still investigating why some Grafana panels shows ~1hr of requests drop instead of ~10m (the actual incident duration)""",The outage was detected by an SRE shortly after an accidental change. Fired alerts confirmed the outage as well:<+icinga-wm> PROBLEM - Host ncredir3003 is DOWN: PING CRITICAL - Packet lossDid the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?,OPTIONAL: General conclusions (bullet points or narrative),"Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	   	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	   	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	   	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	   	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	   	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	yes	Status page entry
Process                                       	Is there a phabricator task for the incident?                                                                     	   	
Process                                       	Are the documented action items assigned?                                                                         	   	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	   	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	Relevant runbook
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-11-15,Server and Infrastructure Failures,high,Service Downtime,
"What happened, in one paragraph or less. Avoid assuming deep knowledge of the systems here, and try to differentiate between proximate causes and root causes.Impact: Who was affected and how? In one paragraph or less.  For user-facing outages, estimate: How many queries were lost? How many users were affected, or which populations (editors? readers? particular geographies?), etc.  Do not assume the reader knows what your service is or who uses it.* Impact statement for read-only period* Impact statement for Kartotherian* Impact statement for search* Impact statement for increased save times","Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?Copy the relevant alerts that fired in this section.Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?TODO: If human only, an actionable should probably be to ""add alerting"".",,"Thumbor has dnsdisc conftool records, but they don’t appear to be used:
*: rzl@cumin1001:~$ confctl --quiet --object-type discovery select dnsdisc=thumbor get
*: {""codfw"": {""pooled"": false, ""references"": [], ""ttl"": 300}, ""tags"": ""dnsdisc=thumbor""}
*: {""eqiad"": {""pooled"": true, ""references"": [], ""ttl"": 300}, ""tags"": ""dnsdisc=thumbor""}
*: They may be a relic from when swift wasn’t active/active; we should be able to clean them up.
* [nitpick] Message “Failed to call …00-reduce-ttl-sleep” seems slightly misleading; it got called but didn’t succeed/failed?
* False-alerts for “Time elapsed since the last kafka event processed by purged on host is CRITICAL: … topic=eqiad.resource-purge” (fixed) and an unknown for “Elevated latency for eventgate-logging-external eqiad”
* Kartotherian in codfw could not cope with the load, eqiad needed repooling, solution may be more hardware (on the way)
* Remove swift from EXCLUDED_SERVICES in sre.switchdc.services and add it to the default list, as we use that DNS record after all. Addressed in 
* Consider a puppet-merge lock/warning, global host MOTD, or other last-minute production-wide lockout/tagout
* Future “attach to tmux” commands should enforce a minimum terminal size requirement
* Consider re-adding the 5m sleep after reducing TTL, since the warmup script now takes less than five minutes to complete
* Consider automatically adding Grafana annotation(s) for critical operations from the cookbook(s)
* 08-update-tendril:<marostegui> parsercache (pc1,p2 and pc3) should have been changed too, but they were not, that's not important
* WRT Elasticsearch & /related/ page feature issues: dcausse@ says that, in the 2018 switchover, they took an existing capacity cache for elasticsearch and made it replicated cross-DC, because otherwise elasticsearch falls over without it.  However, the hit rate for such plummeted during this switchover (graph)
* Changeprop uses consistent connections that need to be killed during the switchover.  Adding a step to restart the Envoy TLS terminators on the old-DC jobrunners would be sufficient to force a reconnect.
* Confd on mwmaint1002 needed to be restarted manually, worth checking why
** [Riccardo] Additional hosts had confd stuck, all for the same reason AFAICT, the expired certificate. I bet is due to the Puppet CA rotation we did a while ago, the one host I checked was having confd not restated for 10 months.
** [Riccardo] I suggest to add a step to restart all confd in the preparation phase.        
* Dewiki had an extra maintenance message telling that we would add a new DC (below the generic maintenance message) TODO: include screenshot from doc
* There was a lot of confusion and alerts spam from primary db read only status. While a better setup (icinga allowing) or downtiming should had happened, a worse underlying issue was detected- confctl was not giving current results on puppetmaster1002, which was used primarily to configure alerts and its urgency on puppet (icinga). Task: T261767
* might be useful in the future for that kind of purpose to not move all services (https://sal.toolforge.org/log/PHHkRHQBLkHzneNNOEPw ) at once. even doing them 1 minute apart would have helped here (5min apart would be nice)
* T261763 save timing increase
* We’d still like to get rid of maintenance cronjobs, so that we can eliminate the 01-stop-maintenance and 08-start-maintenance steps (the latter includes a Puppet run). The only cron left is the WDQS dispatcher; everything else has been migrated to mediawiki::periodic_job, which reads the active DC from etcd.
* DB CPU saturation dashboard exists for eqiad but not codfw. Task: T261868
* Understand this log entry from 04-switch-mediawiki: 2020-09-01 14:03:43,668 rzl 28797 [INFO] Sleeping 23.459 seconds to reach the 10 seconds mark
* In 07-set-readwrite, if the siteinfo fetch times out, consider retrying on a different appserver, instead of failing immediately.
* Improvements for mwmaint switchover crons vs noc.wm webserver (Daniel) T265936
TODO: Link to or create a Phabricator task for every step. Add the #Sustainability (Incident Followup) Phabricator tag to these tasks.",2020-09-01,[],[],[],
"The streaming updater stopped to function properly because a k8s node misbehaved. More details at Incidents/2022-02-22 wdqs updater codfw.Documentation:*https://phabricator.wikimedia.org/T301147For 7 hours (2022-02-06T23:00:00 to 2022-02-07T06:20:00) the streaming updater in eqiad stopped working properly preventing edits to flow to all the wdqs machines in eqiad.The lag started to rise in eqiad and caused edits to be throttled during this period:Investigations:* the streaming updater for WCQS went down from 2022-02-06T16:32:00 to 2022-02-06T23:00:00* the streaming updater for WDQS went down from 2022-02-06T23:00:00 to 2022-02-07T06:20:00* the number of total task slots went down to 20 from 24 (4tasks == 1pod) between 2022-02-06T16:32:00 and 2022-02-07T06:20:00 causing resource starvation and preventing both jobs from running at the same time (flink_jobmanager_taskSlotsTotal{kubernetes_namespace=""rdf-streaming-updater""})* kubernetes1014 (T301099) seemed to have showed problems during this same period (2022-02-06T16:32:00 to 2022-02-07T06:20:00)* the deployment used by the updater used one POD (1db45eb6-2405-4aa3-bec1-71fcdbbe4f9a) from kubernetes1014* the flink session cluster was able to regain its 24 slots after 1db45eb6-2405-4aa3-bec1-71fcdbbe4f9a came back (at 2022-02-07T08:07:00), then this POD disappeared again in favor of another one and the service successfully restarted.* during the whole incident k8s metrics & flink metrics seem to disagree:** flink says that it lost 4 task managers (1 POD)** k8s always reports at least 6 PODS (count(container_memory_usage_bytes{namespace=""rdf-streaming-updater"", container=""flink-session-cluster-main-taskmanager""}))Questions (answered):* why do flink and k8s metrics disagree (active PODs vs number of task manager)?** Flink could not contact the container running on kubernetes1014 and thus freed it's resources (task slots), k8s attempted to kill the container as well but did not fully reclaim the resources (PODs) allocated to it* why a new POD was not created after kubernetes1014 went down (making 1db45eb6-2405-4aa3-bec1-71fcdbbe4f9a unavailable to the deployment)?** From the k8s point of view kubernetes1014 was flapping between the ready and not ready state and preferred to reboot containers thereWhat could we have done better:* we could have route wdqs traffic to codfw during the outage and avoid throttling editsAction items:* T305068: Alert if the number of flink tasks slots go below what we expect* T293063: adapt/create runbooks for the streaming updater and take this incident into account (esp. we should have had reacted to the alert and routed all wdqs traffic to codfw)* To be discussed with service ops:** Investigate and address the reasons why after a node failure k8s did not fulfill its promise of making sure that the rdf-streaming-updater deployment have 6 working replicas** If the above is not possible could we mitigate this problem by over-allocating resources (increase the number of replicas) to the deployment to increase the chances of proper recovery if this situation happens again?* T277876: to possibly improve the resiliency of the k8s nodes",,,"phab:T305068: alert when flink does not have the capacity it expects
*phab:T293063:adapt/create runbooks/cookbooks for the wdqs streaming updater==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	Info not logged
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	Did not page
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	Did not page
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	Did not page
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	1    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	0    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	1    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	0    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	
Total score	Total score                                                                                                                                              	6",2022-02-06,Server and Infrastructure Failures,high,Degraded System Performance,
"The rdf-streaming-updater application in CODFW became unstable and stopped sending updates, resulting in stale data for users connecting through CODFW.","Prometheus alerts for the WCQS cluster fired starting at 2023-05-04T1030 . Alerts were dispatched via email, with subject RdfStreamingUpdaterFlinkJobUnstable .WDQS cluster alerts started a bit later, at 2023-05-05T1908.In addition to the above subject,  WDQS alerts also included the subject  RdfStreamingUpdaterHighConsumerUpdateLag.The alerts correctly identified the problem and linked to the appropriate documentation.",,"Update WDQS Runbook following update lag incident
* Review alerting around Wikidata Query Service update pipeline
* WDQS: Document procedure for switching between Kubernetes and Yarn Streaming Updater==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	T336134
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	no 	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	8",2023-05-05,Code and Application Bugs,mid,Degraded System Performance,
"A provider outage on our primary transport link between eqiad and codfw caused it to be in a constant flapping (going down and up) state.This flapping caused routing re-convergence churn and packet loss between the two sites.On the application level, this translated to elevated 5xx/s from Varnish from ulsfo, eqsin, and codfw from 21:20 to 21:55 UTC.  Varnish reported ""No backend"" for many of the requests.  Host checks in Icinga were flapping ""TTL exceeded"" and service checks flapping ""No route to host.""",Monitoring caught and reported the issue via SmokePing and Icinga.,,"NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.*Those two will help mitigate the consequences of an overly flapping link:
**Configure interface damping on primary links - T196432
**ospf link-protection - T167306
*This one will make it easier (down the road) to a non-netops to failover a link if the need arises:
**Configuration management for network operations - T228388
*This one is about having better monitoring and alerting by replacing Smokeping by something Prometheus based
**Investigate/setup prometheus blackbox_exporter - T169860
*",2019-08-23,Server and Infrastructure Failures,high,Service Downtime,
"As part of T336044, @brouberol began copying Kafka partitions to new brokers.  Somehow, even with a throttle of 50MB/sec, this caused the mw-page-content-change Flink streaming enrichment job to fail producing messages to Kafka. The responders decided to shut down the streaming application until the partitions finished copying to the new brokers.  On Monday Oct 2 The streaming application was restarted. Messages were produced, but , @tchin, @joal and @ottomata noticed the backlog of messages was not decreasing.  They increased the parallelism to 4 replicas, and finally the backlog began to be processed.",Email alert fired.alertname = MediawikiPageContentChangeEnrichJobManagerNotRunningjob_name = mw_page_content_change_enrichkubernetes_namespace = mw-page-content-change-enrichprometheus = k8srelease = mainseverity = criticalsite = codfwsource = prometheusteam = data-engineering,OPTIONAL: General conclusions (bullet points or narrative),"T347884 - mw-page-content-change-enrich should not retry on badrevids if no replica lag
** This will help with backlog processing for future backfills.
* Figure out why copying Kafka partitions caused this issue.
* Consider running the streaming app with more than 1 replica even in normal cases, to help with backfills or spikes. Now running with 2 replicas.
* T345806 - mediawiki.page_content_change.v1 topic should be partitioned.  This will also help with backfills.Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-09-28,Configuration and Deployment Errors,mid,Degraded System Performance,
s3 primary database master had a RAID backup batery failure which cause the host to completely crash. It had to be power cycle from the idrac.,"The problem was clear when we saw that db1075 reported HOST DOWN - however, that only sends an IRC alert, not a page. Masters should probably page for HOST DOWN.
* Alerts were sent to IRC and pages.
* Users reporting issues on #wikimedia-operations","The master lost its BBU and that resulted on a completely host crash, which is something that has been seen before with HP hosts https://phabricator.wikimedia.org/T231638 https://phabricator.wikimedia.org/T225391The master being unavailable means that writes cannot happen: https://grafana.wikimedia.org/d/000000278/mysql-aggregated?orgId=1&var-dc=eqiad%20prometheus%2Fops&var-group=core&var-shard=s3&var-role=master&from=1569174586143&to=1569181181947This is is part of a batch of 6 servers, and 3 of them have already had BBU issues: https://phabricator.wikimedia.org/T233569 so we'd need to evaluate if what to do with then next.
Definitely replacing the current master and promoting another one which is not part of that batch is what is happening next: https://phabricator.wikimedia.org/T230783","Implement (or refactor) a script to move replicas when the master is not available (this wasn't needed yesterday, but could be needed in future issues): https://phabricator.wikimedia.org/T196366
*Fix mediawiki heartbeat model, change pt-heartbeat model to not use super-user, avoid SPOF and switch automatically to the real master without puppet dependency: https://phabricator.wikimedia.org/T172497
*Decide what to do with the same batch of hosts that have already had BBU issues: https://phabricator.wikimedia.org/T233569
*Buy a new BBU for db1075 https://phabricator.wikimedia.org/T233567
*Remove db1075 from being a master https://phabricator.wikimedia.org/T230783
*Address mediawiki spam during readonly/master unavailable https://phabricator.wikimedia.org/T233623
*Make sure primary database masters page on HOST DOWN https://phabricator.wikimedia.org/T233684
*Better tracking of hardware errors in Netbox https://phabricator.wikimedia.org/T233774",2019-09-23,Server and Infrastructure Failures,high,Service Downtime,
"Between 2019-12-11 and 2019-12-17, the job queue was blocked by image annotation request jobs that were being enqueued by the MachineVision extension. These jobs were using the release timestamp feature of the job specification interface that is not well supported by the Kafka job queue; release timestamps are implemented as blocking waits. These waits ended up blocking and causing a sizable backlog of a variety of jobs that are in the main pool of jobs not handled in job-specific Kafka topics. Jobs continued to be processed, but very slowly and with severe delays. No jobs appear to have been lost.Phabricator task: https://phabricator.wikimedia.org/T240518",The issue was first reported by user 1997kB in https://phabricator.wikimedia.org/T240518. That issue specifically concerned delayed global renames. Several other delays in specific wiki functionality were reported over the next few days. The issue was not detected by any automated alerts.,What weaknesses did we learn about and how can we address them?The following sub-sections should have a couple brief bullet points each.,"Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Add alert(s) for unusual job processing backlog increases (phab:T242721)
* Document the danger of the release timestamp feature in code and on-wiki (phab:T242722)
* Kafka job queue should improve its handling of unknown new jobs (phab:T242726)",2019-12-11,Code and Application Bugs,mid,Degraded System Performance,
"Certificates for hostnames of Cloud VPS web proxies were able to expire, such for https://puppet-compiler.wmflabs.org.",,,https://phabricator.wikimedia.org/T273959,2021-02-05,Configuration and Deployment Errors,mid,Security Vulnerabilities or Breaches,
,GModena and TChin reacted to alerts triggered by degrading SLIs.,"Application is running.
* We know what needs to be fixed.","increase max message size allowed by Kafka
* Filter out messages larger than the max allowed size; ==Scorecard==",2023-07-19,[],[],[],
A number of s5 database replicas in the  Eqiad and Codfw data centres alerted about replication being stopped due to a duplicate key. This lead to watchlist and recentchanges delays.Impact: Editors of wikis hosted in the s5 section (see s5.dblist) saw stale data in recentchanges and watchlist interfaces.,"Automated monitoring caught the issue, SRE was paged about stopped replication in a number of s5 replicas. The alerts were appropriate, sufficient and to the point. People responded and escalated to more equipped ones to handle the incident.","There are two possible explanations for this crash:1) The drift was there and we inserted a row that touched it2) The drift was generated somehow from MW.The first option is hard to see, as that table was recently checked before the switchover and came up clean (https://phabricator.wikimedia.org/T260042)
There is also the fact that the row that failed had a timestamp from around the time of the crash.
The sequence of events within the transaction that failed is interesting and it definitely didn't help that we are using INSERT IGNORE here. This is a summary from the binlog transaction for the broken entry and with the affected ipb_address_unique UNIQUE: 'REDACTED',ipb_user = 0,ipb_auto = 0This contains the first insert that went thru (it maybe failed, but as it has the IGNORE...) with the timestamp: 20200925113933 : BEGIN
 INSERT /* MediaWiki\Block\DatabaseBlockStore::insertBlock  */ IGNORE INTO `ipblocks`
Then an UPDATE on that same row that broke on some replicas, with a duplicate entry, it has this timestamp: 20200925110538. UPDATE /* MediaWiki\Block\DatabaseBlockStore::updateBlock  */  `ipblocks` SET ipb_address = 'REDACTED',ipb_user = 0,ipb_timestamp = '20200925110538',ipb_auto = 0
That is the row that was already existing on some hosts: root@db2089.codfw.wmnet[enwikivoyage]> SELECT * FROM ipblocks where ipb_address = 'xxxx'\G
 *************************** 1. row ***************************
          	ipb_id: xxxx
     	ipb_address: xxx
        	ipb_user: 0
    	ipb_by_actor: xxxx
   	ipb_reason_id: xxxx
   	ipb_timestamp: 20200925110538
 The values for ipb_address_unique on the insert were:
 62.XXX.2X,0,0And the values for that same UPDATE for that same key were exactly the same, with just the modification of the timestamp from 20200925113933 to 20200925110538.
What doesn't make sense to me is that there was an existing row with 20200925110538 timestamp (even if the timestamp isn't part of the UNIQUE).Looking for that that same IP on the existing binlogs on an affected replica (binlogs from today till 20th Sept) and there's no trace of that IP being added there. Same on the Eqiad master, which has binlogs from 29th Aug till today, no trace of that IP being added to ipblocks, before the sequence of events. The timestamps of the first entry on eqiad and codfw master are the same.Option 2 for this crash would imply that MW somehow introduced that inconsistency with an unsafe statement like INSERT IGNORE, however, it is difficult to know why only a few hosts failed.","Investigate the deeper causes of the incident. T263842. Deeper causes still unknown. 
* Automatic and continuous data checks (at least for the most important/big tables) T207253
* Re-evaluate the use of INSERT IGNORE on ipblocks T264701",2020-09-25,Schema and Database Issues,mid,Degraded System Performance,
"A bot scraping zhwiki, which we have been monitoring for a while now, started making more expensive requests more aggressively. The bot was concealing itself by using a common User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36Most requests were similar to: http://zh.wikipedia.org/w/api.php?action=parse&pageid=2996886&prop=text&wrapoutputclass=wiki-article&disableeditsection=true&mobileformat=true&mainpage=true&format=jsonThe wrapoutclass url parameter causes a request to bypass parsercache. To make matters worse, the scraper was going through the whole list of French localities on zhwiki, each of which made ample use of some known slow templates, originally seen on occitan wikipedia (euwiki), with the 36k entry table of localities. Each of those requests  required 15-60 seconds to parse.Lastly, while we were investigating, an unscheduled deployment was pushed to production, to fix an UNB! task. The deployment caused s8 to recive an influx of queries, so it was quickly reverted Incident_documentation/20200207-wikidata.","icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001
 14:06:41 <+icinga-wm> PROBLEM - High average POST latency for mw requests on api_appserver in eqiad on icinga1001 is CRITICAL:
 
 14:09:42 <+icinga-wm> PROBLEM - High average GET latency for mw requests on api_appserver in eqiad on icinga1001 is CRITICAL: 
 14:09:52 <+icinga-wm> PROBLEM - Apache HTTP on mw1290 is CRITICAL: CRITICAL
 
 14:17:07 <+icinga-wm> RECOVERY - Nginx local proxy to apache on mw1283 is OK: HTTP OK:",Templates issues are hard to debug.,Emptify the French Commune Data templates and contact the community (already done),2020-02-07,Performance and Load Issues,high,Degraded System Performance,
"An Equinix Singapore IXP peer flapped heavily, which overwhelmed the routing daemon on cr1-eqsin and caused all its BGP and OSPF sessions to flap or go down.In addition to the external connectivity issues, as the primary transport link to codfw is on cr1, it caused the local caches to not be able to reach their peers in the main datacenters and serve 500 errors instead.","The following automated alerts got triggered:*Varnish traffic drop between 30min ago and now at eqsin
*HTTP availability for Nginx -SSL terminators- at eqsin
*HTTP availability for Varnish at eqsin
*BFD status on cr1-codfw
*LVS HTTPS text-lb.eqsin.wikimedia.org - PAGEThis quickly pointed to an network issue in eqsin.Was the alert volume manageable? yesDid they point to the problem with as much accuracy as possible? yes",,"NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.*T236878 Improve resiliency of the eqsin transport link by either:
**Terminating it on cr2-eqsin
**Adding a 2nd link
**Configuring link damping
*Replace cr1-eqsin with a better router (next FY)",2019-10-16,Server and Infrastructure Failures,high,Service Downtime,
"Wikimedia users noticed ""weird and unhelpful search results,"" specifically with regards to autocomplete, as documented here. This was an unexpected result of changes related to Search Platform's Elasticsearch 7 upgrade.",Detection: users reported the errorAlerts: None,,"Create documentation on UpdateSuggesterIndex.php, probably should go on the Search page
*Better monitoring, specifics to be added later
*Sanity-checking for index size/age 
*Pool counter limits should be verified against what's running in production (CompletionSuggest limits are much higher than PrefixSearch, and when we gracefully degrade to PrefixSearch, we need more slots for PrefixSearch). Probably add this as a test this in mediawiki-config.
*Better communication, so others are aware when we roll out a major version change.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	9",2022-09-09,Code and Application Bugs,mid,Degraded System Performance,
"On 2021-09-12 starting at 18:13 UTC, the cache upload cluster at esams (upload-lb.esams.wikimedia.org) was affected by an outage that was the result of a large image being hotlinked by what appears to be multiple Romanian news organization judging from HTTP referer values. The sudden surge in traffic was sent to a single ATS backend instance, saturating the 10G NIC of the host between 18:06 and 18:08 and later triggering an anomalous behavior on 5 Varnish frontend instances in esams out of 8. The behavior consisted in the establishment of thousands of connections from all affected Varnish instances to a single cache backend. At the same time, all instances showing this behavior quickly reached the maximum configured number of work threads (24K given the configuration settings: thread_pools=2, thread_pool_max=12000). Without available work threads, the instances could not serve responses anymore. The error condition was resolved by a rolling restart of all Varnish instances in the upload cluster at esams. The incident was closed at 18:43 UTC. Between 18:13 and 18:37, a high number of web requests against the upload cache cluster at Esams resulted in server errors, up to about 15K req/s at peak. The amount of requests received from Romanian IPs against the upload cache cluster in Esams doubled, and the amount of data sent as response to requests from Romanian IPs increased about 50 times.","The SRE team was notified about the issue by a ""ATS TLS has reduced HTTP availability"" page, as well as IRC and email notifications.",,"Prioritize work on single backend CDN nodes https://phabricator.wikimedia.org/T288106
*  Investigate issue causing Varnish to establish thousand of connections to its origins / max out work threads / mmap counts (possibly different issues)
*  Revisit maximum number of varnish-fe connections to origins (currently 50K)",2021-09-12,Performance and Load Issues,high,Service Downtime,
"During regular maintenance, there was a (scheduled) loss of power redundancy on the codfw-A1 server rack around 14:32:00 UTC.While the servers in this rack did not lose power (given a redundant power supply), they did fully lose network connectivity and thus effectively went down. This happened because the second power cable for the ASW network switch was not plugged all the way in, resulting in an unscheduled full loss of the switch for that rack, and hence the rack's network connectivity.Happily, higher-level service redundancy worked as expected:* regarding LVS, lvs2010 automatically took over from lvs2007, for CDN traffic to Codfw. There was a very temporary increase on response latency for on-the-fly Codfw requests until traffic stabilized.* ns1 DNS server was automatically moved to Eqiad, should not have any user impact.* Most A2 servers alerted about loss of power redundancy, but having 2 power supplies they didn't go down.* App servers could have been affected more, latency-wise while they were automatically depooled, but they were not serving production traffic at this time as Eqiad is the primary DC.After the secondary power cord was properly connected, connectivity recovered with no issues. Maintenance finished at 15:01.",,,,2022-06-21,Server and Infrastructure Failures,mid,Degraded System Performance,
"The confctl command to depool a server was accidentally run with an invalid selection parameter (host=mw1415 instead of name=mw1415, details at T308100). There exists no ""host"" parameter, and Confctl did not validate it, but silently ignore it. The result was that the depool command was interpreted as applying to all hosts, of all services, in all data centers. The command was cancelled partway through the first DC it iterated on (Codfw).Confctl-managed services were set as inactive for most of the Codfw data center. This caused all end-user traffic that was at the time being routed to codfw (Central US, South America - at a low traffic moment) to respond with errors. While appservers in codfw were at the moment ""passive"" (not receiving end-user traffic), other services that are active were affected (CDN edge cache, Swift media files, Elasticsearch, WDQS…).The most visible effect, during the duration of the incident, was approximately 1.4k HTTP requests per second to not be served to text edges and 800 HTTP requests per second to fail to be served from upload edges. The trigger for the issue was a gap in tooling that allowed running a command with invalid input.","The issue was detected by both the monitoring, with expected alerts firing, and the engineer executing the change.Example alerts:07:46:18: <jinxer-wm> (ProbeDown) firing: (27) Service appservers-https:443 has failed probes (http_appservers-https_ip4) - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown07:46:19: <jinxer-wm> (ProbeDown) firing: (29) Service appservers-https:443 has failed probes (http_appservers-https_ip4) #page - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown","When provided with invalid input, confctl executes the command against all hosts, it should fail instead.","T308100: Invalid confctl selector should either error out or select nothing==Scorecard==Incident Engagement™  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were all engineering tools required available and in service?                                                     	yes	
Tooling                                       	Was there a runbook for all known issues present?                                                                 	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	9",2022-05-09,Configuration and Deployment Errors,high,Service Downtime,
"Since the previous evening (April 25), the Telia link from Eqord to Eqiad was down due to a fiber cut. At 05:00 the next morning, a Telia maintenance began that took down our remaining transports from Eqord, to Codfw and Ulsfo. As a result, we were entirely unable to reach the Eqord networking equipment. There was no end-user impact since Eqord is a network-only location with end-user traffic for Codfw and Ulsfo naturally going there directly instead of via Eqord.cathal@nbgw:~$ ping -c 3 208.115.136.238 PING 208.115.136.238 (208.115.136.238) 56(84) bytes of data.  --- 208.115.136.238 ping statistics ---  3 packets transmitted, 0 received, 100% packet loss, time 2055msTelia circuit failure logs: Apr 25 17:32:42 cr2-eqord fpc0 MQSS(0): CHMAC0: Detected Ethernet MAC Remote Fault Delta Event for Port 5 (xe-0/1/5)  Apr 26 05:11:57 cr2-eqord fpc0 MQSS(0): CHMAC0: Detected Ethernet MAC Remote Fault Delta Event for Port 3 (xe-0/1/3) Apr 26 05:11:57 cr2-eqord fpc0 MQSS(0): CHMAC0: Detected Ethernet MAC Local Fault Delta Event for Port 0 (xe-0/1/0) Apr 26 07:15:19 cr2-eqord fpc0 MQSS(0): CHMAC0: Cleared Ethernet MAC Remote Fault Delta Event for Port 3 (xe-0/1/3) Apr 26 07:15:47 cr2-eqord fpc0 MQSS(0): CHMAC0: Cleared Ethernet MAC Local Fault Delta Event for Port 0 (xe-0/1/0)Documentation:* Network design* Restricted document",,,"Grant Cathal authorization to be able to create remote hands cases (DONE)
== Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	Info not logged
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	0    	Manually escalated to netops
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	1    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	0    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	1    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	1    	
Total score	Total score                                                                                                                                              	7",2022-04-26,Server and Infrastructure Failures,low,[],
"mcrouter daemonset on mw-on-k8s: The mediawiki pod has 9 containers. We were working on reducting this number to 7, by introducing the mw-mcrouter service. In practice, our end goal was that each mw-on-k8s pod would use a standalone mcrouter pod running within the same node, instead of its own mcrouter container. From mediawiki's POV, mcrouter's location would be mcrouter-main.mw-mcrouter.svc.cluster.local:4442 instead of 127.0.0.1:11213. The same change was deployed on codfw the day before, but codfw has less traffic. This change increased the number of DNS requests towards CoreDNS, from an average of 40k req/s to 110k req/s, overwhelming the pods.",Antoine noticed an elevated number of events coming from the mediawiki channel on logstash. A few minutes later we got our first alert that we are running out of available php workers.,,"TBAAdd the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2024-04-17,Configuration and Deployment Errors,high,Degraded System Performance,
"At 03:16, the top-of-rack switch asw2-c-eqiad virtual chassis lost connectivity to FPC5, partitioning the network. This caused a hard down event for all hosts in rack C5 (netbox). It also caused additional instability due to how the virtual chassis works, and because it's incorrectly cabled up.We received a burst of both paging and non-paging alerts: Icinga reporting hosts down; BGP status; application-level errors; and MariaDB replica alerts. At least one user also reported via IRC that they couldn't access metawiki (almost certainly uncacheable traffic, due to logged-in state).At 03:22, asw2-c-eqiad:fpc5 came back online. Most systems recovered automatically, but some needed manual attention:* We received HAProxy failover alerts on dbproxy1018 through 1021, and those needed to be resolved by reloading haproxy manually, as expected.* Phabricator's dbproxy had failed over to a read-only replica (as expected) but Phabricator was unavailable for read-only tasks in read-only mode. When users attempted to view a task, they got an error page saying, Unhandled Exception (""AphrontQuery Exception"") #1290: The MariaDB server is running with the --read-only option so it cannot execute this statement This was resolved by reloading haproxy, but Phab was expected to be available for reads.* The Kubernetes API server alerted for high latency until kube-apiserver was manually restarted on both hosts. Documentation:* Grafana dashboard: Home",,,"T313384 Recable eqiad row C switch fabric, so that in the future a failure like this will only impact servers in rack C5.
*T313382#8090176 Move critical hosts, like DB masters, away from rack C5 until its top-of-rack switch is trustworthy.
* T313382#8090224 Add LibreNMS alerting (and runbook) for this scenario, which will speed up troubleshooting.
*T313879 Make read-only Phabricator operations possible when its database is in read-only mode.==Scorecard==Incident Engagement ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	no 	The IC was able to respond effectively to the downstream failures (DB, appservers, Phab, k8s, etc) but wasn't able to identify the root cause or troubleshoot in LibreNMS effectively due to lack of familiarity.
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	Only one SRE responded during the incident, so the incident doc was created afterward, to organize the timeline and followup items.
Process                                       	Was the public status page updated?                                                                               	no 	Not justified given the impact
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	no 	Reading Phab tasks for context was impossible due to its being unavailable in RO mode
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	yes	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2022-07-20,Server and Infrastructure Failures,high,Service Downtime,
Parsercache databases got overloaded due to a malfunctioning host which resulted on spikes of connections on the other 2 active hosts and increased latency on our mwapps servers.,"Icinga paged for pc1008 host that was having performance degradation
 18:43:14 <+icinga-wm> PROBLEM - MariaDB Slave SQL: pc2 #page on pc1008 is CRITICAL: CRITICAL slave_sql_state could not connect 
 https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_slave","The hardware performance degradation was hard to detect via the usual checks: broken BBU, degraded RAID, disks with errors that hasn't removed from the RAID, memory issues.... 
As nothing appeared to be broken, DBAs didn't consider pc1008 as the core of the issue. 
The fact that all the parsercache showed similar connections spike pattern made us think that the problem was on the other side of the spectrum (MW). We later learned thanks to Brad, that parsercache has a ""double write"" behaviour we didn't know of and if one of those fails, the others keep hanging until the request is processed or shutdown.","RFC] improve parsercache replication, sharding and HA: https://phabricator.wikimedia.org/T133523
* Investigate pc1008 for possible hardware issues / performance under high load: https://phabricator.wikimedia.org/T247787
** Once pc1008 is back full - repool it to make sure it is fully fixed after re-creating the raid
** Purge pc1010 old rows once it is out of rotation
* Parsercache sudden increase of connections: https://phabricator.wikimedia.org/T247788#5976651",2020-03-19,Performance and Load Issues,mid,Degraded System Performance,
"Active Wikidata Query Service instances were down (failing readiness probe). Blazegraph ""locked up"" on affected instances, rendering it unable to complete any work. As a result, queries would time out after exceeding nginx's timeout window without receiving a response from Blazegraph.Impact: The period of intermittent service disruption occurred between 20:34 and 22:27 UTC, of which about 75% of the time was spent in a state of total outage.","Issue was first detected when a critical alert was issued and plumbed through to IRC:[2020-09-02 20:42:31] <icinga-wm> PROBLEM - PyBal backends health check on lvs2009 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2003.codfw.wmnet, wdqs2007.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs-ssl_443: Servers wdqs2003.codfw.wmnet, wdqs2007.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs_80: Servers wdqs2003.codfw.wmnet, wdqs2007.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but Alert volume was manageable and fairly clearly indicated the problem (too many nodes were down in order to maintain full availability). Alert could perhaps explicitly show the connection between ""nodes are marked down but not pooled"" and ""the aforementioned is occurring because too many nodes are down"".","See previous incident's conclusions for some general context on our service expectations.Note that compared to the last incident, we identified the problematic queries as being specifically malformed queries (leading to MalformedQueryExceptions getting raised). In the previous incident, it was non-performant but not malformed queries that appeared to cause the deadlock.","Carried over from last incident)  Define SLOs, error budget for WDQS
*  Improve visibility into blazegraph queries causing errors
*  Add an entry in the WDQS Runbook on killer queries",2020-09-02,Server and Infrastructure Failures,high,Service Downtime,
"Impact:For approximately 5 minutes, Wikipedia and other Wikimedia sites were slow or inaccessible for many users, mostly in Europe/Africa/Asia.Documentation:*https://www.wikimediastatus.net/incidents/ft72m2rcs8tg",,,,2022-03-29,Performance and Load Issues,mid,Degraded System Performance,
"Triggered by a failed scap deployment of MediaWiki, the team dug into figuring out the underlying causes, unearthing a network and CPU saturation at the WikiKube Kubernetes cluster api servers. Aside from the issue being witnessed by 2 deployers, no other discernible impact was observed. As an inadvertent result of the investigation, scap deployments became faster.They were multiple interrelated causes that contributed and made debugging slower# For the past few months, during any MW deploy, we've been saturating k8s control plane CPU in both eqiad and codfw. The saturation also manifested as slowly increasing latencies for blackbox probes# More and more worker servers were being added to the WikiKube clusters# 1 of the kubemaster VMs was on a 1G Ganeti node, the other one on a 10G ganeti node. After the first incident, the second kubemaster (the one on 1G ganeti node) took over part of the load. Network wise, it was unable to sustain the level of traffic and ended up having ~6k TCP retransmits per second.# At the end of the week before the incident, we had enabled OpenTelemetry collector in eqiad, following a successful deployment in codfw# We were in the process of adding more kubernetes api-servers in both datacenters.","The issue was detected due to a failed deployment and this necessitates right a human witness. There were alerts of the following nature
FIRING: ProbeDown: Service kubemaster1002:6443 has failed probes
These alerts have been firing for some time every now and then. However, # Due to the very slow increasing nature of the underlying cause (latency due to CPU saturation) it wasn't possible to correlate it. Furthermore when they accompanied caused an incident, corrective action was already being taken (moving Kubernetes masters to dedicated machines)
# The CPU saturation events were short enough to not show up in the ""Host Overview"" Grafana dashboard. This effect was exacerbated by the fact the CPU panels in that dashboard use a resolution of 1/3.","Growing pains for the WikiKube cluster during the mw-on-k8s project were expected. There had already been Kubernetes API related incidents and the team, after an investigation, concluded that adding more etcd capacity, in the form of 3 new dedicated stacked API servers would alleviate those problems. However,* It appears there were also short CPU saturation problems that were never directly detected that caused eventually a few failed deployments.
* The default configuration of the OpenTelemetry collector with k8sattributesprocessor enabled increased the level of network traffic egressing API servers by > 50%. Cumulative traffic egressing from all API servers reached 272MB/s (~2.1Gbps), enough to saturate heavily any 1Gbps link.
* There are 2 different methods of load balancing regarding traffic to the API servers. One from workloads outside the cluster itself (i.e. kubelet, scap, kube-proxy, rsyslog) and one from workloads inside the cluster, i.e. OpenTelemetry collector, Calico, eventrouter (we call these cluster components). The former go via LVS, the latter via Kubernetes probabilistic DNAT. The result is that the 2 methods might end up sending traffic to different sets of nodes, complicating things.","A decision was made to add 10G cards to all new WikiKube api servers. T366204 and T366205. 
* Poke otelcol upstream about using the Kubelet /pods interface    https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/14475#issuecomment-2145825911 
* Investigate whether the resolution of https://grafana.wikimedia.org/d/000000377/host-overview?orgId=1&refresh=5m&viewPanel=3 can be 1/1 instead of 1/3?Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	not needed
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	12 	Counting public entry as ""yes""",2024-05-28,Performance and Load Issues,mid,Degraded System Performance,
"While reconfiguring all services to use our service proxy middleware to make remote procedure calls, a faulty configuration was deployed by yours truly for mobileapps at 08:40. This caused mobileapps to create mobile-html content with broken css and js links for pages regenerated during the day.The issue was reported at 16:40 and the issue was quickly reverted. Then we needed a few hours to actually clear all the caching layers (RESTBase, edge caches). All pages affected were purged by 20:20.",,,"https://phabricator.wikimedia.org/T268484 Tracking task.
* https://phabricator.wikimedia.org/T262437 The bug caused by this incident (Page content service is deployed with localhost links).
* https://phabricator.wikimedia.org/T264340 Create test for api/rest_v1/page/mobile-html* The biggest actionable is of course to always wait for validation from service owners before merging a patch - and the whole outage would've been avoided if that was done. Anything else listed here is purely a second-order actionable.
* While this deployment was the result of bad judgement, SRE need to be able to deploy a configuration change with confidence. The fact that the mobileapps spec tests all passed in staging lulled SRE into a false sense of security. The OpenAPI spec should be extended to include a test for the aforementioned URLs. (TODO: create task)
* We need staging to become a functional environment where we can test more than just a swagger spec test. Maybe linking it to restbase-dev, and making it possible to compare results of urls with production would help (TODO: create task)",2020-09-09,Configuration and Deployment Errors,mid,Degraded System Performance,
"At 9:21, ms-be1062 had a crash in its network stack (tracked at T281107) which caused the server to blackhole traffic. This caused a lot of requests to swift to reach timeouts, causing the consequent starvation of resources for the thumbor cluster workers, which were mostly waiting for a response from swift. This happened during a rebalance operation of the swift cluster, which we've seen in the past can impose quite some stress on the swift backend in our current configuration. Traffic was diverted to the codfw cluster for generating thumbnails at 09:26, thus minimizing impact for users. Rebooting ms-be1062 at 9:38 solved the issue instantly.",,,"Understand why pybal would not depool any of the backends even if they were returning 503s even to monitoring (TODO: Create task)
* Track down the kernel bug that caused ms-be1062 to blackhole traffic. It’s also worrisome that a swift cluster would become unresponsive in such a situation.  https://phabricator.wikimedia.org/T281107",2021-04-26,Server and Infrastructure Failures,mid,Degraded System Performance,
,,,"Incident tracking task
*Investigate if stopping mysql with buffer_pool dump between 10.4 versions is safe==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	     	
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	     	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	1    	
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	1    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	0    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	0    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	     	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	     	
Total score	Total score                                                                                                                                              	5",2022-03-10,,,,
"Before the incident:The configuration setting for Wikibase clients for which ranges of items to read from the old wb term store and which ones from the new, was not passed through from InitialiseSettings.php because of a typo. Instead, the default value in the Wikibase extension was used, which was to read all items from the old store for wmf.16, deployed to the group2 wikis, and to read only from the new store for wmf.18, deployed to the group 0 and 1 wikis. The group 0 and 1 wikis were potentially reading from unmigrated rows, causing pages with missing data to be rendered and cached for Commons and Wikidatawiki; see  for the first report of the issue with group 0 and 1 wikis, and  for a very detailed chart of which settings were in place when.This was deemed UBN, and a deploy made to fix the configuration setting typo (). This resulted in group 2 wikis reading from the new store for items with QID up to 8,000,000. This caused an inordinate load on servers ultimately resulting in an outage.It is now clear that this is the same root cause as the previous day's outage; wmf.18 was deployed on Monday Feb 10 without the config change and there were no problems. See Incident documentation/20200206-mediawiki for details on the previous day's outage.Most of the documentation of this issue is in the comments on  and the follow-up task","Icinga alerted immediately, starting with MediaWiki exceptions and fatals per minute. The icinga-wm bot flooded out of the IRC channel due to too many reports.",What weaknesses did we learn about and how can we address them?,"Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Make sure dashboards are up to date for new features/functionality  before planned transitions to them begin
*  - Consider having a linter that could catch config file entries that set unused variables
*  - Determine why switching group2 wikis to read from the new wb terms store caused the issue
*  - Make sure that UBN/emergency deploys go through releng and SRE teams so that everyone is in the loop (and if SRE folks are in the middle of something urgent, they can ask deployers to wait a bit). See: Deployments/Emergencies in draft status.
*  - Consider bundling config and branch versions together for deployments (User:20after4 knows about this)",2020-02-07,Configuration and Deployment Errors,high,Service Downtime,
"During the scheduled maintenance to upgrade the PDUs in rack A5, CyrusOne flipped the incorrect breaker on the breaker panel, prior to pulling the PDU's power cord out from its circuit.  This resulted in all servers in rack A5 losing power to both its primary and secondary power feeds.  The affected hardware in rack A5 booted back up, once CyrusOne realized the mistake and flipped the breaker back on.* 15:45 <+icinga-wm> PROBLEM - Host graphite2003 #page is DOWN: PING CRITICAL - Packet loss = 100%* 15:45 <+icinga-wm> PROBLEM - Host maps2005 is DOWN: PING CRITICAL - Packet loss = 100%* 15:55 <+icinga-wm> PROBLEM - MariaDB read only s8 #page on db2079 is CRITICAL: Could not connect to localhost:3306* 15:56 <+icinga-wm> PROBLEM - MariaDB read only m1 #page on db2132 is CRITICAL: Could not connect to localhost:3306* ..* 16:00 <+icinga-wm> RECOVERY - MariaDB read only s8 #page on db2079 is OK",,,,2022-07-12,Server and Infrastructure Failures,mid,Service Downtime,
System-wide renaming of Kyverno policies overloaded Toolforge Kubernetes control nodes rendering API unresponsive* Lack of working Kubernetes API prevented rollback,This issue was detected by Arturo -- during ongoing deployment work he noticed that the k8s API had become unresponsive.Some alerts followed but only after the incident was open and fixes in progress.,,"Fix HA proxy load-balancer health check monitor to not poll nodes where the API is not responding (phab:T367349)
* Fix incorrect registry controller config  https://gitlab.wikimedia.org/repos/cloud/toolforge/registry-admission/-/merge_requests/5
* Do some more load testing with kyverno before deploying
* scale up coredns replicas (phab:T333934)
* Verify that kyverno policies match our namespace (phab:T367350)==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	   	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	   	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	   	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	   	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	   	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	   	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	9",2024-06-12,Performance and Load Issues,high,Service Downtime,
"After a code change   rolled out in this week's train, the GlobalUsersPager class (part of CentralAuth) produced expensive DB queries that exhausted resources on s7 database replicas.Backpressure from the databases tied up PHP-FPM workers on the API servers, triggering a paging alert for worker saturation. The slow queries were identified and manually killed on the database, which resolved the incident.Because the alert fired and the queries were killed before available workers were fully exhausted, the impact was limited to s7. Full worker saturation would have resulted in a complete API outage.Because only two engineers responded to the page and the response only took half an hour, we decided not to designate an incident coordinator, start a status doc, and so on. We didn't need those tools to organize the response, and they would have taken time away from solving the problem.Documentation:* Phabricator task detailing the slow query* API server RED dashboard showing elevated latency and errors; php-fpm workers peaking around 75% saturation; and s7 database errors* Same dashboard for the app servers showing measurable but lesser impact",,,"Revert the patches generating the slow queries - done  
* Later (2022-04-06) it was discovered the query killer was using the old 'wikiuser' name, which prevented it from acting. Fixed in , deploying soon.==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	Info not logged
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	paged via batphone
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	paged via batphone
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	paged via batphone
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	1    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	1    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	1    	
Total score	Total score                                                                                                                                              	9",2022-03-31,Code and Application Bugs,mid,Degraded System Performance,
"During the upgrade of the WMCS OpenStack control plane, all cloud VMs presented with the wrong originating IP for outbound traffic.  That broke several services on many VMs, most importantly DNS, LDAP, and NFS.","The problem was immediately evident, as many of the issues produced shinken and icinga alerts.  The team working on the upgrade didn't respond immediately because some level of background alerting was already expected as part of the upgrade process.",,"Some Phabricator tickets opened as a result of this incident:* Various user visible errors in Cloud VPS projects following OpenStack upgrade on 2019-10-07 https://phabricator.wikimedia.org/T234834
* CloudVPS: m5-master databases for openstack may require re-enconding https://phabricator.wikimedia.org/T234830
* nova-conductor running out of mysql connections https://phabricator.wikimedia.org/T234876
* CloudVPS: update DNS record for eqiad1 routing_source_ip https://phabricator.wikimedia.org/T234836",2019-10-07,Configuration and Deployment Errors,high,API or Integration Failures,
"For about 1 hour and 40 minutes, Toolforge services and VMs in Cloud VPS may have experienced connectivity issues	For about 1 hour and 40 minutes, Toolforge services and VMs in Cloud VPS may have experienced connectivity issues	For about 1 hour and 40 minutes, Toolforge services and VMs in Cloud VPS may have experienced connectivity issuesAfter a kernel upgrade for several Cloud VPS network components (cloudnet, cloudgw servers; see T291813), we found problems with Toolforge NFS in Kubernetes. Later LDAP connections were found to be affected. Eventually it turned out to be a problem with all ingress traffic to the network edge for cloud VMs (except those with floating IPs, which were unaffected). The issue was resolved by rolling back the kernel upgrade.Impact: For about 1 hour and 40 minutes, Toolforge services and VMs in Cloud VPS may have experienced connectivity issues=Scorecard=           	Question                                                                                                                                                 	Score	NotesPeople     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	1    	People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	0    	People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	1    	People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	1    	People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	1    	Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	Total score	Total score                                                                                                                                              	9    	= Actionables =* Incident tracking task, T294853* Improve automated testing and monitoring of cloud networking, T294955* Set up static route for cr-codfw, T295288* Avoid keepalived flaps when rebooting servers, T294956",,,,2021-11-02,Server and Infrastructure Failures,mid,Degraded System Performance,
"At 09:23 UTC, alerts indicated connectivity issues to the Eqsin cluster in Singapore. At 09:31 UTC, @Ema deployed a DNS change to depool the Eqsin cluster. This diverted most of its assigned traffic to Ulsfo, and some to Esams. At 09:35 UTC traffic started recovering, with traffic back to regular levels at 09:45 UTC. The 15-minute window is attributed to DNS caches expiring (e.g. at ISPs and on client devices). The connectivy issues were resolved later that day, and at 18:50 UTC @CMooney repooled the Eqsin cluster, with traffic back to regular levels in Eqsin by 19:00 UTC.Impact: For about 35 minutes from 09:20 to 09:45 UTC, the wikis were largely unreachable from countries normally served by the Singapore DC (including India, Hong Kong, and Japan).Documentation:* Wikimedia DNS: DC geo map* Grafana: Navigation Timing by Continent 2021-06-15* Grafana: Traffic volume by DC 2021-06-15",,,"Public tracking task: https://phabricator.wikimedia.org/T284986
* TODO Per-country Frontend Traffic dashboards T286554",2021-06-15,Server and Infrastructure Failures,mid,Service Downtime,
This was due to an issue at the Amsterdam Internet Exchange (AMS-IX).Documentation:* https://www.wikimediastatus.net/incidents/jnqvz8gljzhy*Restricted document,,,,2022-04-06,External Dependencies,mid,Service Downtime,
"While working on updating EventGate to support Prometheus, Andrew Otto deployed the changes to eventgate-analytics in codfw (then-active DC).  This change removed the prometheus-statsd-exporter container in favor of direct Prometheus support, as added in recent versions of service-runner and service-template-node.The deploy went fine in the idle ""staging"" and ""eqiad"" clusters, but when deploying to codfw, request latency from MediaWiki to eventgate-analytics spiked, which caused PHP worker slots to fill up, which in turn caused some MediaWiki API requests to fail.The helm tool noticed that the eventgate-analytics deploy to codfw itself was not doing well, and auto-rolled back the deployment:$ kube_env eventgate-analytics codfw; helm history productionREVISION	UPDATED                 	STATUS    	CHART           	APP VERSION	DESCRIPTION[...]4       	Wed Jul 14 16:07:12 2021	SUPERSEDED	eventgate-0.3.1 	           	Upgrade ""production"" failed: timed out waiting for the co...5       	Wed Jul 14 16:17:18 2021	DEPLOYED  	eventgate-0.2.14	           	Rollback to 3Impact: For ~10 minutes, MediaWiki API clients experienced request failures.Documentation:* Grafana: Envoy telemetry* Grafana: Application Servers dashboard* Grafana: Envoy telemetry / Upstream latency",,,"Figure out why this happened and fix.  Based on this log message, it seems likely that a bug in the service-runner prometheus integration caused the nodejs worker process to die. [DONE]
** Further investigation uncovered that require('prom-client') within a worker causes the observed issue.  Both service-runner and node-rdkafka-prometheus require prom-client.  It was proposed to patch  node-rdkafka-prometheus to handle passing in the prom-client instance. 
** node-rdkafka-prometheus is an unmaintained project, so we have forked it to @wikimedia/node-rdkafka-propetheus and fixed the issue there.  Additionally, if this issue in prom-client is fixed, we probably won't need the patch we made to node-rdkafka-prometheus for this fix.",2021-07-14,Configuration and Deployment Errors,mid,API or Integration Failures,
"Following a large bot import to the Russian Wikinews, expanding the size of that project to 13 million pages, slow queries originating from ruwikinews's usage of the DynamicPageList extension  (also known as ""intersection"") overloaded the s3 cluster of databases, causing php-fpm processes to hang/stall, eventually taking down all wikis with it. The outage was resolved by disabling the DynamicPageList extension on ruwikinews and aggressively killing queries on s3 replicas. Normally, DPL's database queries roughly scale to the size of the smallest category being intersected. This would be bad enough, as ruwikinews has categories that are orders of magnitude higher than other wikis with this extension. However, in this case MariaDB chose a query plan that involved scanning the entire categorylinks table. The query in question seen during the outage took more than 3 minutes to finish on an idle replica.The DPL query was using a sort by page_id, descending (instead of the more common sort by c1.cl_timestamp that is DPL's default sort method). According to the EXPLAIN, MariaDB decided to optimize this by using the PRIMARY key on (cl_from, cl_to) [Remember, cl_from is a foreign key to the page_id of the page in the category, and cl_to is the name of the category in question]. This would have been a good query plan if most of the rows of the В_мире category matched the query (or even just 18 of them), especially since they all had high cl_from since they were all newly created. However, since no rows matched the query, this resulted in a full table scan of the categorylinks table (44,435,648 rows). The other possible query plan, using the (cl_to, cl_timestamp) index to only look at the 180,231 rows for the В_мире category and then filesorting the results, would probably have been more efficient in this case due to no rows matching the query. Potentially, disabling the sort by page_id feature of DPL, and instead only allowing the sort by c1.cl_timestamp (c1 in this case was Опубликовано) would likely result in more efficient queries, relatively speaking. For context the reason the default method of sorting is sort by the first category's timestamp, is to allow the first category to be a ""published"" category, and allow showing results in order of when they were published. It would be even more efficient (And reduce likelihood of filesorts) if the sort was by the timestamp of the smallest category, although that may not be the behaviour desired by users.It appears the triggering event was the creation of n:ru:Category:В мире which was linked on about 180,000 pages that also had this DPL query on them due to a template. This page creation triggered a large number of parse jobs (so links could change from red to blue) all making the same, very slow DPL query, which began to overload the s3 DB, which snowballed. The caching mitigation introduced during the previous incident did not work properly as the query often did not complete prior to a timeout involved, preventing it from being cached. This was true even before the incident when the DB replicas were not under extreme load.Overall were 30 minutes of high latencies, failing to respond, or fatal errors, affecting wikis due to unavailable PHP-FPM workers. Based on traffic graphs the outage impacted to approximately a 15% of all incoming HTTP requests for wikis, those being either lost, suffering high latencies or 5XX error codes. The main impact was uncached requests, suffering a 0% availability during several moments of the outage, on all wikis.Impact: For 30 minutes, 15% of requests from contributors on all wikis were responding either slowly, with an error, or not at all. There were also brief moments during which no readers could load recently modified or uncached pages.","Icinga sent two pages at 10:33 for Not enough idle PHP-FPM workers for Mediawiki on the appserver and api_appserver clusters.The first user report on IRC appears to have been at 10:35 in #wikimedia-sre: <RhinosF1> Meta is down.Because this was a full outage, every host was individually alerting and so were services that depend upon MediaWiki. Each appserver triggered two alerts, like:<icinga-wm> PROBLEM - Apache HTTP on mw2316 is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/Application_servers
<icinga-wm> PROBLEM - PHP7 rendering on mw2316 is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/Application_servers/Runbook%23PHP7_rendering
 icinga-wm sent 145 messages to #wikimedia-operations between 10:34 and 10:36 before being kicked off the Libera Chat network for flooding. That IRC channel was unusable and discussion was moved to #wikimedia-sre and then #mediawiki_security.",,"High-level discussion and brainstorming is happening in T287380: Decide on the future of DPL. Out of that, some specific actionables have been identified: *   Send queries to ""vslow"" database group
*  T287916 Disable DPL on wikis that aren't using it
*  T263220 Limit concurrency of DPL queries
*T287983 Raw ""upstream connect error or disconnect/reset before headers. reset reason: overflow"" error message shown to users during outage
* T288180 Investigate Extension:GoogleNewsSitemap query performance
**T288227 Add concurrency limiting to GoogleNewsSitemap using PoolCounter
* Maybe it would be cool if job runners slow down if a DB overload is detected
*Consider removing ordermethod=created as an unnecessary footgun
* ...",2021-07-26,Performance and Load Issues,high,Service Downtime,
"External demand for an expensive MW API query, caused the MW API web servers to become overall slower to respond to other queries as well. Some of these queries became sufficiently slow as to trigger our execution timeout of 60 seconds. The Recommendation API service was partially unavailable for about 35-40 minutes as it uses the MW API for part of its work.",,,,2020-01-27,Performance and Load Issues,mid,API or Integration Failures,
"After the row C uplinks change (part of T313463) was completed successfully, the same procedure got applied to row D's link to cr1-eqiad. While the asw side went fine (and took down the link as planned, waiting for the cr side to be reconfigured), the configuration change on the cr1 discarded traffic toward that switch. Traffic flowing from cr2 to row D was not impacted. Additionally the VRRP gateway was set to cr2, so outbound traffic from row D was not impacted as well.Troubleshooting was made more difficult as bast1003 is in row D causing management access to be lost. The change was done with an automatic rollback timeout of 2min. At that 2 min mark, the change got automatically reverted, restoring full connectivity before I was able to connect through a different bast host.The exact root cause of why the traffic was discarded is so far still unknown. Safe troubleshooting (eg. remove ae4 IP config, to test lower layer connectivity) will be done at a later date.The 2 dbproxies affected (for m3, m5) were passive, they were reloaded manually afterwards to point back into the usual primary hosts.","Ayounsi figured something was wrong when he lost connectivity to cr1-eqiad and bast1003.Multiple alerts triggered, some of the relevant ones:* 14:53 <jinxer-wm> (ProbeDown) firing: Service api-https:443 has failed probes (http_api-https_ip4) #page - https://wikitech.wikimedia.org/wiki/Runbook#api-https:443 - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown
* 14:53 <icinga-wm> PROBLEM - High average GET latency for mw requests on appserver in eqiad on alert1001 is CRITICAL: cluster=appserver code=200 handler=proxy:unix:/run/php/fpm-www-7.4.sock https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad+prometheus/ops&var-cluster=appserver&var-method=GET
* 14:54 <jinxer-wm> (PHPFPMTooBusy) firing: Not enough idle php7.4-fpm.service workers for Mediawiki api_appserver at eqiad #page - https://bit.ly/wmf-fpmsat - https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad%20prometheus/ops&var-cluster=api_appserver - https://alerts.wikimedia.org/?q=alertname%3DPHPFPMTooBusy
* 14:55 <icinga-wm> PROBLEM - haproxy failover on dbproxy1016 is CRITICAL: CRITICAL check_failover servers up 2 down 1: https://wikitech.wikimedia.org/wiki/HAProxy
* 14:55 <icinga-wm> PROBLEM - haproxy failover on dbproxy1017 is CRITICAL: CRITICAL check_failover servers up 2 down 1: https://wikitech.wikimedia.org/wiki/HAProxy
As it was during a maintenance the root cause was easy to identify.However, if this had happened on its own (even though unlikely), the root cause would have taken more time to identify. Especially as Icinga is running from row C, and thus not seeing the failure.",,"Root cause analysis: Cr1-eqiad comms problem when moving to 40G row D handoff - T320566
*To be discussed: how can we make the servers more resilient in face of such event?==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	9",2022-10-06,Configuration and Deployment Errors,mid,Degraded System Performance,
"From 07:31 to 08:16 UTC, there was increased latency and error rates for some MediaWiki cache misses and authenticated requests. The incident predominantly affected API users and bots on commons.wikimedia.org. API request latency for some requests went up by 5 seconds, and error rates upto 25%.The issue was found to be caused by a bot causing increased load on the commonswiki databases, and consuming API webserver resources. Later in the time range, regular users and other wikis were affected as well through cross-wiki features involving Commons.",,,Tracking task:  https://phabricator.wikimedia.org/T280232,2021-04-15,Performance and Load Issues,mid,Degraded System Performance,
"During network maintenance, the Mailman runner process for delivering emails out of the queue crashed because it couldn't connect to the MariaDB database server and was not automatically restarted. As a result, Mailman continued to accept and process incoming email, but outgoing mail was queued. This was first reported in T331626, via Gerrit Reviewer Bot being broken. It was determined that the mediawiki-commits list was not delivering mail, leading to discovery of a growing backlog of 4k queued outgoing emails in Mailman. The mailman3 systemd service was restarted, causing all of the individual runner processes to be restarted, including the ""out"" runner, which began delivering the backlog. It took slightly over 5 hours for the backlog to be cleared.The network maintenance in question was T329073: eqiad row A switches upgrade. lists1001.wikimedia.org (the Mailman server) was not listed on the task but it was affected and downtimed (see T329073#8672655). icinga monitoring correctly detected the issue (see T331626#8680354), but was not noticed by humans.","Automated monitoring was the first to detect the issue, less than 10 minutes after the runner crashed:14:43: <+icinga-wm> PROBLEM - mailman3_runners on lists1001 is CRITICAL: PROCS CRITICAL: 13 processes with UID = 38 (list), regex args /usr/lib/mailman3/bin/runner https://wikitech.wikimedia.org/wiki/Mailman/MonitoringThe correct alert fired, as it explicitly checks that the expected number of runner processes are actually running.However, it was not investigated until a human reported it, nearly 2 days later.",,"Identify how lists1001 got missed during the eqiad row A switch upgrade preparation
*Add monitoring to out queue size
*Consider making the runner crashed monitoring page? If a runner crashes, it definitely needs manual intervention. And crashes are much much rarer than during the initial MM3 deployment. 
*Someone should probably figure out why lists1001's web service is flaky and randomly going downCreate a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard)  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	   	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	   	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	   	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	IRC alert sent but not noticed
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-03-09,Server and Infrastructure Failures,mid,Degraded System Performance,
"A large video-scaling job made server mw1469 so busy that its capacity was maxed out by ffmpeg processes. Since mw1469 was both a jobrunner and a videoscaler this led to alerts for both jobrunner and videoscaler services.For the first part of the incident alerts could be seen on IRC but there had been no pages yet. Alerts were flapping, also on mw1469 specifically. Around 19:24 it eventually triggered a page. Dzahn and Aokoth were paged andstarted looking at it and kept an eye on it for a while. Since it kept flapping the runbook was followed (Application servers/Runbook#Jobrunners.) and mw1469 was depooled from videoscaler, but pooled in jobrunner. The ffmpeg processes on mw1469 were killed. This protected the jobrunner which is much more important than the videoscaling (quoting runbook). Jobrunner alerts recovered. A little while later, server mw1495 was depooled from jobrunner and turned into a dedicated videoscaler. Videoscaler alerts recovered.","First Icinga started reporting via icinga-wm on IRC, a little later SRE on duty got paged via Alertmanager.",OPTIONAL: General conclusions (bullet points or narrative),"reopen https://phabricator.wikimedia.org/T279100 ?
* dedicated alert for videoscalers https://phabricator.wikimedia.org/T338220==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-05-19,Performance and Load Issues,mid,Degraded System Performance,
Wikidata API calls were not getting responses (getting timeouts) due to DB read load due to backported changes reducing deadlocks around writing to the new terms store for wikibase.,"Humans in Telegram chat.
Confirmed by Addshore.",We could do with more alarms on things that often indicate a problem,"More wikidata alerting https://gerrit.wikimedia.org/r/#/c/547404/ https://grafana.wikimedia.org/d/TUJ0V-0Zk/wikidata-alerts
*  Add alerting to API response times for wikidata
*  Add alerting for wikidata edit rate (if below 100 per minute something somewhere is wrong)
*  Add alerting for MASSIVE database read rate on s8",2019-10-31,Schema and Database Issues,high,API or Integration Failures,
"On Dec 2nd 2020 we Analytics team migrated Hive's netflow data set from the `wmf` database to the `event` database; and accordingly, it's HDFS data was moved from `/wmf/data/wmf/netflow` to `/wmf/data/event/netflow`. This migration was part of T231339, you can check the migration plan here. The migration went well, but a detail was missing in the migration plan: Hive's `event` database, as well as the corresponding HDFS directory `/wmf/data/event`, have a purging job set up to delete all data older than 90 days within that database/directory. Deletion is necessary to abide to WMF's data retention guidelines. That job runs periodically on a daily basis, at midnight UTC. If you want to prevent the purging job to delete a given data set within its premises, you have to white-list it in the job's config (via a regex). We did not do that, and as a result, at the end of the day (00:00 UTC) the purging job deleted all netflow data except for the last 90 days.",,,"Proposed mitigation: Remove --skipTrash flag from some of the data deletion scripts. Namely, the ones that delete data in a generic way for a dynamic number of data sets (like the one in this incident). If we do so, the deleted data will still live in the .Trash folder for 30 days. Usually deleted data is privacy-sensitive, so keeping it for an extra 30 days could be a problem. However, we can consider the .Trash folder as a temporary safety backup, and backups are treated differently by WMF's data retention guidelines; they are allowed to be kept for longer than 90 days. One caveat to take into consideration is that even just 30 days of extra data can become a considerable amount of data to keep and this might in some cases put pressure on the cluster's capacity. https://phabricator.wikimedia.org/T270431
* Proposed mitigation: The deletion scripts should have a safeguard that prevents them to delete the data if its size is unexpected. You can run a deletion script telling it with a flag, how much data you expect it to delete (D). If the data to be deleted is more than D, then the script will no-op, warn and exit with non-0 code. With this feature, we could make sure that no unexpected extra data gets deleted. At some point we might receive alerts when the data size naturally grows, but that's acceptable in exchange of the extra protection. https://phabricator.wikimedia.org/T270433
=== More long-term solutions ===
* Refactor the sanitization process and re-organize the data into a better database structure. We should use explicit names like: temporary_events or unsanitized_events.
* Having a data governance solution would help us specifying which data sets are to be deleted and when (and even maybe how!). Let's keep in mind that deletion should be the default for new data sets, otherwise we'd lose control of what data we keep.",2020-12-02,Data Management and Retention,mid,Data Loss or Corruption,
"The backfill process for Graphite metrics silently failed during the Bullseye migration. A subset of metrics experienced loss for data points before October 11th 2021	The backfill process for Graphite metrics silently failed during the Bullseye migration. A subset of metrics experienced loss for data points before October 11th 2021	The backfill process for Graphite metrics silently failed during the Bullseye migration. A subset of metrics experienced loss for data points before October 11th 2021Impact: The backfill process for Graphite metrics silently failed during the Bullseye migration. A subset of metrics experienced loss for data points before October 11th 2021The process of reimaging a Graphite host is as follows:# reimage host# let metrics flow for a few days to validate the host is working# backfill the rest of the data (online, no downtime) from the other Graphite host following https://wikitech.wikimedia.org/wiki/Graphite#Merge_and_sync_metricsDuring the Bullseye migration the backfill process failed (undetected) for a subset of metrics, leading to metric data loss once the Bullseye migration was complete (i.e. graphite2003 first and then graphite1004 were reimaged and put back in service)","Some Grafana dashboards backed by Graphite showed partial data (starting Oct 11 or Oct 21) for a subset of metrics, as reported by  Lucas Werkmeister in https://phabricator.wikimedia.org/T294355","The whisper-sync backfill process is not as reliable as previously thought, no visible errors were logged and/or detected.","Understand the feasibility (and need) to back up a small subset of important metrics https://phabricator.wikimedia.org/T294355#7464552
* Revise the backfill procedure to be more robust in the face of similar failures in the future (e.g. run a full rsync first, then backfill only the gap) https://phabricator.wikimedia.org/T296295
* Perform validation post-sync / post-backfill to check the number of datapoints across all metric files is roughly in sync between hosts https://phabricator.wikimedia.org/T296295
*Continue (and speed up) the Graphite retirement plan https://phabricator.wikimedia.org/T228380=Scorecard=           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	1    	
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	1    	
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	1    	
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	1    	
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	1    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	0    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	0    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	0    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	1    	
Total score	Total score                                                                                                                                              	9",2021-10-29,Data Management and Retention,mid,Data Loss or Corruption,
"For up to an hour, some regions experienced a partial connectivity outage. This primarily affected the US East Coast for ~13 minutes, and Russia for 1 hour. A subset of readers and contributors from these regions were unable to reach any wiki projects. Services such as Phabricator and Gerrit Code Review were affected as well. It was a partial issue because the network malfunction was limited to one of many providers we use in the affected regions.	For up to an hour, some regions experienced a partial connectivity outage. This primarily affected the US East Coast for ~13 minutes, and Russia for 1 hour. A subset of readers and contributors from these regions were unable to reach any wiki projects. Services such as Phabricator and Gerrit Code Review were affected as well. It was a partial issue because the network malfunction was limited to one of many providers we use in the affected regions.	For up to an hour, some regions experienced a partial connectivity outage. This primarily affected the US East Coast for ~13 minutes, and Russia for 1 hour. A subset of readers and contributors from these regions were unable to reach any wiki projects. Services such as Phabricator and Gerrit Code Review were affected as well. It was a partial issue because the network malfunction was limited to one of many providers we use in the affected regions.Around 16:11 UTC our (non-paging) monitoring and users reported connectivity issues to and from our Eqiad location. Traceroutes showed a routing loop in a provider's network.At 16:19 UTC, using the provider's APIs, we asked the provider to stop advertising the prefixes for Eqiad on our behalf.At 16:24 UTC, the first reports of recoveries arrived as the change propagated through the DFZ.Unfortunately, due to the preponderance of the Eqiad impact and its recovery, we didn't notice that it also impacted users from Russia to reach the wikis through Esams.  We also didn't receive any NEL reports from most Russia users until 16:24 or later, as the location we use for NELs about Esams is itself Eqiad. At around 17:15 UTC, our monitoring shows a full recovery, indicating the issue being resolved upstream.Impact: For up to an hour, some regions experienced a partial connectivity outage. This primarily affected the US East Coast for ~13 minutes, and Russia for 1 hour. A subset of readers and contributors from these regions were unable to reach any wiki projects. Services such as Phabricator and Gerrit Code Review were affected as well. It was a partial issue because the network malfunction was limited to one of many providers we use in the affected regions.Due to the span of this provider's network, the further a client is from Eqiad the more likely they will be using that provider to reach our network and thus could be impacted.Documentation:*Grafana: Ripe Atlas=Scorecard=           	Question                                                                                                                                                 	Score	NotesPeople     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	Information not logged, scoring 0People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	No page occurredPeople     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	No page occurredPeople     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	No page occurredProcess    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	1    	Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	No mention of previous occurrence, scoring 0Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	NEL paging which was deployed shortly afterTooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	0    	NEL receive was degraded due to issues in eqiad which masked issue at other siteTooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	Total score	Total score                                                                                                                                              	6",,,"network provider issues causing all Wikimedia sites to be unreachable for many users
**patch to make NEL alert paging
*Request RFO from provider",2021-10-08,Server and Infrastructure Failures,high,Service Downtime,
https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/923650 was accidentally +2'd without intent to deploy (like on a software repo rather than a config repo). This could have led to confusion for a future deployer.,,OPTIONAL: General conclusions (bullet points or narrative),"Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFire  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-05-30,Configuration and Deployment Errors,low,[],
"After testing https://gerrit.wikimedia.org/r/c/operations/puppet/+/826785 in cp6016. ATS layer prevented Phabricator's (and theoretically, other services too, but it was less impacting) session cookies reaching the service's origin server. 474fb2d didn't work as expected because it hit an ATS bug/missdocumented feature. Cookie data was stored in ts.ctx during do_global_post_remap() and restored in do_global_cache_lookup_complete() but for some reason ts.ctx gets wiped in the middle of those two hooks. 474fb2d also missed the step now performed in hide_cookie_store_response().After the change was reverted, login issues stopped and users were able to perform regular Phabricator logged-in actions.","The issue was quickly pointed out by several people on IRC, in the SRE channel as it impacted highly visible ongoing work of several engineers and volunteers (dcaro, dhinus, jynus, claime).No alerts where sent because it only affected logged-in users, while the site acted normally for anonymous users.","This probably should be written by the service owner, but in my understanding, a new test was setup on production with an undetected bug, causing traffic disruption (cookie filtering). I don't know if there is much else to do as the same bug is unlikely to hit again; except maybe enabling some kind of automatic monitoring detection of a similar issue.","Improve monitoring to detect ability to log in/perform logged in actions on production Phabricator (?)==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	no pages
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	no pages
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	no pages
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	8",2022-08-26,Code and Application Bugs,mid,API or Integration Failures,
"An external client issued a really large number of requests for what appears to be nonexistent original images uploaded to commons that caused the thumbor service to return 503s and eventually paged.Impact: Approximately 291,000 thumb-nailing/resizing for images (which is what the thumbor service is for) requests failed and were returned as 503s (an HTTP error code). 76,000 of those from the offending IP. The incident affected eqiad and esams, so predominantly Europe and East Coast of the American Continents.","We were paged by icinga, roughly 40 mins after the behavior started. The alert was the LVS one, about the entirety of the service. No alerts prior to that, despite the event having started 30+ minutes earlier.","The current per-IP throttling implemented in Thumbor is inadequate. It attempts to create a system where a given IP can only use X workers and have a queue of Y, but since it's based on PoolCounters requests beyond the X limit are actually served by other Thumbor workers. We end up with a situation where the client can keep a lot of workers busy… waiting on its own throttling lock. This wasn't the intention and as such the PoolCounter-based throttle is broken.","Add a Cache-Control header to 404 responses coming from Thumbor/Swift Proxy if there isn't one yet https://phabricator.wikimedia.org/T252425
** Thumbor 2.8 upgrade solved this issue.* Consider a very short term cache (5-10 min?) of 404’s for thumbnails, bearing in mind the possibility for cache pollution attacks https://phabricator.wikimedia.org/T259033* Define who is in charge of basic Thumbor maintenance.* Consider adding alerting for Thumbor query success rate, or for p50/p75 latency.  https://phabricator.wikimedia.org/T290034* Lower poolcounter per-IP limits in Thumbor as much as possible while not breaking the Commons new uploads page too much. https://phabricator.wikimedia.org/T252426
** Changed: worker:4, maxqueue:500, timeout:8 to worker:4, maxqueue:50, timeout: 1",2020-05-11,Performance and Load Issues,high,Degraded System Performance,
"Sensitive credentials (authentication tokens) were leaked via publicly accessible, memcached servers, on a default port & without authentication.Tokens can be used to escalate privileges and (at least) “bump quotas, create and destroy” arbitrary VMs in Cloud VPS/Toolforge. While the service runs in the “production” realm, no escalation paths to other production services and hosts are known so far.These tokens are for access to the Keystone API, which is firewalled to the Cloud VPS and production networks, so they could not be used directly from the internet easily without changing signed cookies in Django/Horizon web UI somehow (which should require the secret key). Therefore, this exposure would theoretically benefit a malicious cloud user to the extent that they could manipulate the Keystone API from inside cloud according to the permissions of the particular captured token.Access was blocked in eqiad at 02:21 UTC and in the development environment in codfw at 08:43 UTC.Keystone auth tokens were manually rotated (the generally have a lifetime of 24 hours)Investigation of potential privilege escalation within Cloud VPS (via leaked Keystone tokens) ongoing, additional mail to be sent to cloud users (see below).",,,"Better understand and refine port-scanning tools in order to detect future vulnerabilities like this one. The daily network scan is in the process of being fixed to scan all ports (instead of a selection of 2000) and when that has happened we need to do a one off investigation of the findings to establish a base point so that all further runs can be studied as a diff against the base check.
* Defense-in-depth measures for cloudcontrol services. Per OpenStack recommendations, “[f]or production deployments, we recommend enabling a combination of firewalling, authentication, and encryption to secure it.”. The first of the three failed here, but it was all three that were missing and allowed this vulnerability.
* More fine-grained access control for the ferm ranges in the profile::memcached::instance class to allow to restrict access for cloudcontrol* further.
* Extend the daily network port scan to cover 64k ports instead of 2000 well-known, this requires some changes to the packet filters running on on our servers https://phabricator.wikimedia.org/T264888
* Meet with security to discuss threat model for cloud services",2020-10-06,Security and Access Issues,high,Security Vulnerabilities or Breaches,
Restricted. Details at https://phabricator.wikimedia.org/T260329.,,,"https://phabricator.wikimedia.org/T260329
* https://phabricator.wikimedia.org/T260281",2020-08-12,[],[],[],
A wrong configuration change caused sessionstore pods to be unschedulable in our WikiKube cluster. This resulted in failed edits across all projects for a period of 9 minutes.,Automated monitoring detected the issue(ProbeDown) firing: Service sessionstore:8081 has failed probes (http_sessionstore_ip4) #page -,,"Fix for sessionstore deployments:https://gerrit.wikimedia.org/r/c/operations/deployment-charts/+/867572==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	2 vs 2, let's say it's sufficient
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	yes	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	14",2022-12-13,Configuration and Deployment Errors,high,Service Downtime,
"asw-a2-codfw, the switch handling the network traffic of rack A2 on codfw became unresponsive rendering 14 hosts unreachable. Besides losing the 14 hosts hosted on rack A2, two additional load balancers lost access to codfw's row A. Services with hosts in the impacted row (such as Swift and various mw-api servers) remained available for clients due to automatic failover and load balancing to remaining hosts. While mw-api remained available for end-users and external clients, the impacted Restbase load-balancer remained pooled causing Restbase to continue to try (and fail) to reach mw-api hosts. Thus, mobileapps API and cxserver API (which rely on Restbase) returned errors to clients for some time.    Several other services were switched over from Codfw to Eqiad: Maps Tile Service and Wikidata Query Service.   Impact: For about 1 hour the Restbase, mobileapps, and cxserver services were serving errors. Reduced capacity of high-traffic1 load balancers and MediaWiki servers in Codfw.","The incident was detected via automated monitoring reporting several hosts of rack A2 going down at the very same time:
* <icinga-wm> PROBLEM - Host elastic2038 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host kafka-logging2001 is DOWN: PING CRITICAL - Packet loss = 100
* <icinga-wm> PROBLEM - Host ns1-v4 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host authdns2001 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host ms-be2051 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host lvs2007 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host thanos-fe2001 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host elastic2055 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host elastic2037 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host ms-fe2005 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host ms-be2040 is DOWN: PING CRITICAL - Packet loss = 100%","As a result of losing one single switch, services were affected more than expected due to several weaknesses:
* 3 load balancers including the backup one get the row A traffic from one single network switch
* Depool threshold of several services is too restrictive to continue to work as expected losing a complete row","Mitigate unresponsive switch (Done). https://phabricator.wikimedia.org/T286787
* Load balancers should be able to handle a NIC failing to be configured. https://phabricator.wikimedia.org/T286924
*Audit eqiad & codfw LVS network links. https://phabricator.wikimedia.org/T286881
*Avoid using the same switch to get traffic from a row on the primary and secondary load balancers. https://phabricator.wikimedia.org/T286879
*Fix Pybal IPVS diff check https://phabricator.wikimedia.org/T286913
* Fix depool threshold for text & upload edge caching services https://gerrit.wikimedia.org/r/c/operations/puppet/+/705381
*Fix depool threshold for mw api service https://gerrit.wikimedia.org/r/c/operations/puppet/+/708072",2021-07-16,Server and Infrastructure Failures,high,Degraded System Performance,
"Network in the kubernetes cluster was briefly disrupted due to a rolling restart of calico pods. For 2 minutes, all service in kubernetes were unreachable. That manifested in elevated errors rates and latencies in mediawiki among other things as mediawiki relies on many of those service, e.g. sessionstore.The incident was the result of an operator forcing a rolling restart of all pods in all namespaces (in order to pickup a docker configuration change), a process that has been done multiple times in the past and never caused issues. The process works by deleting the old pods one by one. It is without much risk mainly because:* Each delete depools first the pod, draining it of traffic* The pod is asked to stop first and then is killed and deleted* In the meantime a new pod has been spun up, probably in a different node, has run the initialization process and is ready to receive traffic* The initialization process of almost all our services is very fast.However this time around, the process fell on a race condition when it entered the kube-system namespace, which contains the components that are responsible for the networking of kubernetes pods. All calico node pods were successfully restarted, however before they had enough time to initialize and perform their BGP peering with the Core routers (cr*-eqiad.wikimedia.org), a component they rely on called calico typha was also restarted. Unfortunately we run only 1 replica of the typha workload. The new replica was scheduled in a new node that did not have the image yet on the OCI imagefs. Enough time elapsed while the image was being fetched and the new pod started by the kubelet that the graceful BGP timers on the core routers expired, forcing the core routers to withdraw from their routing tables the IPv4/IPv6 prefixes for the pod networks, leading to them becoming unreachable. The typha pod started up eventually, calico node pods managed to connect to it, fetched their configuration and BGP peered with the core routers re-announcing their pod IPv4/IPv6 prefixes. Routing was restored and the outage ended. No operator action was necessary to fix the issue, the platform auto-healed.On the plus side, due to this (crude admittedly) form of chaos engineering, we found out a change in our kubernetes clusters and a need to amend our process.Impact: For 2 minutes fetches to mediawiki resulted in higher error rates, up to 8.51% for POSTs and elevated latencies for GETs (p99 at 3.82s). Some 1500 edits failed (although there is duplication there, those are not unique edits). Those seem to have happened later, when the services were restored. Kafka logging lag increased temporarily.Documentation:* Save failures graphs* Sessionstore graphs* Appservers graphs",,,"TODO: Document the rolling pod restart procedure and skip kube-system namespace
*Investigate whether running >1 replicas of calico-typha is feasible and prudent. T292077",2021-09-29,Server and Infrastructure Failures,high,Service Downtime,
,alerting via icinga/alertmanager/victorops - SMS was sent to SREs on on-call duty shift. manual escalation via Klaxon to more SREs.,Maybe onLinksUpdateComplete should not always lok for commons file usage changes or have some throttle.,"Ticket for MediaWiki? - possibly https://phabricator.wikimedia.org/T314020?==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	2 on-callers but they escalated to SRE via klaxon
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	but the incident just lasted ~10 minutes, so that was ok
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	yes	but only retroactively. we did not know if it was an attack at first
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	kind of, https://phabricator.wikimedia.org/T326590 was created and was caused by it but is about one specific effect of it and user created
Process                                       	Are the documented action items assigned?                                                                         	n/a	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	There might be a ticket about onLinksUpdateComplete but this needs input from mw developers.
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	it was using dbctl, not sure if ""db server is overloaded -> depool"" counts as runbook
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-01-09,[],[],[],
"The virtual chassis link between asw2-d1-eqiad and asw2-d8-eqiad failed in two steps.First on Friday the 1st where it was causing packet loss for hosts on D1 without any other signs of failures.This packet loss caused connectivity issues between MediaWiki appservers (and API servers at a lower scale) and memcache servers. Resulting in a significant increase of MediaWiki exceptions being served to the users.This got worked around for the weekend by depooling D1 servers. At this point the cause of the packet loss was unknown.The day after, on Saturday, hosts in D8 started seeing the same issues as in D1. This time the switches were logging errors about the D1-D8 link. Disabling the link solved the issues.Impact: This had little to no effect on traffic (), error rates () and latencies () for anonymous users. A increase in error rates (~1% of requests had errors, with a short spike to ~7.5%, ) and an increase in tail latency (around plus 100%-150%, ) has been observed for logged in users, though.","icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
* <+icinga-wm> PROBLEM - PHP7 rendering on mwXXXX is CRITICAL: CRITICAL - Socket timeout after 10 seconds* Did the appropriate alert(s) fire? Yes
* Was the alert volume manageable? Yes
* Did they point to the problem with as much accuracy as possible? No
The root cause didn't generate any logs at first, and when it did, those logs didn't trigger alerts.","Packet loss through Virtual Chassis Fabric are difficult to pinpoint
* Higher layers monitoring worked as expected
* From history, this failure scenario has a low probability of happening, and is now documented","Either re-cable or cleanup disabled cable - https://phabricator.wikimedia.org/T251663
* Add log alerting for VC link failure - https://phabricator.wikimedia.org/T251663",2020-05-01,Server and Infrastructure Failures,mid,Degraded System Performance,
"See also <https://lists.wikimedia.org/pipermail/wikitech-ambassadors/2020-June/002316.html>Documentation:* User reports: T257522, T256395, T255156* text-backend.inc.vcl.erb#72",,,https://phabricator.wikimedia.org/T260098 Understand beresp.ttl=0 issues in Varnish,2020-06-25,[],[],[],
"A stored XSS security issue was reported, affecting talk pages in MobileFrontend. It was patched in production the same day.The issue and patch were publicly disclosed seven days later on 14 September, at T262213.",,,None),2020-09-07,Security and Access Issues,mid,Security Vulnerabilities or Breaches,
"Incident ID   	2021-11-23 Core Network Routing                                                                                                                                                                                                                                                          	UTC Start Timestamp:                                                                                                                                                                                                                                                                     	YYYY-MM-DD hh:mm:ssIncident Task 	T299969                                                                                                                                                                                                                                                                                  	UTC End Timestamp                                                                                                                                                                                                                                                                        	YYYY-MM-DD hh:mm:ssPeople Paged  	<amount of people>                                                                                                                                                                                                                                                                       	Responder Count                                                                                                                                                                                                                                                                          	<amount of people>Coordinator(s)	Names - Emails                                                                                                                                                                                                                                                                           	Relevant Metrics / SLO(s) affected                                                                                                                                                                                                                                                       	Relevant metrics% error budgetSummary:      	For about 12 minutes, Eqiad was unable to reach hosts in other data centers (e.g. cache PoPs) via public IP addresses due to a BGP routing error. There was no impact on end-user traffic, and internal traffic generally uses local IP subnets that are routed with OSPF instead of BGP.	For about 12 minutes, Eqiad was unable to reach hosts in other data centers (e.g. cache PoPs) via public IP addresses due to a BGP routing error. There was no impact on end-user traffic, and internal traffic generally uses local IP subnets that are routed with OSPF instead of BGP.	For about 12 minutes, Eqiad was unable to reach hosts in other data centers (e.g. cache PoPs) via public IP addresses due to a BGP routing error. There was no impact on end-user traffic, and internal traffic generally uses local IP subnets that are routed with OSPF instead of BGP.At approx 09:37 on Tues 23-11-2021 a change was made on cr1-eqiad and cr2-eqiad, to influence route selection in BGP.  The specific change was to remove a BGP setting which causes the BGP ""MED"" attribute to be set to the OSPF cost to reach the next-hop of the BGP route, as part of T295672. This caused a change in how routes to certain remote sites were evaluated by the core routers there. At a high-level the change meant that the correct, externally-learnt routes for remote BGP destinations, suddenly looked less preferable than reflections of these routes devices in eqiad were announcing to each other.For instance, cr2-eqiad was previously sending traffic for Eqsin BGP routes via Codfw. But following the change BGP decided it was better to send this traffic to its local peer cr1-eqiad. Unfortunately, cr1-eqiad was a mirror image of this, and decided the best thing was to send such traffic to cr2-eqiad. A ""routing loop"" thus came into being, with the traffic never flowing out externally, but instead bouncing between the two Eqiad routers. Alerts fired at 09:39, the first being due to Icinga in Eqiad failing to reach the public text-lb address in Eqsin (Singapore).  At 09:42 the configuration change was reverted on both devices by Cathal.  Unfortunately that did not immediately resolve the issue, due to the particular way these routes update on a policy change. After some further troubleshooting a forced reset was done to the BGP session between the eqiad routers at 09:51, which resolved the issue.The impact was noticed by Icinga via its health checks against edge caching clusters like Eqsin. However, the Icinga alert was likely also the only impact as we generally don't explicitly target remote data centers over public IP. The exception to that is WMCS instances, but that runs within Eqiad and DNS generally resolves public endpoints to the same DC, not a remote one.Impact: For about 12 minutes, Eqiad was unable to reach hosts in other data centers (e.g. cache PoPs) via public IP addresses due to a BGP routing error. There was no impact on end-user traffic, and internal traffic generally uses local IP subnets that are routed with OSPF instead of BGP.",,,"Ticket T295672 has been updated with further detail on what happened and a proposal to adjust our BGP sessions to address both the ""second IXP port"" issue and prevent this happening again in future. The changes were backed out so situation is back to what it was before the incident.== Scorecard ==           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	Info not logged
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	paged via batphone
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	paged via batphone
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	paged via batphone
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	0    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	1    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	
Total score	Total score                                                                                                                                              	6",2021-11-23,Configuration and Deployment Errors,mid,Degraded System Performance,
"Conftool data, stored in etcd, is normally replicated from one data center to the other (currently eqiad to codfw) using an in-house tool called etcd-mirror. Etcd-mirror is intentionally fairly brittle to replication problems; when it encounters an unexpected situation, it stops replicating and pages us, rather than attempt to self-recover and risk corrupting the etcd state.Due to an unusual sequence of data changes (detailed below) etcd-mirror's long-running watch lost track of the up-to-date state of etcd in eqiad, and replication stopped. This had no immediate user impact, but subsequent conftool changes (like pooling/depooling hosts in load-balanced clusters, pooling/depooling data centers in service discovery, or requestctl changes) would not have been propagated between data centers, leaving us without emergency tools and potentially causing a dangerous skew in production config.","We were paged when etcd-mirror failed. (A prior incident was exacerbated after the paging was accidentally dropped in the migration to alertmanager; that was fixed in T317402 as a followup. It was only a moderate factor in that incident, because SREs independently noticed errors in the etcd-dependent tools they were using. By contrast, if the alert hadn't paged us here, it's likely nobody would have noticed the problem for much longer.)In addition to that EtcdReplicationDown alert, we also got a non-paging SystemdUnitFailed alert, which is redundant but not harmfully so (and might be a useful extra pointer for responders not familiar with etcd-mirror).We also got five separate non-paging JobUnavailable alerts for etcd-mirror, at 1:48, 2:38, 2:38, 2:43, and 2:53, the last one being after etcd-mirror had resumed. These didn't contribute any extra information.",,"Long-term solution: Switch conftool from etcd v2 to v3. As part of this effort, etcd-mirror will be replaced with an alternative solution for cross-site replication, a requirement for which is resilience to this class of problem. (T350565)
* In the meantime, consider building the recovery described at https://etcd.io/docs/v2.3/api/#watch-from-cleared-event-index into etcd-mirror, so that it can detect and handle this situation automatically. It may or may not be worth the engineering effort, depending on the timeline for moving to v3, but now that there's much more traffic to etcd keys outside of /conftool (notably because of Spicerack locking) this situation is more likely than it used to be, so we may want to be resilient to it even in the short term. Alternatively, we could replicate the entire keyspace and not just /conftool, if etcd-mirror can sustain the load. (T358636)
* Update Etcd/Main cluster#Replication documentation with safe restart conditions and information. Populate the ""TODO"" playbook link for EtcdReplicationDown in the process. (T317537)==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	There were only two responders and no user impact, so we didn't bother with a doc.
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	No user impact, so a status page entry wasn't appropriate.
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	T317537
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2024-02-28,Data Management and Retention,mid,Degraded System Performance,
"A MediaWiki configuration change making EventBus use TLS for eventgate-analytics was merged. POST requests from MediaWiki application servers to eventgate-analytics started timing out. This resulted in HTTP server errors being served to users in all data centers. The very high rate of application server requests timing out caused troubles elsewhere in the stack, with 50% of the Varnish frontend caches in esams crashing due to memory allocation failures. Exacerbating the problems, although the Varnish frontend processes were automatically restarted, ATS-tls kept seeing them as down.","The SRE team was first notified about the issue by various Icinga PHP7 rendering alerts on IRC, shortly followed by multiple pages regarding api.svc.eqiad.wmnet socket timeouts, as well as ATS TLS and Varnish reduced availability.",,"Make scap skip restarting php-fpm when using --force https://phabricator.wikimedia.org/T243009
*  Scap should be able to wait longer on the canaries https://phabricator.wikimedia.org/T217924
*  varnish-fe should not crash trying to allocate memory https://phabricator.wikimedia.org/T242417
*  varnish parent should be able to send signals to its child https://phabricator.wikimedia.org/T242411
*  Raise severity of varnish child restart to critical: https://gerrit.wikimedia.org/r/#/c/operations/puppet/+/563174/
*  ats-tls should immediately mark varnish as up after the latter restarts https://phabricator.wikimedia.org/T242620
*  Per-cgroup CPU graphs would allow to quickly find out misbehaving processes: https://phabricator.wikimedia.org/T183146",2020-01-08,Configuration and Deployment Errors,high,Service Downtime,
"All users of dawiktionary, dawiki, and svwiktionary sometimes received fatal exceptions while browsing.  Logs indicated MediaWiki was encountering a data  issue which also entered the object cache.  The source of the problem was traced back to a data migration implementation bug in how bash interprets double-quotes ("") characters.",The issue was noticed by a volunteer and reported after business hours via IRC.  Another volunteer saw the IRC message and escalated to SRE via Klaxon.,,"TODO==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	Not Used
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	Klaxon
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-06-06,Code and Application Bugs,mid,Degraded System Performance,
"Something happened between 2024-04-27 00:00 and 2024-04-27 00:08 UTC, and the rsync clients of most puppetmasters/puppetservers to sync data from puppetmaster1001 hung indefinitely.  On 2024-06-10, they were killed and restarted manually.This means that new data from puppet 'volatile' was only rarely/intermittently synced to much of the fleet during this window.The problem was particularly bad in codfw, where all 3 puppetservers had failed to rsync data for the entire duration.Getting a firm idea of the impact of this is difficult.  The new GeoLite2 files were unavailable, but also not in use in production yet(?).  The older 'enterprise' file was still being used for many uses, however, and would have grown stale.  Analytics (a heavy user of GeoIP data) was probably not very affected because it is eqiad-only.  Any CheckUser calls would have likely been affected by stale data.  On the other hand, aside from the one service wishing to use newly-added files ... no one noticed?  So this puts a sort of upper bound on the potential impact, however unsatisfying.","Manual. Kosta Harlan asked on #wikimedia-sre-foundations IRC to confirm that the new GeoLite2 files were available, as part of work on https://phabricator.wikimedia.org/T366272.  cdanis began investigating and discovered that the files were missing on most hosts in codfw where they were expected to exist: https://phabricator.wikimedia.org/P64540",,"Add a generous default TimeoutStartSec in systemd::timer::job.  It cannot be infinite.
* Enable monitoring and logging for the systemd::timer::jobs defined in modules/puppetmaster/manifests/rsync.pp
* Ensure that timer failures for sync-puppet-volatile get reported somewhere (#-sre-foundations IRC?)
*Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFire Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2024-06-10,Server and Infrastructure Failures,mid,Degraded System Performance,
"Sometime after 22:00 UTC, a traffic surge started to fill up the capacity of a network port in the Ulsfo cluster. By 23:00 UTC the issue was mitigated.Impact: For up to an hour, a portion of requests may have received a slower response from regions served by the San Francisco DC. This includes New Zealand, and parts of North America.Documentation:*Wikimedia DNS: DC geo map",,,None.,2021-06-30,Performance and Load Issues,low,Degraded System Performance,
"A subset of redundant Cassandra hosts underwent a scheduled administrative shutdown to conduct power maintenance in Codfw. During the planned outage, other Cassandra hosts were running out of disk space due to accomulating log files that Cassandra uses to repair clusters after downtime. The issue was resolved by bringing the remaining Cassandra nodes back online, ahead of schedule.",,,"T315517 Create (and document) a process for disabling hinted-handoff during maintenance events
*T315517 Create (and document) a process for truncating hinted-handoff 
*T315517 Ensure that future clusters have a dedicated storage volume for hinted-handoff
*T315517 Establish (and document) best practice for sizing of hinted-handoff volumes==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	no pages
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	no pages
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	no pages
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	10",2022-08-10,Server and Infrastructure Failures,mid,Degraded System Performance,
,"The issue was first detected by users of Chinese Wikipedia.  There was no automated monitoring.On investigation, there do not appear to be any parser tests or other test cases which exercise language conversion on the table of contents (phab:T295187) or which verify the correct operation of the WikidataPageBanner extension (phab:T295003).Rollback removed the table of contents from many articles on all wikis; this was also not detected by any monitoring.Rollback did cause spurious (unrelated to the table of contents issue) alerts, as discussed above: phab:T295079.","This incident exposed weaknesses in test coverage of the Table of Contents, and in the way that Parser Cache content interacts with our deployment and versioning systems.  Content stored in RESTBase has the potential for similar issues, as discussed below, but has slightly better purging and versioning systems to allow prevention and/or mitigation of version mismatch issues such as these.In addition, procedural weaknesses were exposed in flagging potentially ""risky"" patches, and in the forum used for rollback discussions.  A related issue is that, due to time zone skew, detecting and reacting to failures in Chinese Wikipedia (deployed late UTC time on Thursday) can easily push timelines past 5pm local time on a Friday for engineers involved in the response.  The community involved in the smaller group 1 projects, like Chinese Wikivoyage, would in theory have noticed both ToC problems a full day earlier, but those community members did not successfully relay the issue to WMF staff.  It may be advisable to move Chinese Wikipedia from group 2 to group 1 in order to accelerate detection and response to issues.","Technical changes:* Add test cases which exercise language conversion on the table of contents T299973
* Add test cases for WikidataPageBanner extension, esp for the table-of-contents replacement code (selenium tests?) T299974
* Create and document on wiki a test and deploy plan for changes to Parser Cache contents: (TODO: Phab ticket  #Sustainability (Incident Followup) )
** Patch which adds ""future compatibility"" (to handle ""future"" parser cache contents) should land first and roll out with the train, *before* any changes to parser cache contents are made.  This should have test cases verifying correct behavior during roll back (that is, correct behavior if ""parser cache contents from the future"" are encountered).  (These test cases were absent in this incident, since this split was not done.)
** A subsequent patch which *changes* parser cache contents to the ""future"" form can roll out in a separate train; this will make it more likely that this train can be safely rolled back if issues are found.  This should have test cases verifying correct behavior during roll forward (that is, correct behavior if ""parser cache contents from the past"" are encountered).  (These test cases were present in this incident, which is a ""what went well"".)
** Documentation for ParserOutput::getText() in particular should reference this plan to provide guidance around how best to make backwards compatible changes; it should note that such changes should be always be flagged as potentially risky to the SREs and have communicated a rollback plan.
** Make this plan relevant to DiscussionTools and other users of parser cache contents as well.
* Create and document on wiki a test and deploy plan for changes to RESTBase contents, including documenting user:ppchelko's ""five lines of code"" to allow for purge during rollback.  (TODO: Phab ticket  #Sustainability (Incident Followup) )
* Allow earlier visibility for issues involving language variants by some means, such as by moving zhwiki or srwiki from group2 to group1.  (TODO: Phab ticket  #Sustainability (Incident Followup))
* Enable pig latin variant on English Wikipedia beta cluster (T299975). This has two benefits:
** It allows easier testing of variant-related issues in beta, for example to validate a patch on master before cherry-picking it to production.
** It possibly also facilitates earlier visibility of issues involving language variants, although effectiveness requires someone to activity monitor the status of this wiki.Process changes  (TODO: Phab ticket (??) Add   #Sustainability (Incident Followup) )* Revise communication protocols based on the current expectations of communication medium of WMF employees. For example, if product engineers are required to be available on IRC, that should be communicated broadly. If that's not a requirement, we should perhaps use email/Phabricator as primary communication",2021-11-05,,,,
"We are not aware of all consumers of this stream.  No one noticed this change for over 3 months.  Root CauseThe developer (Andrew Otto) and reviewers did not catch the accidental change to extension.json that unregistered the EventBusHooks:onPageUndelete hook handler.Affected Datasets and ServicesThe main fallout is that WDQS and WCQS will have inconsistencies in their downstream datastores: any wiki pages that were undeleted during this time period will not be available in WDQS. There may be exceptions to this, the WDQS updater is supposed to detect inconsistencies (i.e. getting an edit on a deleted page) and apply some reconciliation but apparently this system did not work as expected here. Resolving the inconsistencies for WDQS will be achieved via full data-reload (something that was already in progress).There may be other affected services as well.  The event.mediawiki_page_undelete table in Hive will be empty for this time.  We also expose this stream publicly via stream.wikimedia.org, so if there are external consumers (Internet Archive?) they will also have missed these page undelete events.",,,"Add automated stream throughput alerting: T329070
* Review the “reconciliation” mechanism of the WDQS updater to understand why it was not able to recover these missing events after further edits on the affected entities and fix it: T329089 ==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	   	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	no 	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-02-07,Code and Application Bugs,mid,Data Loss or Corruption,
"During a reimage operation, the cloudelastic Elasticsearch cluster lost a shard and went into red status, indicating data loss.Until the data was restored, search results were incomplete on Cloudelastic. Restoration from production snapshots, using the previously understood and documented process, failed consistently, requiring a different approach to be devised which is why restoration was delayed by a month. Restoration was completed on 12 July. Documentation:*https://phabricator.wikimedia.org/T309648#8072778",,,"Restore data to cloudelastic
*Document cloudelastic cluster (what is its purpose, who are the stakeholders, etc)
*Document restore process
*Review monitoring for cloudelastic
*Inform stakeholders of the current situationTODO: Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard) Phabricator tag to these tasks.==Scorecard==Incident Engagement ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the incident status section actively updated during the incident?                                             		
Process                                       	Was the public status page updated?                                                                               		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were all engineering tools required available and in service?                                                     		
Tooling                                       	Was there a runbook for all known issues present?                                                                 		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-06-01,Data Management and Retention,mid,Data Loss or Corruption,
mark>,Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?Humans reported an error via this Phab task,,"Phab task for follow-up
** Elasticsearch: Alert on Downstream Errors
** Create alert for LVS if it is configured for unreachable back-end server IPs.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	no 	
People                                        	Were fewer than five people paged?                                                                                	no 	No paging happened
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	No paging happened
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	No paging happened
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	Search SRE have never done it
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2024-04-26,,,,
"services (citoid, cxserver) in eqiad had partial failures. In the case of citoid, results were not augmented enough, in the case of cxserver, translations were not returned.",Detection was by both by human as well as Icinga.,What weaknesses did we learn about and how can we address them?* Humans are the weakest link. The process went perfect in codfw and hence not everything was checked before applying it in eqiad.,"Fix the calico configuration https://gerrit.wikimedia.org/r/537121. 
* Monitor coreDNS. https://phabricator.wikimedia.org/T234545
* Monitor zotero indirectly via citoid https://phabricator.wikimedia.org/T234544",2019-09-16,API and Integration Failures,mid,API or Integration Failures,
On-wiki search for all wikis was delayed or unavailable for a subset of end users.,"Was automated monitoring first to detect it? Or a human reporting an error?Monitoring caught it, but not quickly enough for humans to prevent user-facing impact.Copy the relevant alerts that fired in this section.** PROBLEM alert - graphite1005/CirrusSearch codfw 95th percentile latency - more_like is CRITICAL (Grafana alert sent to IRC and Search Platform email list)Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?Yes to all questions.",,"Determine what Elasticsearch-related steps are needed prior to a datacenter switchover.
* Update documentation as mentioned in ""Links to relevant documentation"" section above.
* Review https://phabricator.wikimedia.org/T285347 (suggestions for switchover improvements) and decide whether to implement.
* phab:T347034 Update the RESTBase /related/{article} endpoint to make a GET request and not a POST in order to help having a CirrusSearch query cache warm in both DC when running multi-DC
* Review current shard balance for busy wikis, particularly enwiki_content. In my haste to mitigate, I (inflatador) did not check the shards of the highly-loaded host. Dcausse theorized that two enwiki shards may have been on the same host, which is extremely expensive, particularly when more_like queries are involved. That would suggest that the problem was worsened by the DC switchover, but not directly caused by it.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	N/A	
People                                        	Were the people who responded prepared enough to respond effectively                                              	Y  	
People                                        	Were fewer than five people paged?                                                                                	Y  	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	Y  	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	Y  	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	N  	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	N  	
Process                                       	Is there a phabricator task for the incident?                                                                     	Y  	
Process                                       	Are the documented action items assigned?                                                                         	N  	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	Y  	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	Y  	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	Y  	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	Y  	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	Y  	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	N  	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	10",2023-09-20,Performance and Load Issues,mid,Degraded System Performance,
"An upgrade to Kartotherian caused spurious Icinga alerts so was rolled back.  Rollback fails and the service goes down, due to non-robustness in the deployment configuration templates.","Icinga alerts for ""kartotherian endpoints health"" in #wikimedia-operations began to fire, for example:
 13:27 <+icinga-wm> PROBLEM - kartotherian endpoints health on maps2006 is CRITICAL: /{src}/{z}/{x}/{y}.{format} (Untitled test) is CRITICAL: Test Untitled test returned the 
                    unexpected status 301 (expecting: 200): /{src}/{z}/{x}/{y}@{scale}x.{format} (Untitled test) is CRITICAL: Test Untitled test returned the unexpected 
                    status 404 (expecting: 200): /{src}/info.json (Untitled test) is CRITICAL: Test Untitled test returned the unexpected status 404 (expectingUser claime correctly guessed that the alerts were related to awight's deployment and pinged them in IRC.We learned later that these alerts were spurious, caused by an automatically-generated monitoring job ().When rollback failed, we monitored full service failure through its dedicated dashboard, https://grafana.wikimedia.org/d/000000305/maps-performances.",A more robust or fully automated and containerized deployment for the maps service would have prevented an outage.,"Improve documentation for maps production deployment.
*  Fix Icinga monitoring for maps, 
* Make kartotherian configuration robust enough to deploy without the ""--env"" flag, 
* Improve documentation about how the OpenAPI specification is automatically wired into CI and monitoring, .
* Containerize the kartotherian service, ==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-01-30,Configuration and Deployment Errors,high,Service Downtime,
"Note: the corresponding maps outage is not treated here. Its timings will just be reported in the timeline for reference.At 19:47 UTC, the news of Kobe Bryant's death was announced. This caused a surge in both edits and page views, causing a stampede of requests and a general slowdown of both the application servers and the apis. This caused both high contention in editing/parsing as expected, and can be seen by the number of poolcounter full queues, as well as high cpu usage, very high response times, and general unavailability of the MediaWiki application layer (both appservers and api). This was caused by a series of concauses:* High contention in editing/parsing, resulting in a lot of workers blocked on waiting for poolcounter.* Saturation of the network link of two memcached servers due to the high edit/reparse activity. This caused about 1/9th of the memcached keys to become unavailable* A spike in latencies resulting from the failure of the memcached servers caused a rise in the cache-misses for babel calls from the appservers, flooding the api servers. pretty much as the same mechanism of the incident of February 4 (although the ignition of the incident was different).Given the sustained higher level of requests, it took more than one hour for the clusters to recover.","The monitoring correctly and promptly detected the issue, and 3 minutes later we received the first page. Given most SRE were already around for a previous incident, most of us didn't need the page to be online.2020-01-26 19:52:33     +icinga-wm      PROBLEM - MediaWiki memcached error rate on icinga1001
2020-01-26 19:53:57     +icinga-wm      PROBLEM - High average GET latency for mw requests
2020-01-26 19:55:11     +icinga-wm      PROBLEM - LVS HTTPS IPv4 #page on text-lb.codfw.wikimedia.org",,"Reduce Wikibase calls to Babel when not needed https://phabricator.wikimedia.org/T243725 
* Reconsider https://gerrit.wikimedia.org/r/#/c/operations/puppet/+/511751/ (apache forensic logging) 
* Proxy calls from MediaWiki to all https calls through a proxy https://phabricator.wikimedia.org/T244843
* Reduce read pressure on memcached servers by adding a machine-local Memcache instance https://phabricator.wikimedia.org/T244340
* Investigate why a slowdown in response times from the API causes a surge in cache misses for Babel data in Wikibase. See https://phabricator.wikimedia.org/T244877",2020-01-26,Performance and Load Issues,high,Service Downtime,
"Summary of what happened, in one or two paragraphs. Avoid assuming deep knowledge of the systems here, and try to differentiate between proximate causes and root causes.","Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?Copy the relevant alerts that fired in this section.Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?TODO: If human only, an actionable should probably be to ""add alerting"".",OPTIONAL: General conclusions (bullet points or narrative),"Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard)  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        		
Process                                       	Was a public wikimediastatus.net entry created?                                                                   		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    		
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-10-04,,,,
"The TLS certificate for etcd.eqiad.wmnet expired. Nginx servers on conf* hosts use this certificate, and thus conftool-data could not sync between conf hosts anymore. During this time, puppet-merge returned sync errors. labweb (wikitech) hosts alerted because of failed timers/jobs.We got paged by monitoring of ""Etcd replication lag"". We had to renew the certificate but it wasn't a simple renew, because additionally some certificates had already converted to a new way or creating and managing them while others had not. Our two core data centers were in different states. Only Eqiad was affected by lag and sync errors. After figuring this out, we eventually created a new certificate for etcd.eqiad using cergen, copied the private key and certs in place and reconfigured servers in Eqiad to use it. After this, all alerts recovered.Documentation:*https://grafana.wikimedia.org/d/Ku6V7QYGz/etcd3?orgId=1&from=1651361014023&to=1651428319517*https://logstash.wikimedia.org/goto/1e1994e64e8c23ef570fb19f562bf08b",,,"Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.*T307382 (Modernize etcd tlsproxy certificate management)
*T307383 (Certificate expiration monitoring)
* https://gerrit.wikimedia.org/r/q/topic:etcd-certs (5 Gerrit changes)TODO: Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard) Phabricator tag to these tasks.==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	1    	probably? do we actually go through the last 5 incidents? Which list to use?
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	combined knowledge of both responders did it
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	1    	15 paged, 2 responded
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	Are any pages routed to subteams yet?
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	weekend and late
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	no public impact that would have made it useful
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	no public impact that would have made it useful
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	1    	https://phabricator.wikimedia.org/T302153 was reused, as well as follow-up task https://phabricator.wikimedia.org/T307382
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	unsure though, maybe but before we made reports for them
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	could have had one to migrate eqiad certs to cergen
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	IRC
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	Yes, but only when cert was already expired. Should have had alerting before that.
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	
Total score	Total score                                                                                                                                              	8",2022-05-01,Security and Access Issues,mid,Degraded System Performance,
"Two main issues happened during this window:# The prep work to bring online the replacement switch from the prior incident triggered a Junos bug which brought instability in row B (mostly impacting connectivity between the different switches in row B)# Another Junos bug (triggered by an operator error) broke IPv6 connectivity for the whole rowThe first point was quickly fixed, while the 2nd required onsite work to minimize downtime as it required B2 to come back up to be able to reboot B7 (those two are the uplinks to the core routers).","LibreNMS alerts fired. Example verbiage: Alert for device asw-b-codfw.mgmt.codfw.wmnet - virtual-chassis crash
* The issues were triggered by Netops work, so engineers were already looking and identified the bugs quicklyDid the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?Yes and yes",,"Create a cookbook to properly depool (or switchover) all services from a datacenter T327665
** Cookbook for rack depool - https://phabricator.wikimedia.org/T327300
* Visually represent (grafana) where a service is being served from any given time (eqiad, codfw, or both), so we can return to the same state before the incident T327663
* Data check es2020 
* Upgrade all of eqiad and codfw rows - https://phabricator.wikimedia.org/T327248
* (longer term) Plan codfw row A/B top-of-rack switch refresh - https://phabricator.wikimedia.org/T327938==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	yes	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	T327001
Process                                       	Are the documented action items assigned?                                                                         	no 	No action items yet, will consult with more experienced SREs before creating
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	Originally a question mark, so defaulting to no
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	10",2023-01-17,Server and Infrastructure Failures,mid,Degraded System Performance,
"For 8 minutes, the Codfw cluster experienced partial loss of IPv6 connectivity for upload.wikimedia.org. Thanks to Happy Eyeballs there was no visible user impact (or, at worse, a slight latency increase). The Codfw cluster generally serves Mexico and parts of the US and Canada. The upload.wikimedia.org service serves photos and other media/document files, such as displayed in Wikipedia articles.	For 8 minutes, the Codfw cluster experienced partial loss of IPv6 connectivity for upload.wikimedia.org. Thanks to Happy Eyeballs there was no visible user impact (or, at worse, a slight latency increase). The Codfw cluster generally serves Mexico and parts of the US and Canada. The upload.wikimedia.org service serves photos and other media/document files, such as displayed in Wikipedia articles.	For 8 minutes, the Codfw cluster experienced partial loss of IPv6 connectivity for upload.wikimedia.org. Thanks to Happy Eyeballs there was no visible user impact (or, at worse, a slight latency increase). The Codfw cluster generally serves Mexico and parts of the US and Canada. The upload.wikimedia.org service serves photos and other media/document files, such as displayed in Wikipedia articles.After preemptively replacing one of codfw row B spine switch (asw-b7-codfw) for signs of disk failure, the new switch was silently discarding IPv6 traffic (through and within the switch).As this switch was a spine, ~50% traffic toward that row (from cr2) was transiting through it.Row B being at this time the row hosting the load-balancer in front of upload-lb.codfw, this was the most visible impact.Monitoring triggered and the interface between asw-b7-codfw and cr2-codfw was disabled, forcing traffic through the cr1<->asw-b2-codfw link. Resolving the upload-lb issue.Replacing the switch didn't solve the underlying IPv6 issue, showing that it was not a hardware issue. Forcing a virtual-chassis master failover solved what we think was a Junos (switch operating system) bug.Note that at the time of the issue, our Juniper support contract was expired, preventing us from opening a JTAC case. Impact: For 8 minutes, the Codfw cluster experienced partial loss of IPv6 connectivity for upload.wikimedia.org. Thanks to Happy Eyeballs there was no visible user impact (or, at worse, a slight latency increase). The Codfw cluster generally serves Mexico and parts of the US and Canada. The upload.wikimedia.org service serves photos and other media/document files, such as displayed in Wikipedia articles.Documentation:*Original maintenance/incident task, T295118=Scorecard =           	Question                                                                                                                                                 	Score	NotesPeople     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	0    	Info not loggedPeople     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	page routed via batphonePeople     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	page routed via batphonePeople     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	page routed via batphoneProcess    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	0    	Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	0    	Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	0    	Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	1    	Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	0    	Total score	Total score                                                                                                                                              	5",,,"Icinga check for ipv6 host reachability, T163996",2021-11-18,Server and Infrastructure Failures,low,Degraded System Performance,
"For approximately 24 minutes, uncached calls to the API on the eqiad datacenter overloaded the application servers, running out of threads (all busy) creating unreasonable latency or failing to respond to requests. This caused sending errors to some clients using the action API and Parsoid in the primary datacenter. Elevated latencies persisted for the following 3 hours, when traffic load organically went down.This was caused by the DiscussionTools MediaWiki extension adding a ResourceLoader module on almost all page views -even non-discussion pages- which created an API call, that, combined with a significant 50% increase in overall traffic, led to an overload and increased latencies on the API cluster. codfw app server cluster was mostly unaffected due to not receiving POST uncached traffic at the time (it is read-only).","Monitoring and paging worked as intended, paging everybody (it was a weekend) as soon as the issue become major at 17:56:* 17:59 Service: [FIRING:1] ProbeDown (10.2.2.22 ip4 api-https:443 probes/service http_api-https_ip4 ops page eqiad prometheus sre)
* 18:00 Service: [FIRING:1] PHPFPMTooBusy api_appserver (ops php7.4-fpm.service page eqiad prometheus sre)
* 18:00 Service: [FIRING:1] FrontendUnavailable (varnish-text page thanos sre)
* 18:01 Service: [FIRING:1] FrontendUnavailable cache_text (page thanos sre)A task was created also by a community member at 19:03, when the team was already analyzing the issue.However, there were reports that ""php fpm busy has been flapping all weekend, just not enough to page"".",OPTIONAL: General conclusions (bullet points or narrative),"Fix sre.mediawiki.restart-appservers cookbook and doc 
* Patch  
* Patch  
* : Avoid uncached action=discussiontoolspageinfo API calls on page load 
*  ==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	no 	SRE had no insight into existence of the DiscussionTools ongoing A/B Test
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	no 	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	yes	
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	yes	https://www.wikimediastatus.net/incidents/2w4ygdj4vc20
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	T325477
Process                                       	Are the documented action items assigned?                                                                         	yes	see above also completed so this incident doesn't repeat
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	Outage was caused by a known problem documented in a TODO comment T325477#8476954
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	runbook failed
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	8",2022-12-18,Performance and Load Issues,high,Service Downtime,
"From approximately 7:31 to 8:16 UTC, there has been instability serving Wiki uncached content (mostly impacted would be authenticated users and bots, as well as POST requests), affecting at first commonswiki, but later most other wikis, albeit at a lower rate.The impact between those timestamp was increased latencies (up to 5+ seconds to return results) and high level of errors (approximately 1/4th of request lost). This was due to overload on commonswiki requests blocking resources on most api & database servers, which in turn created contention on most other wikis.",,,"Uncached wiki requests partially unavailable due to excessive request rates from a bot https://phabricator.wikimedia.org/T280232
* Determine safe concurrent puppet run batches via cumin https://phabricator.wikimedia.org/T280622",2021-04-20,Performance and Load Issues,high,Service Downtime,
Lots of HTTP 429 from Varnish (due to ongoing rate-limiting) caused overload at the HAProxy level and general service disruption.Documentation:*HTTP 503s reported on phab https://phabricator.wikimedia.org/T310368*https://www.wikimediastatus.net/incidents/5k90l09x2p6k,,,"Re) evaluate effectiveness / usefulness of varnish/haproxy traffic drop alerts https://phabricator.wikimedia.org/T310608
*Mitigate/fix overload situations between varnish and haproxy https://phabricator.wikimedia.org/T310609==Scorecard==Incident Engagement ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the incident status section actively updated during the incident?                                             		
Process                                       	Was the public status page updated?                                                                               		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were all engineering tools required available and in service?                                                     		
Tooling                                       	Was there a runbook for all known issues present?                                                                 		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-06-10,Performance and Load Issues,high,Degraded System Performance,
"The api-appserver cluster has been overwhelmed with expensive parsoid-batch calls from particular pages, resulting in brownouts and frontend availability at ~97%.","Alerts for appservers went off on IRC during brownouts through the first half of the day (6-12 UTC) followed by general widespread alerts for affected internal systems (restbase, recommendation, mobileapps alerts went off) starting around 15 UTC","API appservers are susceptible to storms of many long-running parsoid-batch requests, in particular external requests which end up exhausting the cluster's resources and affecting all API users.","Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Limit external requests concurrency/parallelism ()
* Icinga alerts for appserver high CPU should be less spammy ()
* Bigger idea: it would be nice if there was, aggregated across the whole cluster, some report available of what request types, users, user-agents, etc were using the most appserver CPU.
* …",2019-08-02,Performance and Load Issues,mid,Degraded System Performance,
"Summary of what happened, in one or two paragraphs. Avoid assuming deep knowledge of the systems here, and try to differentiate between proximate causes and root causes.","Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?Copy the relevant alerts that fired in this section.Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?TODO: If human only, an actionable should probably be to ""add alerting"".",OPTIONAL: General conclusions (bullet points or narrative),"Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard)  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	no 	
People                                        	Were fewer than five people paged?                                                                                	no 	no pages at the time of impact starting
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	It did eventually, but not initially
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2022-08-24,[],[],[],
"WDQS updaters stopped processing updates in Codfw due to a failure with Flink in Codfw.The API maxlag feature, is configured on Wikidata.org to incorporate WDQS lag. The updateQueryServiceLag service exists to transfer this datapoint from Prometheus to MW. Because bots generally opt-in to be friendly and enable the ""maxlag"" parameter, and because the metric was configured to consider both Eqiad and Codfw, their edits were rejected for two hours.",,,"https://phabricator.wikimedia.org/T238751 (pre-existing ticket) would have prevented the period in which Wikidata edits could not get through despite the affected hosts having already been depooledTODO: Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard) Phabricator tag to these tasks.==Scorecard==Incident Engagement™  ScoreCard           	Question                                                                                                                                                 	Score	Notes
People     	Were the people responding to this incident sufficiently different than the previous five incidents? (score 1 for yes, 0 for no)                         	1    	
People     	Were the people who responded prepared enough to respond effectively (score 1 for yes, 0 for no)                                                         	1    	
People     	Were more than 5 people paged? (score 0 for yes, 1 for no)                                                                                               	0    	unclear if this paged, please update if known
People     	Were pages routed to the correct sub-team(s)? (score 1 for yes, 0 for no)                                                                                	0    	unclear if this paged, please update if known
People     	Were pages routed to online (business hours) engineers? (score 1 for yes,  0 if people were paged after business hours)                                  	0    	unclear if this paged, please update if known
Process    	Was the incident status section actively updated during the incident? (score 1 for yes, 0 for no)                                                        	1    	
Process    	Was the public status page updated? (score 1 for yes, 0 for no)                                                                                          	0    	
Process    	Is there a phabricator task for the incident? (score 1 for yes, 0 for no)                                                                                	0    	
Process    	Are the documented action items assigned?  (score 1 for yes, 0 for no)                                                                                   	1    	
Process    	Is this a repeat of an earlier incident (score 0 for yes, 1 for no)                                                                                      	0    	
Tooling    	Was there, before the incident occurred, open tasks that would prevent this incident / make mitigation easier if implemented? (score 0 for yes, 1 for no)	0    	
Tooling    	Were the people responding able to communicate effectively during the incident with the existing tooling? (score 1 for yes, 0 or no)                     	1    	
Tooling    	Did existing monitoring notify the initial responders? (score 1 for yes, 0 for no)                                                                       	1    	
Tooling    	Were all engineering tools required available and in service? (score 1 for yes, 0 for no)                                                                	1    	
Tooling    	Was there a runbook for all known issues present? (score 1 for yes, 0 for no)                                                                            	1    	
Total score	Total score                                                                                                                                              	8",2022-02-22,API and Integration Failures,mid,API or Integration Failures,
"During routine maintenance on a sessionstore Cassandra node, the Kask service developed a split brain issue where it did not recognise the newly re-added Cassandra node in the cluster despite it being healthy from a Cassandra perspective. This lead to Kask falsely failing to get read and write quorum on the cluster, leading to a disruption of session-related functions on the wikis.This issue was ultimately thought to be a result of an outstanding bug in gocql that leads to an inconsistent view of cluster health. The initial mitigation of this issue consisted of removing the newly imaged host from the cluster. The medium-term fix to regain stability consisted of depooling eqiad (where the initial host was situated), restarting the sessionstore1001 Cassandra instance, ensuing that the cluster was healthy, restarting Kask and then re-pooling eqiad.","The issue was first human-detected and announced by Tamzin in #wikimedia-operations. However, some alerts also announced the degradation:
12:31 <+icinga-wm> PROBLEM - Sessionstore eqiad on sessionstore.svc.eqiad.wmnet is CRITICAL: /sessions/v1/{key} (Store value for key) is CRITICAL: Test Store value for key returned the unexpected status 500 (expecting: 201) https://www.mediawiki.org/wiki/
Kask
12:35 <+icinga-wm> PROBLEM - MediaWiki centralauth errors on graphite1004 is CRITICAL: CRITICAL: 60.00% of data above the critical threshold [1.0] https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-
alerts?panelId=3&fullscreen&orgId=1
12:40 <+icinga-wm> PROBLEM - MediaWiki edit session loss on graphite1004 is CRITICAL: CRITICAL: 100.00% of data above the critical threshold [50.0] https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000208/edit-count?orgId=1&viewPanel=13
None of these alerts paged. The errors seen in Cassandra logs were along the lines of the following, and were generated by Kask rather than any Cassandra-level cluster inconsistencies: 
DEBUG [prometheus-http-1-1] 2022-09-15 12:26:35,180 StorageProxy.java:2537 - Hosts not in agreement. Didn't get a response from everybody: 10.64.48.178,10.64.32.85,10.192.16.95",,"Expand documentation for Kask debugging
*Incorporate the upstream fix for this issue into Kask
*Alert on Kask error rate==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	Yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	Yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	Yes	
Process                                       	Was the public status page updated?                                                                               	No 	
Process                                       	Is there a phabricator task for the incident?                                                                     	No 	
Process                                       	Are the documented action items assigned?                                                                         	Yes	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	Yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	No 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	Yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	No 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	Yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	No 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	8",2022-09-15,Server and Infrastructure Failures,high,Service Downtime,
"Around 17:00 UTC, a faulty network switch caused partial failure of one rack in the Codfw cluster. As 8 individual hosts become unreachable, this led to reduced redundancy or capacity for some of the affected services. The hosts were moved to new switch ports, and were unreachabe for about 1 hour.* Elastic search: Three elastic nodes down, mild impact to redundancy (yellow state).* Swift media storage: Three ms-be nodes down in the standby cluster, no user impact (should catch up once reachable).* Edge cache: One cp node unreachable for the Codfw PoP, automatically depooled by healthcheck.* DNS: One dns node unreachable, effectively depooled automatically per lack of BGP advertising.",,,Tracking task for the incident: https://phabricator.wikimedia.org/T279457,2021-04-06,Server and Infrastructure Failures,mid,Degraded System Performance,
"Table wb_terms in wikidatawiki that provides labels and description lookup of entities (but also getting labels and description of entities; works both ways) is being replaced by a set normalized tables we call TermStore. Part of TermStore, is a class called DatabaseTermIdsResolver, picked up the right host for retrieving wikidata data in client wikis (including cawiki) but didn't use the right db name -wikidatawiki- thus code was trying to get information from wrong database  -cawiki-. The patch that fixes the issue gives more in-depth information on it. It was deployed five hours earlier on client wikis (The patch that enabled it on client wiki is 527087) then after caches got invalidated and started to use the new store.","We got the alert on IRC: 
 PROBLEM - MediaWiki exceptions and fatals per minute on graphite1004 is CRITICAL: CRITICAL: 90.00% of data above the critical threshold [50.0]",We don't have proper knowledge of how wikidata works and interacts with wikis.,"None for now - TBA
NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.",2019-08-07,Configuration and Deployment Errors,mid,Degraded System Performance,
"Parts of /srv/deployment were lost on active deployment server (deploy1002) due to the wrong command, rm -rf, being executed. This halted deployments for some time, until we were able to restore the directory from a backup and we checked it for correctness.Documentation:*T307349*Bacula backups",,,"https://phabricator.wikimedia.org/T309162==Scorecard==Incident Engagement™  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	/  	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	/  	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were all engineering tools required available and in service?                                                     	yes	
Tooling                                       	Was there a runbook for all known issues present?                                                                 	yes	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-05-02,Data Loss or Corruption,high,Service Downtime,
"Babel is a MediaWiki extension that displays infoboxes on user pages, expressing the user's proficiency in speaking one or more languages. When looking up language data for a user, it first consults the WANObjectCache, backed by memcached. If that user's data isn't in the cache, the Babel extension makes an API call to itself (that is, from Babel on an appserver to Babel on an API server), which fetches the language data from the database and caches it for next time. The cache hit ratio is normally about 90%.From 16:03 to 16:12 UTC on 2020-02-04, the WANObjectCache hit ratio for Babel keys dropped, bottoming at about 52%. (graph) The total rate of cache lookup attempts -- that is, hits plus misses -- remained roughly constant at about 10,000 per minute (graph), suggesting that either the Babel keys were suddenly evicted from the cache and had to be repopulated, or Babel traffic suddenly shifted to a new set of keys that was not yet cached. (TODO: Determine which, and clarify.)Because of the increased cache misses, the rate of Babel API requests also increased. These requests were sent over HTTPS (CommonSettings.php) so the extra computation due to TLS caused high CPU load on the appservers (graph). Each request hung for ten seconds waiting for a response before timing out (graph), which tied up appserver resources and delayed other traffic. Appserver errors and latency recovered immediately when the Babel cache hit ratio returned to normal, but a Kartotherian outage, which may have been triggered by the same spike in memcached misses, continued for some time.It seems that the origin of the incident is a further slowdown of the appservers in response to a surge in traffic that happened around 16:01 (graph). While the surge seems relatively small, it corresponds with the initial slowdown of application servers.TODO: Supplement graph links with inline static images.","The first Icinga CRITICAL alert in #wikimedia-operations was at 16:03 (for ""phpfpm_up reduced availability""). The first page was at 16:05 (""LVS HTTPS IPv6 #page on text-lb-ulsfo-wikimeda.org_ipv6 is CRITICAL: CRITICAL - Socket timeout after 10 seconds""). In all, we received 239 IRC-only alerts and 7 pages. (These numbers reflect only the MediaWiki latency issue, not the Kartotherian issue.)All seven paging alerts were of the form ""LVS HTTPS IPv{4,6} #page on text-lb.{DATACENTER}.wikimedia.org_ipv{4,6} is CRITICAL: CRITICAL - Socket timeout after 10 seconds"" across all five caching data centers.",What weaknesses did we learn about and how can we address them?The following sub-sections should have a couple brief bullet points each.,"TODO: This is an incomplete list of actionables, not all actionables have tasks, and not all tasks are tagged ""incident.""* Eliminate all Mediawiki appserver self-calls over HTTPS. Short-term, move them to HTTP; longer-term, to Envoy or similar. https://phabricator.wikimedia.org/T244843
* Make timeouts on http calls short enough not to cause a cascading outage when one cluster is slow. Done for Babel.
* Implement some automated way of sharing incident docs with WMDE phab:T244395
* MediaWiki's HTTP call abstraction should pass through the X-Wikimedia-Debug header if it was set in the original request
* Fix a bad interaction where Wikidata/CachingPropertyInfoLookups don't actually cache data in WANObjectCache, leading to many repeated calls for the same memcached key on the same server. phab:T243955 Done.
* Reduce read pressure on memcached servers by adding a machine-local Memcache instance phab:T244340
* Investigate and propose options on getting structured data from API and application server logs, to improve observability of exactly what's expensive on appservers. phab:T235773
* Investigate why a slowdown in response times from the API causes a surge in cache misses for Babel data in Wikibase. See https://phabricator.wikimedia.org/T244877",2020-02-04,Performance and Load Issues,high,Degraded System Performance,
"A change was made to the Memcached configuration of Wikimedia Commons. The change would distribute cache keys in a more optimal way (T252564). It had the expected impact that there would be a temporarily increase in re-computations, and therefore it was applied to only one big wiki at a time. Each wiki has its own cache namespace, and there is a global namespace for shared cache keys. Global keys must be distributed in the same way across all wikis (regardless of the wikis' own cache configuration), to avoid a split-brain scenario.It turned out that for many years, the cache keys relating to thumbnail metadata were wrongly marked as ""local"" instead of ""global"". Which meant that Commons' now used the new distribution, while Wikipedia used the old still, thus a split-brain scenario.From first report to resolution took 4 hours. Total time the issue is presumed to have existed is 21 hours.Impact: About 10% of media uploads hit a race condition causing them to wrongly return a cached impression of the file not existing and thus unable to be inserted into articles. No data was lost. All affected files eventually became accessible either by themselves after a cache churn, or when our fix was deployed.","The first known report (retroactively speaking) was in the Commons Village Pump. The first time we were aware of it was through a user report on Phabiricator.As far as we know, no alerts were fired at any time relating to this issue.* Memcached was working fine from a service perspective. Usage levels and usage patterns were also normal.
* MediaWiki was operating fine.
* The frontends and health checks were all fine.The one area were the issue may've been noticable to our monitoring is the HTTP traffic breakdown by status code.When a user tries to acces a Commons file description page locally on a wiki, they would have gotten a 404 Not Found for the affected files. If this issue affected more than 10% of uploads, and if accessing such pages directly was commonly done by users for some reason, then it would have likely showed in the HTTP 40x respoonse monitoring. However, neither was the case.",,"Adopt a policy to by default prevent the train from pausing mid-way over a weekend. By Friday, either roll out or roll back. Weekend incident investigation should not have to deal with multi-version. Even if the train is not longer blocked and there simply wasn't time to roll out completely, either rollback or roll out anyway. – T260401
* Determine stewardship for UploadWizard extension. – T240281
* Determine stewardship for MW Core's upload API.  – T240281
* Determine stewardship for MW Core's FileRepo backend.  – T240281
* Strengthen FileRepo logic for cache keys to be less fragile and duplicated. – T261534
* Fix FileRepo cache statistics to follow current conventions to avoid confusing our monitoring dashboards  – T261534
* Consider having some amount of regular QA for uploading and multimedia in Beta and prod. – T260402",2020-05-22,Configuration and Deployment Errors,mid,Degraded System Performance,
"The metadata is aimed at helping provide a quick snapshot of context around what happened during the incident.Incident ID   	2021-11-10 cirrussearch commonsfile outage                                                                                                                                                                	UTC Start Timestamp:                                                                                                                                                                                      	YYYY-MM-DD hh:mm:ssIncident Task 	https://phabricator.wikimedia.org/T299967                                                                                                                                                                 	UTC End Timestamp                                                                                                                                                                                         	YYYY-MM-DD hh:mm:ssPeople Paged  	<amount of people>                                                                                                                                                                                        	Responder Count                                                                                                                                                                                           	<amount of people>Coordinator(s)	Names - Emails                                                                                                                                                                                            	Relevant Metrics / SLO(s) affected                                                                                                                                                                        	Relevant metrics% error budgetImpact:       	For about 2.5 hours (14:00-16:32 UTC), the Search results page was unavailable on many wikis (except for English Wikipedia). On Wikimedia Commons the search suggestions feature was unresponsive as well.	For about 2.5 hours (14:00-16:32 UTC), the Search results page was unavailable on many wikis (except for English Wikipedia). On Wikimedia Commons the search suggestions feature was unresponsive as well.	For about 2.5 hours (14:00-16:32 UTC), the Search results page was unavailable on many wikis (except for English Wikipedia). On Wikimedia Commons the search suggestions feature was unresponsive as well.On 10 November, as part of verifying a bug report, a developer submitted a high volume of search queries against the active production Cirrus cluster (eqiad cirrussearch) via a tunnel from their local mw-vagrant environment. vagrant provision was (probably) later run without the tunnel being properly closed first, which resulted in (for reasons not yet understood) the deletion and recreation of the commonswiki_file_1623767607 index.As a direct consequence, any Elasticsearch queries that targetted media files from commonswiki encountered a hard failure.During the incident, all media searches on Wikimedia Commons failed. Wikipedia projects were impacted as well,Log events of all affected requests (note: requires Logstash access) through the ""cross-wiki"" feature of the sidebar on Search results pages. This cross-wiki feature is enabled on most wikis by default, though notably not on English Wikipedia where the community disabled search integration to Commons.Note that the search suggestions feature, as present on all article pages was not affected (except on Wikimedia Commons itself). The search suggestions field is how how most searches are performed on Wikipedia, and was not impacted. Rather, it impacted the dedicated Search results page (""Special:Search"", which consistently failed to return results on wikis where the rendering of that page includes a sidebar with results from Wikimedia Commons.",,,"Future one-off debugging of the sort that triggered this incident, when it requires production data, should be done on cloudelastic, which is an up-to-date read-only Elasticsearch cluster. If production data is needed but <= 1 week stale data is acceptable, relforge should be used instead.",2021-11-10,Performance and Load Issues,high,Service Downtime,
A change to the global k8s defaults was merged that made the next mediawiki on kubernetes deployment pick up a wrong certificate for TLS termination.,"The issue was detected by a page: (ProbeDown) firing: (6) Service mw-api-ext:4447 has failed probes (http_mw-api-ext_ip4)
 (ProbeDown) firing: Service mw-api-int:4446 has failed probes (http_mw-api-int_ip4)There was also secondary failures: [12:51] <jinxer-wm> (KubernetesAPILatency) resolved: (4) High Kubernetes API latency [12:02] <icinga-wm> PROBLEM - Check unit status of httpbb_kubernetes_mw-api-int_hourly on cumin2002 is CRITICAL",,"Because of the low traffic (relatively small amount of errors) it took some time to pin this to k8s- does that need some actionables, or will it resolve itself as k8s becomes the majority of requests?
* Review if some deployment procedures/testing should be strengthen (e.g. surprising changes on next deployment, canary deployment for k8s, etc)
* Some metrics become unavailable or unhealthy during deployment- could something be done about that (either for the metrics or mitigation of deployment impact)
* Should depooling k8s have been done earlier?==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	yes	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the ""Incident status"" section atop the Google Doc kept up-to-date during the incident?                        	no 	no google doc
Process                                       	Was a public wikimediastatus.net entry created?                                                                   	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	no 	
Process                                       	Are the documented action items assigned?                                                                         	yes	no action items
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	yes	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2023-08-09,Configuration and Deployment Errors,mid,Security Vulnerabilities or Breaches,
"Network connectivity for the A4 codfw server rack went down due to full power loss of its switch. This caused lots of alert spam, but otherwise it had very little to no impact on users due to services not being pooled on codfw or redundancy working as intended.This was very similar incident to Incidents/2022-06-21 asw-a2-codfw accidental power cycle (bump wrong cable on switch side again). See that page for more details.After the secondary power cord was properly connected, connectivity recovered with no known issues. Power maintenance on that rack finished at 15:50.",,,,2022-06-30,Server and Infrastructure Failures,low,[],
"After an inadvertent restart of some WMCS cloudvirts and their associated VMs, all sites within the Beta Cluster (e.g. https://meta.wikimedia.beta.wmflabs.org/wiki/Main_Page) failed to load, with Error: 502, Next Hop Connection Failed — this persisted post-restart of the relevant VMs.Drafting: possibly an apache config/puppet failure (https://phabricator.wikimedia.org/T315350#8159826), restarting trafficserver seems to have fixed it (https://phabricator.wikimedia.org/T315350#8159954)The incident was complicated by the lack of Beta Cluster's maintenance meaning ongoing ""normal"" errors distracted from the cause.","TheresNoTime has an uptime monitor at https://uptime.theresnotime.io/status/wmf-beta which ""paged"" her
* User reports
* CI errors from beta sync",,"logspam watch broken on beta 
* Remove two cherry-picked reverts from deployment-puppetmaster04 
* Rebase & merge or re-cherry-pick 668701 on deployment-puppetmaster04
* Replace certificate on deployment-elastic09.deployment-prep 
* Add basic alertingCreate a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.Add the #Sustainability (Incident Followup) and the #SRE-OnFIRE (Pending Review & Scorecard)  Phabricator tag to these tasks.==Scorecard==Incident Engagement  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	no 	
People                                        	Were fewer than five people paged?                                                                                	yes	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	no 	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	yes	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	no 	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	no 	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	5",2022-08-16,Configuration and Deployment Errors,high,Service Downtime,
"A router hardware failure caused widespread issues. While a single hardware failure like that shouldn't cause issues, more hardware failure had preceded this one, lowering our capability to absorb failures.",Humans detected this first. Icinga alerted 1 minute later with pages for cr2-esams arriving to multiple SREs,esams is a SPOF effectively.,Restricted phabricator tasks T246009 and T245825 have been filed for replacing the FPCs on the routers after RMA process was recommendended by the vendor. Tasks are restricted per the usual policy for vendors,2020-02-24,Server and Infrastructure Failures,high,Service Downtime,
"Logged-out users saw the user interface in their browser language (Accept-Language header) rather than the default wiki content language,due to a configuration change inadvertently causing the $wgULSLanguageDetection setting to be unset and falling back to the default in the UniversalLanguageSelector extension.","Reported by users at phabricator:T246071 and other tasks.
There were no alerts as far as  is aware.If human only, an actionable should probably be ""add alerting"".",What weaknesses did we learn about and how can we address them?,"Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.NOTE: Please add the #wikimedia-incident Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
*  Remind more people of double sync workaround for T236104 – ops-l email by , SWAT documentation update
* T236104
* T246212 - the setting was moved to CommonSettings.php and documented.
* …",2020-02-25,Configuration and Deployment Errors,mid,Degraded System Performance,
"An increase in Score requests (musical note rendering) from Parsoid overwhelmed the Shellbox service. This was mitigated by allocating more k8s pods to Shellbox. The overload took place from 11:17 to 11:33.From the Grafana dashboard, we see that a majority of requests took over 5,000ms (5s) instead of less the usual 10ms (0.01s), and for two minutes 11:25-11:27 requests actually failed with HTTP 503 instead.From the Logstash dashboard, we measure approximately 35,000 failed requests during this time. Of which 99.9% were from Parsoid, for de.wikipedia.org requests that render musical notes through the Score extension. 26K received HTTP 503, and 9K received HTTP 504. The remaining 0.1% were edits or pageviews calling Shellbox for syntax highlighting. See 2022-07-11 Shellbox and parsoid saturation for re-occurrence and follow up action items.Documentation:*Shellbox requests dashboard",,,"T310557: Improving Shellbox resource management
*T312319: Reduce Lilypond shellouts from VisualEditor==Scorecard==Incident Engagement ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              	no 	
People                                        	Were the people who responded prepared enough to respond effectively                                              	yes	
People                                        	Were fewer than five people paged?                                                                                	no 	
People                                        	Were pages routed to the correct sub-team(s)?                                                                     	no 	
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.	yes	
Process                                       	Was the incident status section actively updated during the incident?                                             	yes	
Process                                       	Was the public status page updated?                                                                               	no 	
Process                                       	Is there a phabricator task for the incident?                                                                     	yes	
Process                                       	Are the documented action items assigned?                                                                         	no 	
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               	no 	
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.	no 	
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         	yes	
Tooling                                       	Did existing monitoring notify the initial responders?                                                            	yes	
Tooling                                       	Were the engineering tools that were to be used during the incident, available and in service?                    	yes	
Tooling                                       	Were the steps taken to mitigate guided by an existing runbook?                                                   	no 	
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)                                                                    	7",2022-07-03,Performance and Load Issues,high,Degraded System Performance,
"A flood of API traffic from an AWS instance caused caching servers to be overloaded. Services behind our caching layer were up, but not reachable during this time.",,,"T308952 - get a legend for haproxy ""anomalous session termination states""
*T308940 - follow-up on user reported ticket with  public incident report
*T308941 - semi related: Klaxon redirects to http==Scorecard==Incident Engagement™  ScoreCard                                              	Question                                                                                                          	Answer
(yes/no)	Notes
People                                        	Were the people responding to this incident sufficiently different than the previous five incidents?              		
People                                        	Were the people who responded prepared enough to respond effectively                                              		
People                                        	Were fewer than five people paged?                                                                                		
People                                        	Were pages routed to the correct sub-team(s)?                                                                     		
People                                        	Were pages routed to online (business hours) engineers?  Answer “no” if engineers were paged after business hours.		
Process                                       	Was the incident status section actively updated during the incident?                                             		
Process                                       	Was the public status page updated?                                                                               		
Process                                       	Is there a phabricator task for the incident?                                                                     		
Process                                       	Are the documented action items assigned?                                                                         		
Process                                       	Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?               		
Tooling                                       	To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? Answer “no” if there are
open tasks that would prevent this incident or make mitigation easier if implemented.		
Tooling                                       	Were the people responding able to communicate effectively during the incident with the existing tooling?         		
Tooling                                       	Did existing monitoring notify the initial responders?                                                            		
Tooling                                       	Were all engineering tools required available and in service?                                                     		
Tooling                                       	Was there a runbook for all known issues present?                                                                 		
Total score (count of all “yes” answers above)	Total score (count of all “yes” answers above)",2022-05-21,Performance and Load Issues,high,Service Downtime,
"At 23:56 UTC on Friday 20th, the top of rack switch asw2-d2-eqiad went down causing all servers in rack D2 to go offline. As this switch was also row D virtual-chassis master and spine to the upstream routers all row D servers suffered a brief connectivity loss. Some services were still degraded after the situation stabilized but were eventually recovered.","The first alert came from Icinga showing commons was down, but the switch failure wasn't clear until the alert storm had settled and correlation could be drawn from down hosts in Icinga cross-referenced with Netbox.
    23:57 UTC - <+icinga-wm> PROBLEM - Host commons.wikimedia.org is DOWN: /bin/ping -n -U -w 15 -c 5 commons.wikimedia.org
    < Alert storm here >",What weaknesses did we learn about and how can we address them?,"Investigate the switch failure - https://phabricator.wikimedia.org/T233645
*Make it easier to distinguish per-host icinga spam vs real whole-service-level icinga spam (cf. alerting infrastructure roadmap)
*Investigate solutions to non-UTF8 logging crashing logstash mutate plugin (and ultimately the pipeline) https://phabricator.wikimedia.org/T233662
*Emergency response to logstash being backlogged https://phabricator.wikimedia.org/T233735
*Fix MediaWiki spammy logs during such outages https://phabricator.wikimedia.org/T233739",2019-09-20,Server and Infrastructure Failures,high,Service Downtime,
