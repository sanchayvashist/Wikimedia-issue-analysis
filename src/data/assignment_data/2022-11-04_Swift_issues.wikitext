
{{irdoc|status=review}}

== Summary ==
{{Incident scorecard
| task = https://phabricator.wikimedia.org/T322424
| paged-num = 2
| responders-num = 10
| coordinators = denisse
| start = 2022-11-04 14:32
| end = 2022-11-04 15:21
| metrics = No relevant SLOs exist
| impact = Swift has reduced service availability affecting Commons/multimedia
}}…<!--Reminder: No private information on this page!-->

For 49 minutes the Swift/mediawiki file backend returned errors (how many? Which percentage?) for both reads and new uploads. {{TOC|align=right}}

== Timeline ==
[[File:Screenshot 2022-11-06 at 22-39-13 2022-11-01 Swift issues.png|thumb|A Grafana dashboard showing a "client errors" and a "server errors" graph for the Swift service at the time of the incident]]
''All times are in UTC.''
[[File:Swift service errors.png|thumb|A Grafana dashboard showing the memory usage of the Swift instances at the time of the incident]]

* 14:32 ('''INCIDENT BEGINS)'''
* 14:32 jinxer-wm: (ProbeDown) firing: Service thumbor:8800 has failed probes (http_thumbor_ip4) #page - <nowiki>https://wikitech.wikimedia.org/wiki/Runbook#thumbor:8800</nowiki> - <nowiki>https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DProbeDown</nowiki>
* 14:32 icinga-wm: PROBLEM - Swift https backend on ms-fe1010 is CRITICAL: CRITICAL - Socket timeout after 10 seconds <nowiki>https://wikitech.wikimedia.org/wiki/Swift</nowiki>
* 14:38 Incident opened.  denisse becomes IC.
* 14:41 [https://sal.toolforge.org/log/9yYTQ4QBa_6PSCT9dW78 depool ms-fe2009 on eqiad]
* 14:44 [https://sal.toolforge.org/log/ELAZQ4QB8Fs0LHO5gexm restart swift-proxy on ms-fe1010]
* 14:48 [https://sal.toolforge.org/log/oCYdQ4QBa_6PSCT9hHbo restart swift-proxy on ms-fe1011]
* 14:51 [https://sal.toolforge.org/log/k7AgQ4QB8Fs0LHO5UvHS restart swift-proxy on ms-fe1012]
* 14:52 r[https://sal.toolforge.org/log/bCYgQ4QBa_6PSCT9n3k7 epool ms-fe2009 on eqiad]
* 14:54 [https://www.wikimediastatus.net/incidents/lb4gcj9w5wl5 Statuspage incident] posted “Errors displaying or uploading media files.”
* 15:00 [https://sal.toolforge.org/log/yuAoQ4QB6FQ6iqKixKNd restart swift-proxy on codfw]
* 15:01 recovery of the ms-fe2* instance
* 15:21 ('''INCIDENT RESOLVED)''' (Statuspage updated) 
<!--Reminder: No private information on this page!-->

== Detection ==
The issue was detected automatically and the engineers On Call received a page from Splunk on Call

Alerts that fired during the incident:

* [https://portal.victorops.com/ui/wikimedia/incident/3133 Incident #3133]
* [https://portal.victorops.com/ui/wikimedia/incident/3134 Incident #3134]
* [https://portal.victorops.com/ui/wikimedia/incident/3135 Incident #3135]
* [https://portal.victorops.com/ui/wikimedia/incident/3136 Incident #3136]
* [https://portal.victorops.com/ui/wikimedia/incident/3137 Incident #3137]

The alerts that fired were useful for the engineers to solve the incident.

== Conclusions ==

=== What went well? ===

* Automated monitoring detected the incident
* Several engineers helped debug the issue

=== What went poorly? ===

* Our documentation for Swift could be updated.

=== Where did we get lucky? ===

* An expert in the Swift service was present
* We had unused hardware laying around

== Links to relevant documentation ==

* [https://docs.google.com/document/d/1gAJhqBnDCQK6bLz61w1Mr9z2wYiSwMRuFkU3EYQgNyw/edit?usp=sharing 2022-11-01 Swift issues]
* [https://docs.google.com/document/d/1Gd98aR28A4dw6dsXf0lMpHOWZoBwEzgTieR_5m7NSuk/edit# 2022-11-04 Swift issues (II)]

== Actionables ==

* Investigate why the alerts scalated to batphone even when the engineers on call have already ACK'd the initial alert.

* Add runbooks, documentation on how to troubleshoot this issues.

== Scorecard ==
{| class="wikitable"
|+[[Incident Scorecard|Incident Engagement ScoreCard]]
!
!Question
!Answer
(yes/no)
!Notes
|-
! rowspan="5" |People
|Were the people responding to this incident sufficiently different than the previous five incidents?
|no
|
|-
|Were the people who responded prepared enough to respond effectively
|no
|preparedness, we just discussed we don't understand what happened and that the documentation is a decade old
|-
|Were fewer than five people paged?
|no
|
|-
|Were pages routed to the correct sub-team(s)?
|no
|
|-
|Were pages routed to online (business hours) engineers?  ''Answer “no” if engineers were paged after business hours.''
|no
|
|-
! rowspan="5" |Process
|Was the incident status section actively updated during the incident?
|yes
|
|-
|Was the public status page updated?
|yes
|Jaime was not one of the oncallers nor the IC, but he was the first to speak up with the suggestion of updating the status page, quite a long time into the outage

Checking who can access file
|-
|Is there a phabricator task for the incident?
|yes
|<nowiki>https://phabricator.wikimedia.org/T322424</nowiki> 
|-
|Are the documented action items assigned?
|no
|The incident is very recent
|-
|Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?
|no
|
|-
! rowspan="5" |Tooling
|To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? ''Answer “no” if there are''
''open tasks that would prevent this incident or make mitigation easier if implemented.''
|yes
|We don't know what's causing the issue so there was no way to have a task for it
|-
|Were the people responding able to communicate effectively during the incident with the existing tooling?
|yes
|
|-
|Did existing monitoring notify the initial responders?
|yes
|
|-
|Were the engineering tools that were to be used during the incident, available and in service?
|no
|We didn't have any cummin cookbooks on how to restart the Swift service so the engineers had to figure out the right commands during the incident
|-
|Were the steps taken to mitigate guided by an existing runbook?
|no
|
|-
! colspan="2" align="right" |Total score (count of all “yes” answers above)
|6
|
|}